{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "8-Deep-Learning-and-text-training-models.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "090ee4aa14d84b42aea545b5844da99f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_36dc7ffb811548bea912fe1329989a3e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_062a2ebce7b749a6bd554b2f5d6fd4b5",
              "IPY_MODEL_d889373684b74108b13c7f79b9067ed0"
            ]
          }
        },
        "36dc7ffb811548bea912fe1329989a3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "062a2ebce7b749a6bd554b2f5d6fd4b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f52aecb10ee84613b6167b2f6e59eb1f",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 361,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 361,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9c3c477ba27c496185f1ff64ea92836b"
          }
        },
        "d889373684b74108b13c7f79b9067ed0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5de47dd68bdc4876bac230a768c7ba9f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 361/361 [00:00&lt;00:00, 7.04kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ff8a5dd805634809b0c275e63bdd4a80"
          }
        },
        "f52aecb10ee84613b6167b2f6e59eb1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9c3c477ba27c496185f1ff64ea92836b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5de47dd68bdc4876bac230a768c7ba9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ff8a5dd805634809b0c275e63bdd4a80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c9e49d0d6da3420d95e93b19e9f7cd97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0d41b8ccf1124d2a923e85d0dcbfcef1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f8653ce07dbd4154a9eec5a750bd68bb",
              "IPY_MODEL_50700167127e482980215f5c9f5a95dd"
            ]
          }
        },
        "0d41b8ccf1124d2a923e85d0dcbfcef1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f8653ce07dbd4154a9eec5a750bd68bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_65b1305bbbd549c098b7f52d7deb4df2",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_67892a51ee394f68b962e199fbbe8b0a"
          }
        },
        "50700167127e482980215f5c9f5a95dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a21e6c85932143a1b0ed3b4238fab85b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 440M/440M [00:36&lt;00:00, 12.1MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_baa24e0a0e3c4711a029f15ebe1885bc"
          }
        },
        "65b1305bbbd549c098b7f52d7deb4df2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "67892a51ee394f68b962e199fbbe8b0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a21e6c85932143a1b0ed3b4238fab85b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "baa24e0a0e3c4711a029f15ebe1885bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rachel-ker/Content-Analysis-2020/blob/master/8_Deep_Learning_and_text_training_models%20-%20Colab\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nFkME77f1Zh",
        "colab_type": "text"
      },
      "source": [
        "## Week - 8 - Deep Neural Nets and Text - Training Models\n",
        "\n",
        "In this week we will be introduced to using Deep Neural Networks to work with text. We have already seen some uses of neural networks for text in our classification HW, where we used a simple neural network to classify text - it performs quite well, but they can come up short in more sophisticated classification tasks, such as in predicting intent. We have also seen neural nets in the form of word embeddings such as Word2Vec - and while they certainly work well, they have some drawbacks, such as dealing with words with multiple meanings. \n",
        "\n",
        "BERT, which is a language model built using bidirectional encoders, allows us to have a powerful pre-trained model which we can then use to perform our own tasks based on the data we are analysing. \n",
        "\n",
        "In this notebook we will use ```huggingface/transformers```, which is a python package which allows for an easy interface to use pre-trained BERT models. It is built using Tensorflow and PyTorch, two computational graph packages which are built specifically for creating powerful neural networks. We will also be introducing Keras, which allows us to easily build Neural Networks in an abstracted way. Keras is a popular way to understand how we can stack layers to create such Neural Networks, though to reach state-of-the-art results we will stick with using BERT and similar models.\n",
        "\n",
        "To demonstrate this, we will use the [Corpus of Linguistic Acceptability](https://nyu-mll.github.io/CoLA/). We will also be using BERT by learning how to extract embeddings from such a model and use it to semantically probe sentences.\n",
        "There are a bunch of new packages and methods we will be using so be sure to update lucem_illud_2020.\n",
        "\n",
        "Note that this notebook is different to take advantage of the GPU here. We only run the GPU heavy tasks here, where we need models to be fine-tuned.\n",
        "\n",
        "The first section contains the CoLA classification task, the second contains training a model on Trump tweets, and the last bit has us training a model on US and UK blog posts.\n",
        "\n",
        "To switch on your GPU, go to edit -> notebook settings -> hardware accelerator -> enable GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTCP80HqWCZs",
        "colab_type": "code",
        "outputId": "1551f3ba-ce34-47b9-829d-14ae9fed288c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DkVY2HIWRPC",
        "colab_type": "code",
        "outputId": "bccfce3b-f7ba-499a-d3ed-16661758d2de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vzt0EKCqWVsN",
        "colab_type": "code",
        "outputId": "09d840a1-0548-44e4-812e-bb9442e493db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.5.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pwp7W3BWWERu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertConfig\n",
        "from transformers import AdamW, BertForSequenceClassification\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPbf8Yvyf-Hn",
        "colab_type": "text"
      },
      "source": [
        "We'll now load the CoLA dataset up here to Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZH0HWkNgbho",
        "colab_type": "code",
        "outputId": "557c7900-8904-4035-da38-734d00daf400",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRqSO7DUWXmR",
        "colab_type": "code",
        "outputId": "6f7d1757-b716-455b-e3d4-0762bed74e09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import wget\n",
        "import os\n",
        "\n",
        "print('Downloading dataset...')\n",
        "\n",
        "# The URL for the dataset zip file.\n",
        "url = 'https://nyu-mll.github.io/CoLA/cola_public_1.1.zip'\n",
        "\n",
        "# Download the file (if we haven't already)\n",
        "if not os.path.exists('./cola_public_1.1.zip'):\n",
        "    wget.download(url, './cola_public_1.1.zip')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading dataset...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dewGhdzgEkH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Unzip the dataset (if we haven't already)\n",
        "if not os.path.exists('./cola_public/'):\n",
        "    !unzip cola_public_1.1.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzyIo3XfgVxJ",
        "colab_type": "code",
        "outputId": "e68a77b7-d981-4d50-ab63-fa3825d744fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# # Load the dataset into a pandas dataframe.\n",
        "# df = pd.read_csv(\"./cola_public/raw/in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# # Report the number of sentences.\n",
        "# print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# # Display 10 random rows from the data.\n",
        "# df.sample(10)\n",
        "# df.loc[df.label == 0].sample(5)[['sentence', 'label']]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 8,551\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_source</th>\n",
              "      <th>label</th>\n",
              "      <th>label_notes</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5694</th>\n",
              "      <td>c_13</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I am drinking lemonade and eating a brownie.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3716</th>\n",
              "      <td>ks08</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>They told Angelica to arrive early for the award.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5084</th>\n",
              "      <td>ks08</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>It was the girl who kicked the ball.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6861</th>\n",
              "      <td>m_02</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The other plan she rejected out of hand.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7585</th>\n",
              "      <td>sks13</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>What she considers is John proud of his work.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6245</th>\n",
              "      <td>c_13</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>Is likely to Jean dance.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7978</th>\n",
              "      <td>ad03</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The analysis of Lucy took longer than that of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7706</th>\n",
              "      <td>ad03</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The landlord donated a helicopter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5072</th>\n",
              "      <td>ks08</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>It's their teaching material that we're using.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8318</th>\n",
              "      <td>ad03</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>You said that Anson thought that Julie had fai...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     sentence_source  ...                                           sentence\n",
              "5694            c_13  ...       I am drinking lemonade and eating a brownie.\n",
              "3716            ks08  ...  They told Angelica to arrive early for the award.\n",
              "5084            ks08  ...               It was the girl who kicked the ball.\n",
              "6861            m_02  ...           The other plan she rejected out of hand.\n",
              "7585           sks13  ...      What she considers is John proud of his work.\n",
              "6245            c_13  ...                           Is likely to Jean dance.\n",
              "7978            ad03  ...  The analysis of Lucy took longer than that of ...\n",
              "7706            ad03  ...                  The landlord donated a helicopter\n",
              "5072            ks08  ...     It's their teaching material that we're using.\n",
              "8318            ad03  ...  You said that Anson thought that Julie had fai...\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onaOGdjxgiyG",
        "colab_type": "code",
        "outputId": "55bca73f-526f-43d9-a055-a63b2dc9aab3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "df_raw = pd.read_csv(\"./refugee_coca_foranalysis.csv\")\n",
        "df = df_raw[['text','genre','year']]\n",
        "df.rename(columns={'text':'sentence', 'genre':'label'}, inplace=True)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:4238: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  return super().rename(**kwargs)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pjoz66ZUTmcK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a621f40e-1270-44b8-d57a-2910c689a7b7"
      },
      "source": [
        "df.label.unique()"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['MAG', 'SPOK', 'NEWS'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-pmj0icTpYS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "0837185b-2e58-4786-c1eb-7cfb118e6bff"
      },
      "source": [
        "df['label'] = df['label'].mask(df['label']=='MAG',0)\n",
        "df['label'] = df['label'].mask(df['label']=='SPOK',1)\n",
        "df['label'] = df['label'].mask(df['label']=='NEWS',2)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUhJf_oPDCNV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "a61cda8e-07e7-4514-a4d6-e192c6eb2bef"
      },
      "source": [
        "df.sample(10)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3949</th>\n",
              "      <td>beirut , lebanon -- syria 's top government s...</td>\n",
              "      <td>2</td>\n",
              "      <td>2012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>950</th>\n",
              "      <td>!ted-koppel : voice-over one week and 12,000 ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1387</th>\n",
              "      <td>bob !schieffer , host : good morning again , ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2713</th>\n",
              "      <td>with the nato bombing campaign against yugos...</td>\n",
              "      <td>2</td>\n",
              "      <td>1999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>582</th>\n",
              "      <td>\" if new orleans lacks material wealth , it p...</td>\n",
              "      <td>0</td>\n",
              "      <td>2006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>983</th>\n",
              "      <td>harry !smith , co-host : it 's 12 minutes unt...</td>\n",
              "      <td>1</td>\n",
              "      <td>1992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1028</th>\n",
              "      <td>frank sesno , cnn news : voice-over making th...</td>\n",
              "      <td>1</td>\n",
              "      <td>1992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2374</th>\n",
              "      <td>visiting the former yugoslavia not long ago ...</td>\n",
              "      <td>2</td>\n",
              "      <td>1994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3351</th>\n",
              "      <td>there is a grim joke about haiti among intern...</td>\n",
              "      <td>2</td>\n",
              "      <td>2010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3536</th>\n",
              "      <td>!margaret-brennan , -# well , bob , secretary...</td>\n",
              "      <td>1</td>\n",
              "      <td>2013</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence label  year\n",
              "3949   beirut , lebanon -- syria 's top government s...     2  2012\n",
              "950    !ted-koppel : voice-over one week and 12,000 ...     1  1991\n",
              "1387   bob !schieffer , host : good morning again , ...     1  1999\n",
              "2713    with the nato bombing campaign against yugos...     2  1999\n",
              "582    \" if new orleans lacks material wealth , it p...     0  2006\n",
              "983    harry !smith , co-host : it 's 12 minutes unt...     1  1992\n",
              "1028   frank sesno , cnn news : voice-over making th...     1  1992\n",
              "2374    visiting the former yugoslavia not long ago ...     2  1994\n",
              "3351   there is a grim joke about haiti among intern...     2  2010\n",
              "3536   !margaret-brennan , -# well , bob , secretary...     1  2013"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saNk183jgjVP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create sentence and label lists\n",
        "sentences = df.sentence.values\n",
        "\n",
        "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
        "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
        "labels = df.label.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoH-oU6iOEGd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "fe00cb88-5558-4f9b-d69a-72306305073c"
      },
      "source": [
        "sentences[:5]"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]   \" bums . \" that \\'s what radio havana called them . and indeed , some of the 125,000 cubans who fled to the u.s. 10 years ago were thugs and hoodlums . but not photojournalist eduardo suarez . his is a classic american success story , even though his finances are a little out of focus .  the date : may 22 , 1980 . the place the dingy industrial port of mariel , 27 miles west of havana . the time early evening . there on the docks under a cloudless , moonlit sky stood television cameraman eduardo suarez , 30 , his bride of seven months , betty , 22 , and her 55-year-old mother rosa . taking advantage of castro \\'s temporary liberalization of cuba \\'s emigration policy , they had come to mariel to embark on a life of freedom . in just 18 hours , that new life would begin as it has for millions of immigrants in america , at the bottom of the social and economic ladder .  slowly they filed onto a 50-foot shrimp boat named           about 25 people . that night it sat low in the water , jammed with 80 or so refugees , some of them just released from cuba \\'s mental hospitals and jails . money ? the suarez party had none ; it was illegal to take cuban pesos out of the country . belongings ? the clothes on their backs , a blanket and a single pillow . hopes and dreams ? of those they had plenty . and the next day , after a voyage that began calmly and ended with the passengers retching , salt-stained and exhausted , eduardo , betty and rosa could begin to act on them , initially from a resettlement camp in a world war ii seaplane hangar in key west . there to greet them that steamy friday afternoon were 3,000 or so of the 125,000 boatlift exiles who came to be branded \" marielitos \" -- a pejorative term that literally means little people from the port of mariel .  fast forward to may 6 , 1990 . the place the now abandoned seaplane hangar , long since cordoned off by barbed           sultry late sunday afternoon , is eduardo suarez , 40 . but this time he arrives as a 13-time emmy award-winning producer , director , cameraman and operations manager for channel 51 , a spanish-language television station in greater miami , whose 605,000 cuban expatriates ( 31% of the population ) make it the second largest cuban city in the world . the intense professional , who makes about $65,000 a year , has come to film la calma despues de la tormenta ( the calm after the storm ) , a documentary about the mariel refugees \\' fate . as he gazes at the hangar , the hair on his forearms rises . \" me erize -- i have goose bumps , \" he says , then adds , \" i came through here . \"  and he has barely looked back . in the 10 years since the may-to-september 1980 mariel boatlift , eduardo and his shy , practical-minded wife betty have built an american dream . they both have good jobs he at the tv station , she as a $19,000-a-year draftsperson for the northwest miami engineering           ranch house along a canal in a middle-class neighborhood in southwest miami , complete with 33-foot swimming pool , coconut palms and , yes , a white picket fence . cost : $86,000 , financed four years ago with a 30-year , 9.5% fixed-rate mortgage . they have a high-spirited and independent two-year-old daughter , andrea . and they have all american aspirations a parochial-school education for andrea followed by the college of her choice , as well as financial security for themselves .  but to realize their aspirations , the suarezes will need to bring their finances into sharper focus . like many immigrants , they have been concentrating on making money , not managing it-and their spending is so out of control that nearly 30% of their income remains unaccounted for . then too , with andrea to look out for , eduardo needs to beef up his life and long-term disability insurance . and he and betty -- whose english , like eduardo \\'s , is spotty-have yet to take the simplest and least expensive step to protect their daughter \\'s future making a will .            mariel refugees -- belies the stigma that the cuban boat people bear . of the 125,000 whom the cuban government allowed to leave in 1980 , only some 2,000 were confirmed criminals or mentally ill persons freed by castro . \" the u.s. has always wanted to pick the best brains of our people , \" radio havana cackled at the time . \" let them also pick up the bums . \" although a few exiled felons were apprehended by u.s. immigration officials , most were not . \" the conditions were too crowded , \" says u.s. army col. juan armando montes , who coordinated the reception . at the peak in early summer , he says , some 3,500 people were arriving every day .  as a result , within months of the boatlift \\'s end in september , some mariel refugees had committed murders , burglaries or hijackings . yet many others became model citizens . indeed , says lisandro perez , chairman of sociology and anthropology at miami \\'s florida international university and himself a 1960 cuban refugee , mariels were driven less by politics           , make money and put their kids through college . \"  the suarezes are a perfect example . eduardo had worked as a television cameraman on and off for seven years in cuba in the 1970s . top pay the equivalent of about $300 a year . but he was drawn to the u.s. after a 1979 visit to his ailing father in miami ( the elder suarez died last march ) . \" in cuba , \" eduardo explains , \" you had to work closely with the communist party in order to advance . \" trouble was , eduardo never identified with any regime . \" i just wanted to live up to my potential , \" he says , \" and i needed freedom in order to do it . \"  the decision to emigrate , though , was excruciating . for eduardo , it meant leaving behind his mother , brother and , hardest of all , five- and six-year-old edgar and elianne , his son and daughter from a previous marriage . betty , though she had an older sister and an aunt in           to leave her father rolando , a contractor . \" he could n\\'t even speak the word good-bye , \" she recalls .  the 18-hour , 104-mile ride on the tropic dream was peaceful at first . in the middle of the night , however , the boat began to pitch in rising seas . betty and many others became seasick . and all were bedraggled by the time the captain -- himself a cuban exile-turned them loose in key west the next day . the captain \\'s advice ? \" the first year is the hardest , \" he told them . \" you \\'ll cry a lot and wish you never left cuba . but then things will get easier . \"  his words proved prophetic . the suarezes moved into an eight-foot-by-10-foot bedroom at betty \\'s aunt \\'s house . eduardo found a $3.10-an-hour job as a photo lab assistant . betty , who had studied drawing at the instituto tecnologico oswaldo herrera in cuba , landed a drafting position that paid a scant $4,992 a year . both were lonely and depressed . \" i           \" says betty , who grew up in a comfortable havana neighborhood . \" for the first 12 months , i even missed the walls . \" adds eduardo : \" i thought about my children all the time . \"  their first break came within two months , however , when eduardo was hired as a cameraman for $4.50 an hour at channel 23 , a nonunion spanish-language tv station . they moved to their own one-bedroom apartment , paying $350 a month in rent , and stayed there two years while eduardo advanced to chief photographer and betty moved to the engineering firm . by 1984 , they were ready to buy a $54,000 two-bedroom townhouse in suburban kendall ( \" we thought it was our castle , \" says eduardo ) . and in \\' 86 , they closed on their current dwelling . call it miami nice the gleaming white interior of the 20-year-old home bespeaks extensive renovation . the money for the $30,000 project came from savings and a $7,000 , 18% consumer loan from southeast bank on which the suarezes still pay $115 a           eduardo \\'s career was booming . he was promoted three times at channel 23 before becoming operations manager at rival channel 51 two years ago . eduardo has now won more local emmy awards -- 13 , for outstanding camera work , producing and directing -- than he has spent years in this country . but which emmy does he cherish most ? numero uno , for a piece about the 1982 crash in new orleans of a pan am flight that originated in miami . he still keeps a yellowed miami herald clip about the awards ceremony . \" this award means a lot to me , \" he told a hushed crowd of 2,000 . \" it is true this is the land of opportunity . \" neither eduardo nor betty would move back to cuba , even if castro left power . says betty : \" this is our home now . \"  while the emmys were impressive signs of success , the suarezes \\' greatest reward by far since coming to america was the birth of andrea in may 1988 . \" she is the most           eduardo , \" and i want to give her everything i can . \"  but when it comes to andrea \\'s education , the suarezes will not be able to afford their dreams unless they reform their spending . in some ways , life at casa suarez seems like one big fiesta . concedes eduardo : \" i love having many people at my house . \" most sundays , for example , eight to 10 members of their large extended families gather at the suarez \\'s home for one of his specialties churrasco al barbecue , an argentine beefsteak dusted with mojo criollo -- seasoning salt -- and grilled over hickory chips . average cost for the feast $80 . the couple also enjoy dining out with friends at places such as malaga , a $15- to $20-a meal spanish joint in miami \\'s pulsating little havana , and christy \\'s , a clubby coral gables steakhouse where prime rib for two , plus wine , costs about $75 . on top of that , they go for lunch every day with colleagues from work . \" eating in           unfortunately , it \\'s also his achilles heel , since he and betty spend around $10,000 a year eating out .  with this and other spending , including the home renovation , the suarezes have set aside only about $11,700 over the past few years . that \\'s simply too little , given their approximately $85,000 combined salaries . about 60% of the savings is in a 5 1/4% passbook account and 40% in a 7% bank certificate of deposit . also , on the advice of a colleague , eduardo rolled over $3,200 of his channel 23 profit-sharing money into individual retirement accounts invested in a real estate limited partnership and a money fund at dean witter ( recent yield 7.8% ) . betty has about $6,000 in the engineering firm \\'s retirement plan , about 85% of it in a money fund and the remainder in a real estate limited partnership .  as for liabilities , in addition to the $4,500 they still owe on the home improvement loan , eduardo and betty have $4,500 outstanding on an 11.5% three-year home-equity loan . they took it out           let balloon to $6,000 , mostly to cover the costs of clothing , gifts and furniture . the monthly payment is $115 . auto loan balances total $7,248 on her \\' 86 thunderbird and his \\' 89 mitsubishi galant . payments on both come to $600 a month , and insurance on the cars costs another $1,400 a year . eduardo \\'s and betty \\'s life insurance and his disability coverage ( betty has none ) come through channel 51 ; the premiums , deducted from his paycheck , total $148 a year . but eduardo \\'s life insurance would pay only about $1.30,000 on his death ( $280,000 if he dies in an accident ) and $60,000 for betty in the event that her death is accidental ; otherwise , she is uncovered . on the disability side , eduardo \\'s policy would pay two-thirds of his salary , beginning 26 weeks after he \\'s injured and lasting until age 65 .  these and other fixed expenses , however , amount to only about 70% of their income . the rest remains unaccounted for , although they think the           betty \\'s father came over from havana for an extended stay last october and again in april -- the latest in a stream of visiting in-laws . the suarezes picked up the tab , including air fare , of $2,500 to $3,000 . in addition , several times a year , eduardo sends packages stuffed with cash , nikes , nintendo games and the like to edgar and elianne , now 15 and 16 respectively ( \" as the children get bigger , the gifts get more expensive , \" he sighs ) . eduardo says he can not put a figure on how much he and betty have laid out to help their families over the past 10 years except that it amounts to \" many thousands of dollars . \" and every penny , he adds , was well spent .  yet some of that money no doubt should have gone into savings . and even eduardo seems to realize that now . \" before andrea , i never really thought about the future , \" he says . \" i just did what i had to do           advice  the problems : curb spending , make an estate plan and save for andrea \\'s education .  the solutions : start budgeting , write wills , shore up life and disability coverage , and invest in conservative , growth-oriented mutual funds .  money asked miami certified financial planners joseph ross of first affiliated securities and harold evensky of evensky &; brown to advise eduardo and betty suarez on how to get their finances in shape . recommendations :  get a grip on cash flow . although the suarezes have fared exceedingly well in their careers , they wo n\\'t be able to control their finances until they figure out where their money is going . for two months , the couple should keep a daily log of all their fixed and incidental expenses -- \" down to the penny , \" ross says . \" the more you find to put away , the more you \\'ll have for andrea \\'s future . \" one way betty and eduardo can immediately and painlessly begin to save is by expanding the credit line on their 11.5% home-equity           the money to pay off the $4,500 balance on their 18% consumer loan . interest on the home-equity loan is fully tax deductible ; that on the consumer loan is only 10% deductible this year and not at all in 1991 . first-year after-tax savings : $404 . they should not , however , use the credit line for extra spending .  shore up insurance and write a will . even before betty and eduardo begin saving for andrea \\'s education , they should set aside enough money to ensure that she is well taken care of if something happens to them . \" andrea does n\\'t start school for three more years , \" ross points out . \" but one or both of you could die or become disabled tomorrow . \" with the help of an attorney , therefore , betty and eduardo should draw up wills immediately and include provisions in them for andrea \\'s guardianship . probable cost : $400 to $800 . they should also supplement their life insurance . evensky figures eduardo should have $400,000 of total coverage and betty $150,000 that will           he recommended universal life policies , which combine insurance with tax-deferred savings , offered by fee for service , a no-load insurance company in tampa . cost : $1,400 a year for eduardo and $510 for betty . eduardo should also boost his disability insurance to cover 80% of his salary , instead of 67% , at an added cost of $317 a year . since betty contributes about a fourth of the family income , she should also have 80% disability coverage . her premium will run $340 a year .  paying for school . evensky estimates that 13 years of parochial school for andrea ( including kindergarten ) plus four years of private college will cost more than $140,000 in today \\'s dollars -- or $367,000 if college costs rise a plausible 7% a year . \" if the suarezes set aside about $1,000 a month and earn a moderate 7% on their money after taxes , \" he says , \" they can pay for it . \" the $1,000 goal is admittedly a tough one , but the suarezes should aim for it nonetheless -- putting           best returns of any financial asset ( about 10% a year before taxes ) . the suarezes , says evensky , \" can afford to take some risks . in fact , with so many expenses ahead of them , they ca n\\'t afford not to . \" the planners suggest that eduardo and betty begin making regular contributions to a couple of conservative equity funds with strong long-term records . examples : lindner fund ( no load ; 314-727-5305 ) and mathers fund ( no load ; 800-962-3863 ) , which are up 97% and 111% respectively over the five years that ended on june 1 .  eduardo and betty suarez were slightly overwhelmed by the planners \\' recommendations . but they started keeping track of expenses , as the advisers suggested , and expanded their home-equity , line of credit . they also planned to write wills and augment their insurance coverage . \" we ye lived here 10 years , and no one ever told us about these things , \" said betty of the planners \\' advice . for his part , eduardo was determined to           , you really have to plan a lot , \" he observed . \" we must . and we will . \" the leaking hole in their budget  their expenses aside , the suarezes are n\\'t sure where nearly 30% of their income goes . table  photos ( color ) : eduardo suarez in a 19 76 self-portrait ( left ) as a havana photographer ; and today ( right ) as the emmy award-winning operations manager of a miami tv station  photo ( black &; white ) : andrea \\'s second birthday party ( left ) , attended by 60 friends and family members , was typical of the generous entertaining the suarezes need to control . above , the children by an earlier marriage -- edgar and elianne -- whom eduardo had to leave in cuba .  photo ( color ) : striking contrasts of today and yesterday : above , betty at her draftsperson \\'s job , with snapshots of andrea on the wall  photo ( black &; white ) : below , the refugee center where the suarezes and other mariel refugees             [SEP]',\n",
              " '[CLS]  section : investing  expanding petrochemical plants around houston will keep business growing for the kirby corp. , whose dixie carriers subsidiary is a top barge operation  some places are hot while others are not . here are six regions where savvy investors will make money in the 1990s .  look northwest , young investor . let \\'s say that three years ago , you wisely saw a bonfire building in exports from oregon and washington , sparked by strong airliner and timber sales to europe and the far east . so you bought shares in the composite northwest 50 index , a mutual fund that holds stocks tied to that region \\'s economy . since then , while few money managers have beaten the overall stock market , you and your fund have prospered : composite northwest scored a total return of 55.7% , nearly 11/2 times better than standard &; poor \\'s 500 .  the moral of this tale : in the eight years since the last national recession , a pattern of powerful regional business cycles has taken shape , offering opportunities           the key players . national business cycles have n\\'t been repealed entirely . softness in enough parts of the economy could still produce a countrywide recession , but that is less likely than in the past . \" now we have a more diversified economy , where exports and other international factors play a bigger role , \" explains chief economist edward yardeni of prudential-bache .  to choose the best regions for investors in the 1990s , we looked first at projections of growth in jobs and population by the economic consulting firm dri/mcgraw-hill . \" where jobs are expanding , that attracts migration , \" says dri senior regional economist beth burnham mace . and that in turn fuels retail spending , adding more jobs and ensuring strong growth . employment in the six standout regions that money identified is expected to expand in the 1990s by 15% to 22% , compared with a projected 13.8% national average . population in our six areas will grow 9.5% to 23% -- against 7.2% nationally .  in addition , better climate and lower living costs in states in the southern           and people , according to dri and u.s. census projections . and many economists believe that the federal bailout of failed savings and loans will boost the economies of texas , new mexico and other states where s &l; problems have been acute . by one estimate , over 30 or more years texas could receive $4,775 more in federal funds per resident than its citizens pay out in federal taxes , thanks to the bailout . upshot : the move to the sunbelt of the 1970s and early 1980s -- interrupted by the oil bust in the mid- \\' 80s -- is resuming . that explains why most of our hot regions are in the sunbelt .  we ranked the regions on the basis of investment opportunities . for example , the southwest outside texas -- an area that is expected to be tops in both jobs and population increase -- stands only third on our list because of a paucity of large , publicly traded companies positioned to cash in on that growth . our advisers \\' picks are either in businesses dependent on a regional economy \\'s           in a region by its raw materials or transport system ( a barge line hauling commodities from texas on in land waterways , for example ) .  how much money should you invest regionally ? portfolio analysts note that the risk varies from region to region and cycle to cycle , but they generally recommend that you commit no more than 10% to 15% of your portfolio to such investments .  here is a closer look at our six picks . note that where individual states -- florida , hawaii and texas -- have distinct economies , we have treated them as regions unto themselves : texas  bouncing back from a decade when its growth business was bankruptcy law , texas is expected to outpace the national economy handily through 2000 . job growth is projected at 1.6% a year , compared with 1.3% nationally . the texas economy now lives and dies less on the swing in oil prices : energy accounts for 15% of the state \\'s economy , compared with 27% in 1981 . \" over the next 10 years , texas will see steady           of the center for economic development and research at the university of north texas . cheap office space and low living costs already have led 49 companies -- including exxon and j.c . penney -- to move to the dallas area in the past two years . office construction has begun in that area for a number of relocating companies , including gte corp . and mci communications . forecasters also believe that housing construction will pick up in the next two years . all of this building should benefit justin industries ( recently traded over the counter at $16 ) , a maker of bricks and other building materials , with estimated 1990 annual revenues of $300 million , says research director eugene melnitchenko of the dallas brokerage eppler guerin &; turner .  petrochemicals are also perking along , with companies planning to expand production by 15% for two major products , ethylene and propylene , in the houston area . a major beneficiary could be $170 million kirby corp . ( american stock exchange , $10.75 ) , the country \\'s largest operator of inland tank barges .           demand for environmentally clean natural gas , nudged by government regulation and an end to the oversupply of the past decade , will push up prices 25% by the early 1990s . they are recommending gas exploration companies such as $10.5 billion enron ( new york stock exchange , $56.75 ) and $2.6 billion arkla ( nyse , $23.50 ) .  fund of the southwest ( 4.75% load ; 800-262-6631 ) invests mainly in firms doing business in texas and five other states -- arizona , arkansas , louisiana , new mexico and oklahoma . since manager sharon stone , 34 , took over the fund in 1987 and diversified into consumer-product companies , the $15 million portfolio has outperformed the s &p; 500 by three percentage points in 1988 , 1.2 points in 1989 and one point so far this year . ( the mutual funds cited in this article are sponsored by small to medium-size money-management companies in their respective regions . ) pacific northwest  even before its moist piney vistas and superior cherry pie were getting prime-time praise on abc-tv \\'s twin peaks , the northwest           life will keep them coming . douglas pedersen , an economist at the security pacific bank in seattle , looks for \" a rapid inflow of new residents , plus strong income and employment gains . \" washington and oregon are projected to add new jobs at about a 2%-a-year pace-double the national rate -- in the early 1990s and 1.5% later on .  pedersen points out that the region is coming off unsustainable boom levels : nearly 1.00,000 new residents and about 5% job growth in 1988 and 1989 . but new businesses have built in something this region never had before : stabilizing economic diversity . to its traditional mix of airplane manufacturing and lumbering , the northwest has added a burgeoning technology sector . and soaring trade with the far east -- from importing clothes to exporting high-tech components -- is fueling growth .  foreign passenger-plane orders keep swelling the backlog at boeing , which still generates 15% to 20% of washington state \\'s economic activity . but after a nearly 50% run-up in boeing \\'s share price so far this year , many analysts deem           jr . in seattle recommends two boeing suppliers instead , both trading at a price/ earnings ratio at or below the stock market \\'s . eldec ( otc , $12.50 ) , with $110 million revenue , produces airliner parts ; $500 million precision castparts ( nyse , $31.50 ) makes jet engine parts .  in the timber and paper business , torn over the endangered spotted owl , pending restrictions on cutting timber on federal land will benefit weyerhaeuser ( nyse , $25.75 ) , argues whitlow \\'s dain bosworth colleague george haloulakos . weyerhaeuser ( revenues of $10.3 billion ) owns 2.9 million acres of timberland in the northwest -- double its closest competitor .  the composite northwest 50 index fund ( 4.5% load ; 800-543-8072 ) closely reflects the northwestern economy . the fund holds 50 stocks at a time within industry sectors weighted by their contributions to personal income in the region . annual returns over the fund \\'s 31/4-year life average 20% vs. 13.5% for the s &p; 500 . southwest outside texas  by the year 2000 , the lonesome mountains and deserts           development . nevada is forecast by dri/mcgraw-hill to be no. 1 in both population and job growth , while arizona , colorado , new mexico and utah are among the top 10 states for both categories after growth picks up by 1992 -- though parts of the region are still coping with economies stunned by real estate problems . casinos account for 45% of the nevada economy , but much recent growth has come from small manufacturing companies in such fields as medical instruments and aerospace parts . many are refugees from high-tax california , and unfortunately most are not publicly traded .  among stocks you can buy , manager sharon stone of the fund for the southwest favors southwest airlines ( nyse , $26.50 ) , with revenues of $1.1 billion and a heavy schedule of short-hop flights throughout the region . industry analysts say southwest is a low-cost airline with a high level of customer satisfaction . for income-oriented investors , value line analyst guy woodlief suggests $10.2 billion u , s. west ( nyse , $36.50 ) , which provides phone service for all of the region           rising business telephone use should keep the company growing , he says . its dividend , already yielding 5.3% , could increase by 5% a year over the next five years , he adds . hawaii  the japanese appetite for hawaiian land has kept a real estate run-up going at a time when markets elsewhere are melting . honolulu housing prices rose 25% in 1989 alone . with hawaii \\'s population expected to expand nearly 20% by 2000 , the state will add new jobs at a 1.7% annual rate -- 30% faster than the national average . \" the real estate boom will continue because there is strong foreign demand for residential and commercial space and very little supply , \" says bert dohmen-ramirez , an investment adviser in honolulu .  how does an investor enter this pricey scene ? not through reits or companies solely in local real estate , advises dohmen ; they tend to be too heavily in debt . instead , he suggests two companies with large hawaiian real estate holdings that stand to benefit from asian demand as well as other operations .           revenues of $770 million , grows sugar cane and operates the largest shipping line hauling freight between the islands and the u.s. west coast . and castle &; cooke ( nyse , $30.50 ) , with revenues of $3 billion , owns 98% of the island of lanai , where it has just opened a new luxury resort : . the company also includes the pineapple producer dole food , which it plans to spin off soon . dohmen believes that the two parts will be worth more than the current stock price . florida  with sand and sun and mickey mouse to attract tourists and new residents , florida is forecast to be the decade \\'s second-fastest-growing state in population after nevada . retirees will be prominent among the arrivals , but jobs are also expected to increase at a robust 2.1% annual rate .  one way to invest in this vigorous growth is through $360 million advanced telecommunications ( otc , $20 ) , which has 10% of the florida market for long-distance services . advanced , which concentrates on small businesses and offers below-at &t; rates           the next five years , says analyst robert bolen of the regional brokerage j.c . bradford &; co. in nashville .  and then there \\'s the orlando play . in addition to the mickey and goofy gang , a movie studio in orlando opened in june by universal is drawing big crowds , including a flood of foreign visitors drawn by a weaker dollar as well as the hollywood-comes-east ambience . a company positioned to profit from the fun-biz boom is major realty ( otc , $13.50 ) , a $12 million developer that owns 274 acres adjacent to universal . hotel chains are inquiring about the site , the company says . some analysts believe that its real estate alone may be worth $30 a share .  another strong tourism entry is miami-based carnival cruise lines ( ase , $24 ) , the nation \\'s largest such line . look for carnival , with estimated 1990 revenues of $1.4 billion , to more than double its earnings in the next five years , says analyst richard lilly , who specializes in florida companies for the regional brokerage jw           to raise passenger capacity 50% by 1995 . southeast outside florida  this slice of the old south -- the carolinas , georgia and virginia -- will enjoy a 13% population spurt in the \\' 90s . that \\'s not up to florida \\'s dizzying 20% , but it \\'s almost twice the u.s. rate . the region is expected to add jobs at a healthy 1.4% a year , according to dri/mcgraw-hill . the mild climate and below-average living costs will keep attracting companies and households from the northeast and midwest .  southeastern regional banks , the best of which never took on many of the bad loans that crippled their cousins in the southwest and northeast , are a strong buy now , says research coordinator david lee of the regional broker robinson-humphrey in atlanta . the stocks remain cheap -- selling at price/earnings ratios about half those of the overall stock market -- after being knocked down in anticipation of stringent federal audits that are now partially complete , says lee . he recommends two big regionals \" that have come through their federal audits with flying           ) , based in atlanta , will have nearly $50 billion in assets when it completes a merger this fall and becomes known as avantor financial . ncnb ( nyse , $36.50 ) , based in charlotte , n.c. , with assets of $68 billion , operates throughout the southeast and also has a large texas subsidiary that figures to profit in the years ahead .  automation has allowed the textile industry to build new plants in the southeast . ted price , chief investment officer of wheat investment advisors in richmond , favors unifi ( nyse , $15.75 ) . with a dominant position in the market for polyester fibers , unifi can increase prices readily . price notes that unifi , which acquired a major competitor in 1986 , is now financially strong enough to expand again through more takeovers . the stock is selling at a below-market 11 times the company \\'s estimated 1990 earnings .  southeastern growth will provide business for ryan \\'s family steak houses ( otc , $7.75 ) , says robinson-humphrey analyst bryan elliott . ryan \\'s , with $275 million in           of its 119 cafeteria-style restaurants aimed at middle-income families in the southeast . elliott projects 20% annual earnings growth over the next five years .  photo ( color ) : as sure as owls hoot , logging limitations in federal forests will be a boon to weyerhaeuser , which owns twice as much timberland as its nearest competitor .  by jerry edgerton  reporter associate : carla a. fried  rating the top 40 towns  this table ranks the 40 largest u.s. metropolitan areas according to how well stocks of companies based in them have performed . money compiled it using indexes of selected stocks from the individual cities . each index incorporates the stocks of at least 10 major companies based within 100 miles of a city \\'s center . unlike the stocks cited in the accompanying story , those here are often not rooted in their local economies . many , in fact , have multinational operations . ranking is by the index changes over the latest year , in most cases ending on june 30 . table  by jordan e. goodman   [SEP]',\n",
              " '[CLS]  section : clothes that work american history 501  samoans have lavalavas . greeks have pom-poms on their shoes . japanese have kimonos . austrians have those chico marx hats but with deer fur pins . out of our melting pot and our push west came the clothes that are a part of the american image .  if you could take the hand of an angel and fly back to your own best day , it is doubtful that you would be wearing a three-piece suit . no , i do n\\'t mean your wedding day , but the day you felt yourself most comfortable and full of purpose . chances are , you would have been wearing a pair of jeans . blue jeans , levi \\'s , dungarees , denims : cotton work trousers of heavy blue drill that gain comfort and character with age .  downstairs , i have a drawerful of old jeans that i ca n\\'t bear to throw out . jeans live with you so intimately that they take on a life of their own . have n\\'t you ever           attending a meeting , when a pair of jeans , hanging in the closet with a ranger belt already looped through , stopped you : whoa , pilgrim . look out that window . that \\'s some day , right ? hang those clerking pants back up and let \\'s get out of here .  in 1853 , a bavarian immigrant named levi strauss , an astute merchant in san francisco , responded to the gold-rush need for tough miner \\'s clothes . he had his stock of brown cotton tent canvas run up as plain trousers , no belt loops and no back pockets . a cinch \\' belt in the back kept them up . scrabbling among too many rocks and too little gold , crawling along shafts , wrestling timber supports and balky dray mules , strauss \\'s \" overalls \" lasted . they \\' were cheap and they felt good .  strauss switched to denim ( from serge de nimes , a twill made in southern france ) and had it dyed in reliable , uniform indigo . by the 18605 , levi strauss \\'s           cattlemen throughout the west . in 1873 he bought , for $69 -- the price of the patent application -- an idea from a russian immigrant tailor in reno for making miner \\'s pants stronger by riveting the critical seams . they were nicknamed jeans after the city of genoa , where sailors wore blue cotton canvas .  by 1880 , the levi was full-blown , with orange stitching ( including the trademark \" arcuate \" design across the back pockets , once the functional anchor for pocket lining ) , bar tacking , rivets , watch pocket and the \" two horse \" leather patch . lot numbers are assigned to products and , for the 01-weight denim used , the \" waist-high overalls \" are called 501s . it \\'s true ; more so than most of the thin ghosts we call up for our heritage , levi \\'s are rooted in the real stuff .  henry david lee was another kind of merchant . he started out in ohio selling kerosene and moved west to salina , kansas , with a small bundle of venture capital           and offered a line of eastern work clothes . when shortages and shipping did n\\'t suit henry david , he set up his own garment works , producing overalls , jackets and dungarees . dungarees refer specifically to cotton drill pants without bib fronts , and generally to the rough blue cotton cloth named for the dyer \\'s section of bombay -- dungri -- where it originated . lee \\'s chauffeur probably came up with the lee union-all , a denim coverall that became the uniform of mechanics and other workers in grimy environments . later , it evolved into the flight suit .  in the 1920s , about the time lee was introducing the first zipper fly , levi strauss was deleting the crotch rivet . chafed horsemen had pressed the company for years to remove it , but it took a fly-fishing trip by the chairman of the board to do so . as he crouched near a campfire listening to a story , that central copper rivet heated up nicely . the chairman bolted upright -- and the rivet went . later , with the universal acceptance           , dining room chairs , saddles and car fenders became extinct  jeans do more than cover your body . they hold you . they support and comfort , they remind you that you are girded for the struggle . putting on jeans makes a rough morning easier .  they take your measure and keep your faith . jeans mold to you and become yours alone . if you eat too much , they tell you , hey , back away from the trough , hoss , you \\'re straining the measure . their blue color , in all its variations , suits any kind of day . they look fine over bally loafers or chippewa workboots , under a redskins sweatshirt or a harris tweed jacket . they are easy , unstrained , unpretentious . they are egalitarian ; it is a severe test of thomas jefferson and not of jeans that i can not picture that gentleman farmer in a pair of levi \\'s .  between lee riders and levi \\'s you must make your own choice . ( land \\'s end , l.l. bean and others           authentic . lee claims a better-designed crotch . good . levi \\'s are undoubtedly the original . good . lee pays great attention to women \\'s fit and makes six grades of embrace from baggy to epidermal . good . i , would no more suggest a preference in this matter than i would suggest whether you wear boxers or briefs .  there is a down side to jeans . all that holding and comforting can , like an over-attentive spouse , get in your way . the very cling that you can lean against takes effort to overcome . jeans are not the best climbing gear . straining your knee up to and past your belt , sometimes necessary in scaling a peak like the devil \\'s needle , wears on you after a few hours of moving vertically . in addition , jeans can be hot , so they may not be the prime choice for heavy work in the sun .  we go back to our heritage . who works hard in the sun ? the farmer . what does he wear ? bib overalls .           their own but it takes a big man to wear bib overalls . i happen to be wearing a pair now . there is much to recommend in these oshkosh b\\'gosh bib-front blues . the bib is probably a vestige of protection for walking through fields of corn leaves , or a response to the railroader \\'s need to lean against and over greasy machinery . unlike jeans \\' gun-belt tightness around the center of gravity , bibs and their suspenders have a looser , more general embrace . like the bedouin burnoose , it promotes circulation . bibs are designed around a walking , stooping , reaching man , rather than around a riding man .  when wearing jeans , you can carry some folding money ( \" keep the change , i \\'m wearing jeans . \" ) , a bandanna , a stockman \\'s knife that wears through the pocket in about 10 minutes , a pocket watch that can withstand several atmospheres of compression , and a note from the foreman . a wallet in the hip pocket looks like a misplaced pacemaker and galls the           \\'ve got room for a socket set and a desk enyclopedia . i \\'ve got pockets down there for folding carpenter \\'s rules , loops here for hammers , buttonholes up here for my railroad watch fob ( engineers and brakemen always wore bib overalls ) , pockets toward the rear for bandanna and notebook . these things feel good , too ,  the only down side is that i catch myself in a passing mirror a few times a day and do a classic double take . at first glance , i look like a cartoon . at second glance , i look like a refugee farmer . but careful examination reveals a confident man concerned with his own comfort and cargo capaciy , a man who is no slave to fashion . it takes a big man to wear bib overalls .  oshkosh b\\'gosh turns out bibs for men , women and tiny children in several colors and in their \" hickory-striped \" railroad blue and white . they also make \" painter \\'s pants \" : work pants with a free and easy cut . the           of stories from the manufacturer about inhabitants of bib overalls who fell from trains , horsedrawn harrows and quarry lifts only to be saved by the strength of their oshkosh b\\'gosh garments . no stories exist about all the switchmen and combine operators who were standing about musing on whatever when a passing freight or harvesting arm grabbed them by the bibs and took them away into that eternal silence . i \\'m not sure i would operate the kind of machinery that warns against \" loose clothing \" while wearing bib overalls .  the clothing industry is braced for the wave -- soon , this reporter is informed -- of bib overall fashion . nato has been braced against a similarly foretold wave for 40 years . i like bibs ; they are comfy and commodious and they make a jingling noise with their metal fittings when i walk . i do not think , though , that the image of the farmer -- as honorable and essential as he is -- will supplant the headier concept of the cowboy as the quintessential american male . even in deep           the night air over plowed fields , the plowman changes out of his bib overalls and into his 50is , his cowboy boots and his go-to-meeting high-roller hat before he heads for sally \\'s crossroads tavern . we can all use a little heritage on a good night .  photo ( black &; white ) : levi strauss , king of denim  photo ( color ) : modern jeans ( above ) are variation on the original blue theme .  photos ( color ) : today \\'s canvas trousers ( above ) are close in spirit to levi \\'s first miners pants , worn ( left ) by miners in placer county , california , in 1882 .  photo ( color ) : oshkosh b\\'gosh still makes classic bib-front overalls .  photo ( color ) : as sturdy as denim but less publicized ...  photo ( color ) : ... canvas pants are comfy after a wash .  photo ( black &; white ) : today \\'s canvas trousers ( above ) are close in spirit to levi \\'s first miners pants , worn           , in 1882 .  by jan adkins   [SEP]',\n",
              " '[CLS]  section : movements from socialist to republicans , a generation of student activists test the limits of volunteerism .  university of houston student lloyd jacobson was organizing for a sleep-in outside a local homeless shelter and invited the head of the campus college republicans to join in . \" do n\\'t you want to be part of bush \\'s thousand points of light ? \" jacobson asked , and quickly got the student to agree . others in the republican group hesitated , but reluctantly came along . jacobson next enlisted the college democrats : \" they \\'re participating . are n\\'t you going to as well ? \"  the houston students initially regarded homeless people as faceless and disposable . but they were unnerved by what they witnessed while trying to sleep on cardboard sheets on the hard , cold ground . huddled together , they gingerly began talking with shelter residents . the young republican coordinator returned to write an editorial for the campus daily paper , in which he said he was amazed to find the homeless had comprehensible thoughts , and that           relying on \" worthless government bureaucrats , \" something clearly needed to be done . \" you can not possibly know the situation , \" he concluded , \" until you open your eyes . \"  everywhere , formerly apolitical students have begun to stream into neighborhood soup kitchens , shelters , and tutoring campaigns . at some campuses , volunteer service programs have grown tenfold in the past few years , placing as many as a thousand students a week in community projects .  it \\'s easy to dismiss these students as gushy candy stripers , or to chide them for not re-creating the acts of those who rebelled twenty , years ago at berkeley or columbia . but by breaking students out of insulated worlds , setting no bars on who may participate , and demanding that individuals take seriously the lives of those they hope to assist , the community-service movement has the potential to create the broadest base for campus activism in years . if the past is a guide , these seemingly innocuous charitable involvements may even produce later commitments that could shift the           service movements began by making an end run around prevailing barriers to political involvement . most students mistrusted campus activists , believed solutions to society \\'s major problems were individual , not structural , and felt they could have little influence on key public issues . service participants instead performed immediately useful tasks that did not force them to challenge entrenched institutional authority . these efforts still remained a marginal campus presence in 1984 , when a group of recent college graduates launched the campus outreach opportunity league ( cool ) as a national vehicle to promote student community involvement . angered by media stereotypes that called his generation uncaring , cool cofounder wayne meisel undertook a fifteen-hundred-mile walk from maine to washington , d.c. , where he stopped at campus after campus to spread the message of commitment . cool grew , by 1989 , to a six-hundred-school network .  when i first talked to cool \\'s current executive director , twenty-six-year-old julia scatliff , she said the movement \\'s task was \" to help the person lying bleeding in the street , to put our hands over the           larger issues , scatliff said , up to the nation \\'s leaders , who would respond when they heard sufficiently eloquent testaments of need .  scatliff recalled how author and homeless-rights activist jonathan kozol had praised the students cool worked with by equating them with the founding generation of vietnam-era sds activists , and how she \\'d bridled at the comparison . \" we \\'re different , \" she said . when l asked how , she answered , smiling , \" we have macintoshes and modems , \" then added , \" we live our beliefs . if we say something , we back it up . if we talk about housing , we get involved with housing . \" scatliff stressed cool \\'s authentic commitment , as opposed to the radical posturing that she associated both with her 1960s predecessors and today \\'s campus politicos .  commitment and authenticity , however , do not always create social change . at a recent conference , one stanford student explained how he \\'d learned more from volunteering than from all his courses in school , then concluded , \"           have the same experience working in the same homeless shelter that i did . \"  from its beginning , movement participants have alternated between a fear of taking on explicitly political challenges , and a sense that they can not allow the conditions they address to continue . the resulting tension expresses itself in a variety of ways . one group of dartmouth students volunteered eagerly in the soup kitchens , but wanted nothing to do with an oxfam fast for hunger , considering it \" too ideological . \" a young woman at a small north carolina college began an innovative mentor program for pregnant teens , yet feared that if she worked to shape larger policy , she would get burned out . \" i do community service for myself , \" she concluded , \" because i have a passion for it . i ca n\\'t save the world . \"  this vision is limited not only by the students \\' own wariness , but by praise from the nation \\'s dominant elite . the movement can seem a neat fit with the bush administration \\'s           volunteer action award , and a congratulatory telegram from barbara bush . following lengthy internal debate , cool also accepted a 1988 grant from coors , whose owners are major funders of the new right agenda .  yet distinctions between political and service efforts continue to blur . in march 1988 , twelve university of minnesota students drove down to brownsville , texas , to spend a week as participants in one of the alternative spring breaks that have recently proliferated across the country . they visited the gulf coast beaches , but also tutored kids , rebuilt an old woman \\'s burned house , and stopped at a shantytown . later , they met three teenage refugees from honduras , the last surviving males from their village , who described seeing friends killed by the military or ground down by poverty . the students concluded their week with a visit to the nearby sanctuary center , casa oscar romero . when the refugees brought out guitars , the students led off with the only songs they could think of , the themes from gilligan \\'s island and the brady           with loss , offered their own ballads , about homes left behind and their hopes for justice .  a few days after the students arrived back home , minneapolis residents demonstrated in opposition to governor rudy perpich \\'s decision to send the minnesota national guard to central america . nearly all the students who had visited brownsville were there .  at a cool-sponsored leadership summit this past summer , some students wrote a series of questions for the ad hoc newsletter : \" is community service an end itself , \" they asked , \" or is it a means for social change ? \" \" how do we reconcile the tension between those who do n\\'t want to be forced into politics \\' ... and those who believe that community service without commitments to long-term structural change does more harm than good ? \" \" given our diverse backgrounds and motives for doing what we are doing , what is it that brings us all together ? \"  they are questions that go to the heart of the student service movement \\'s growing pains . during cool           \\'d worked in surrounding bronx neighborhoods sparked bitter debate when they insisted that service activists must begin taking political stands . they influenced the organization to later endorse its first national rally , the october 1989 washington , d.c. , march against hunger and homelessness . at a recent cool gathering , alex byrd , a black junior at rice , said he was tired of all the brochures showing middle-class white kids cradling little black children in their arms , and initiated discussions on race .  responding to these concerns , cool recently presented workshops where black students suggested that white volunteers might have to do things they \\'ve never done : \" approach black colleges , the alpha fraternities , or the urban league . relinquish power and work on someone else \\'s project where you \\'re totally outnumbered . put yourself in the situation a lot of minorities put themselves in every day . \"  by 1989 , cool director julia scatliff explained how her involvement with a racially and culturally diverse group of movement participants had given her a new circle of friends and introduced           a lot in the year since our initial conversation and now stressed her discomfort with the image of \" a bunch of florence nightingales running out to bandage people \\'s wounds . \"  historically , involvement in service efforts has laid the foundation for some of the country \\'s most significant social movements . at the turn of the century , key future new dealers -- like secretary of labor frances perkins , works project administration head harry hopkins , and first lady eleanor roosevelt -- first encountered political issues in their youth , while working in the settlement houses that sought to assist new immigrants . similarly , in the middle 1960s , the young organizers of the sds economic research and action project ( erap ) lived and worked in the ghettos of northern cities , striving to build \" an interracial movement of the poor \" and find what former organizer sharon jeffrey described as \" a meaning of life \" that would be \" personally authentic . \"  if today \\'s student volunteers have a distinct vision , it is that social change requires broad-based           , in the words of one cool staffer , \" you do n\\'t have to believe any eight specific things to be part of the club . \" the volunteers are proud that their centers draw in everyone from republicans to socialists .  many of these students have only begun to deal with critical questions of power , conflict , and privilege . some still stake too much on influencing the future elite , and thus downplay any hint of radical dissent . they still need to learn how to organize , how to pressure entrenched bureaucratic institutions , and how to articulate a shared political vision . yet the rest of us would do well to treat their movement as a common challenge , and to take the service movements \\' goal of inclusion seriously .  some students may treat their experiences with the poor as little more than exotic tourism . but , over time , many student participants have come to acknowledge the necessity for political involvements that would have terrified them before . as the service movement gains strength , members can heed the lessons           build a society where such entities will not be needed .  illustration  by paul rogat loeb   paul rogat loeb is the author of nuclear culture ( new society publishers ) and hope in hard times ( lexington books ) . he is writing a book on the political sensibility of today \\'s college students .   [SEP]',\n",
              " '[CLS]   inside a dusty cement-block house with worn linoleum floors , miqueas mishari sat at a long , wooden conference table , his broad , bronzed face distended , drawn-down cheeks pinched into a frown . patting the pocket of a pressed , white , short-sleeved shirt , he seemed oblivious to the bustle oil , organizers and clerks working in the head . quarters of peru \\'s amazon indians . they hurriedly photocopied and passed around news stories-under headlines like the indian war -- that were worrying mishari so . \" es compleja , \" he murmured , staring at the wall . it \\'s complicated reflexive peruvian saying , commonly delivered with a shrug , was about to become the mantra of our journey together into the amazon basin . mishari was divining the odds , trying to figure what the latest news from the jungle would mean for trip . \" this could be tricky , this could be difficult . everybody \\'s going to be on edge . the trip little dangerous . \" which was not what he had had in mind .           way a year ago into the presidency of aidesep ( the inter-ethnic association for the development of the peruvian jungle ) , had expected to play guide for a nuanced story about the amazon , a slice deeper than the usual grim surface trees falling , cultures dying , gold mines bustling . against a backdrop of environmental destruction , mishari wanted to show off his organization \\'s remarkable accomplishments . even in the midst of widespread economic collapse , aidesep is using $1.2 million in aid from the danish government to identify , mark , and gain legal title to thousands of square miles of traditional indian land throughout the amazon basin . indians there are protecting the forest by defending their own rights , he says . mishari hoped a story about the project might educate outsiders about the lives of an estimated 1.2 million indians who live in the amazon , a region widely misunderstood so far only as the \" lungs of the world , \" home to rare animals and exotic plant life . a mass , armed rebellion by mishari \\'s own tribe , the           track , derailing us into the details of a nasty guerrilla war .  trouble had begun last december , when guerrillas kidnapped , and then killed , the ashaninkas \\' grand chief . even during an insurgency that has cost eighteen thousand lives in the last decade , the ashaninka response was so unusual that it drew national fascination . thousands of ashaninka warriors gathered up bows and arrows , &dared; war on the guerrillas , and rousted them from a large section of indian territory . among the ashaninka , including mishari \\'s coworkers , stories with the aura of mythology had already sprung up to explain how indians with bows and hunting rifles had vanquished soldiers with ak-47s . tales of witchcraft , including whispered asides about the use of traditional hallucinogenic herbs against the guerrillas , spiced the accounts of men who claimed to be eyewitnesses .  the sudden antiguerilla offensive initially came as a pleasant surprise to the peruvian army , which has been steadily losing ground . but what came next welcome news : in parts of ashaninka territory , the indians had not           they had gone on it chase out mixed-blood mestizo settlers as well . \" we never thought we would take up arms to kill , but the civilized ones have taught us this savagery , \" the tribe \\'s new leader , or pinkatsiri , announced . \" we have returned to a savage state to defend what \\'s our . \" and then , the ashaninkas took over the city of puerto bermudez , jailed the district commissioner , and began shooting some prisoners to death with their bows and arrows . this journey was getting even more compleja .  from lima , a gray and fetid city where the skies hardly ever open for the rain , the specter , of long-suffering natives , suddenly rising up in frightening bloody fury against all outsiders , seemed fantastical , something from a novel by mario vargas llosa . ( in fact , stories about the ashaninka rebellion crowded vargas llosa , the presidential candidate , off the front pages for several days running . ) the news seemed sure to add thousands more peasants and mestizo settlers to the           peruvian countryside for lima , the capital already crammed to bursting with nearly 30 percent of the country \\'s twenty million people . what the immigrants find here , of course , is a society no longer disintegrating ; it can fairly be said to have already disintegrated .  in taxicabs , cafes , even in government offices , two questions -- from people across the political spectrum-emerged in any conversation : is there a job for someone like me in the united states ? where can i send my kids to get them away from the bloodbath sure to come ? at the ministry of agriculture , one official interrupted her discourse on the law , the environment , and indigenous rights to say she could not bear to leave peru herself , but wanted to send her fifteen-year-old daughter to the united states . would i be willing to take the teenager in ?  even miqueas mishari , so committed to his work as an indian leader , sometimes fell victim to the prevailing desperation . after living in a small made-over garage in lima with his           mishari recently sent his son to school in mexico , and he was thinking about taking a leave from aidesep to look for work in the united states . his information , like that of many other peruvian immigrants , about opportunities in the united states was a bit jumbled he \\'d heard there was good money to be made planting pineapples in chicago .  a few minutes \\' drive from mishari \\'s one-room home , in miraflores , lima \\'s most upscale shopping district , well-dressed young men elbowed one an . other out of the way to trade peruvian intis for dollars , struggling to earn enough to buy a ticket out of the country . this widespread money changing is an informal laundry for the estimated $2 billion in peruvian coca exports each year-an estimated 70 percent of the u.s. supply , and the one remaining strut in the peruvian economy .  henry mancini was in mid-serenade as dinner-time arrived . a group of indigenous women , some in traditional clothes hawkea batman posters , mickey mouse sunglasses , and records , including an album of           a benefit for the people of kampuchea . couples weaved through a maze of restaurants the haiti , don corleone , super parking , and super parking ii . beyond a line of those waiting to see honey , i shrunk the kids , mementos of the amazon were for sale , stretched across the sidewalk in see-through boxes . giant black tarantulas , mahogany letter openers , oversized brown flying beetles , stuffed baby alligators , preserved piranhas with mouths agape , and a display of butterflies , turquoise , magenta , and golden yellow , glinting in the reflection of the neon movie marquee-nature transformed into exotic , tamed trinkets under glass .  \" we \\'re trying to make the world understand , and we can only hope we \\'re in time . all you have to do is see what \\'s happening in the cities and frontier towns-and in the parts of the amazon where indigenous rights have not been respected to see what the tremendous stakes are . but how many can see this yet ? \" haroldo salazar , garrulous and open faced , hoists           calluses of his hands together . he \\'s just left off shellacking mishari with frank criticism the project is behind schedule , why have n\\'t organizers been paid for the past two months , where are the supplies , why has n\\'t mishari been around to help ? but now his face softens as we walk across a narrow , suspended wooden footbridge on our way to salazar \\'s pride twenty acres etched into the jungle a few miles outside the wild frontier town of pucallpa .  these acres make up a demonstration project-sponsored by aidesep , run on a shoestring , and staffed by volunteers from indigenous organizations-that is an ecologist \\'s dream . the curriculum has been taught by salazar , a taut ashaninka farmer , for the past few years , and it includes organic methods ( no use of chemical fertilizers or pesticides ) , an aggressive program of reforestation , and reinforcement for the use of traditional crops and medicines . each year , six indian couples from throughout peru spend a few months meeting together to discuss these issues and to help work the           and what we had , \" explains salazar .  mishari follows behind salazar , relishing the contrast between the rich , composted soil on the project side of a drooping barbed-wire fence , and the adjacent stunted landscape belonging to a large landowner , where five mangy cows graze on scattered patches of grass . \" this border tells the story of our battle , \" mishari says . \" on our side , the land supports us and we care for the land . on the other side , a large patron saps the land of its future , all for the sake of a few ugly cows like those , and then moves on to clear more trees somewhere else . \"  salazar leads us to a large gazebo where colorful wall , sized posters of indian families subjected to a barrage of outside influences are hung across the enclosure . the paintings depict the terrors of communal lands broken up , cultural ties fractured , dependence on the outside world setting in . \" we teach what a lie it is that/surrounded by all the resources           tin , \" salazar says . \" we start needing to pay for things we used to grow , and in place of the things we had -- pure air and clean water-we get a jungle destroyed . \"  at this point , salazar is speaking so quickly , in the style of a testimonial , that it \\'s difficult to/keep up , but it \\'s easy to see why he \\'s so agitated . just as the biological diversity of the amazon environment can not be sustained in tattered patches , so too are the prospects for survival of the indigenous world threatened by fight physical constraints on indian communities . in orbits around us , large-scale timber development , cattle ranching , coca growing , and mining are destroying the amazon basin -- in peru alone , at the of 700,000 acres rate each year .  just an hour by dugout canoe from aidesep \\'s demonstration project , the village of san francisco is surrounded by colonists and a factory , one thousand people squeezing into just a few acres per person , far too little to           francisco know their fates probably lie in joining the great tide of immigration , first to frontier towns , then on to lima , and perhaps even out of the country . \" we are at the end of the line . there is no room for the children to grow up and stay here , \" complains the village chief .  gaining communal titles to larger indian communities would slow massive migration to the cities , where widespread unemployment , prostitution , and drug abuse await peru \\'s next generation of displaced indians . titling larger communities may even be the key to survival of the indigenous world , according to salazar . bouncing along a rutted dirt road near the demonstration project , in the back of a pickup , he is bearing down on this subject . \" i grew listening to the stories of my mother , \" salazar says , staring straight ahead now , as if in a trance . \" she used to tell me my tribe \\'s ritual myths . about the land when it was pure ashaninka . i remember her           e were . how jaguars roamed peacefully through the territory . how no invaders came or , when they came , they were sent away . so growing up i heard these stories . when i entered the world , it had changed , of course . that world was gone . lots of invaders . no more territory that was pure ashaninka , and not much calm . no jaguars .  \" but still , for me those things are real . what keeps them alive ? those stories , and my people \\'s history , are alive in these trees and in those birds crying in the treetops . this jungle keeps the stories , and everything i was told about , alive . \"  near salazar \\'s demonstration project , though , an army base , lumber mills , factories , and the city \\'s streets -- the products of thousands of people pressing into a frontier town -- slash into the vermillion soil and leave jagged scars . from the air in mid-afternoon , in a small plane streaking away from pucallpa south toward atalaya           jungle has been tamed , used up , and discarded . gradually , the scars give way to unbroken forest . deep , blackish greens are threaded by rivers that wind back on themselves in elegant curlicues . the sensation of color suddenly intensifies , as if a gray film has been lifted from the land . clouds at eye level loom like spun confections : tigers , toucans , and old bearded men .  back on the ground in searing heat , the sputtering whir of the plane \\'s engine is replaced by a deafening cacophony of insects . along rutted dirt trails , we trudge past clapboard storefronts , the shopkeepers and lumbermen glaring at mishari , murmuring curses and threats . sweat is dripping in sheets off mishari \\'s back as he totters under the weight of his bag and a large cardboard box holding a gift for his nephew : a television set , as it turns out . there are no telephones in atalaya , no automobiles , neon , or faxes ; but after the sun go , is down , televisions glow from           on our first night in town to watch tv , clustering in the cafes to watch peru \\'s favorite shows . one is peru eres -- fantastico , a cross between wheel of fortune and let \\'s make a deal , in which contestants are swept off their feet while trying to gob money blown at them through vacuum tubes . grand-prize winners from another humiliation can walk away with $150 -- and trips for two to miami . ( on this night a woman from cuzco comes close to clinching the trip . )  next up is a strap opera about the smart ways of city-wise mestizos , attributing unbelievable stupidities to darker , rural people . in the shows dramatic denouement , a mestizo doctor rushes in to save the life of a young man on the verge of death , to the audible relief of the town \\'s audience , slapping at mosquitoes and sitting on edge . but as the show fades , to a political commercial promising a \" great change \" with marlo vargas llosa , an indian lumber worker can be heard moaning           the tv , seeming not  julio is a teenager , having returned from harvesting mahogany trees a few days \\' hike away . he says no food was provided by the lumber foreman , who made meals for himself but forced his indian workers to go hungry . julio had survived by scavenging and eating worms . but something had gone dreadfully wrong . julio \\'s stomach has ballooned , as though he had a termite colony inside his belly , and he is in agonizing pain . but without a doctor , or even a curandera ( healer ) , in atalaya , julio does n\\'t stand a chance . along with bottles of rum , curious onlookers grace his bedside , sharing the scorching pain of being powerless to comfort or save his life , as julio calls out a few times for his mother , lapses into unconsciousness-and dies . outside , a whir of cicadas covers the shuffling sounds of preparing the body for burial .  on a patio across the way , a coffee grower is complaining loudly about the backwardness of the local           for the natives . and them not knowing what to do with it ! \" he says , turning his palms up . he talks about the tremendous contributions of the colonizers to the town , the bravery they \\'d demonstrated in moving to the frontier . \" people warned me against coming here , \" he points out . \" they said there were jaguars and alligators . but , look , i \\'ve been here a dozen years and i \\'ve never seen a jaguar . \"  a local lumberman , tapping his forefinger on the table , adds that the world has recently been turned upside down . with this crazy land-tiffing program , he says , indians are now reluctant to work for others , demand high wages , require payment before they work , and often do n\\'t complete their jobs . he mutters that peruvians would n\\'t \" even be on horseback yet if the indians had their way , \" and that indians are responsible for the insurgency . \" it \\'s like a dog with rabies , \" he says firmly .           cure a dog with rabies ? \"  this kind of talk makes the coffee grower nervous , particularly with an outsider present . \" let \\'s acknowledge that everyone is in a squeeze , \" the grower counters , jovially . \" conflict is n\\'t good for anyone . we \\'re all in quite a fix . without a road , we ca n\\'t get our crops to lima . so while rice and corn are stacked up all over atalaya and in every hacienda around , the government is importing rice from china . buying rice ! from china ! and not from us . \" the coffee grower smiles and lays his arms across the table , defenseless . he pivots toward the outsider . \" once i turned down a chance to go work in europe . do you think there is work for me now in the united states ? \"  in a hotel room within hearing , just past an adobe wall surrounding the coffee grower \\'s garden , mishari is splayed out on his bed , his hands beneath his chin and a           a letter accusing him of being a drug trafficker and a guerrilla has been received by the local army commander , but he greets this news placidly , as if it \\'s just another day at the office . when he first trekked into atalaya four years ago , mishari discovered hundreds of ashaninkas working in slavelike conditions for large local landowners , and he helped organize a national campaign against the practice . then , too , he \\'d been threatened on the street . \" at the time i did n\\'t walk through the town , even in daylight , \" mishari says , smiling and shaking his head as if marveling at the mischief of a child . \" now it \\'s better , because over the years these people have seen how many of us there are and how we stand together ... it \\'s still possible that the patrones could hurt us , or even kill . but i doubt they will . \"  mishari muses about his own home village and begins recalling , in a murmuring singsong , a series of stories told           stories about ashaninka lore , which he first heard as a child laboring in harsh conditions on a plantation with armed guards , come back to him in a wave of nostalgia . it \\'s odd but true , he says , that the ashaninkas have no creation myth . in traditional stories , the ashaninkas have always been , and always will be . their history of survival , of successfully repelling intruders for the past five hundred years , is a common theme .  mishari says :  in ancient times , there was a being named aviriri . he was old and bent , and be carried his grandson on his shoulder wherever be went . he never looked up . but when his grandson would ask him , \" granddad , what \\'s that ? \" be answered : \" that \\'s a heron . \" \" that \\'s a jaguar . .... that \\'s a mahogany tree . \" this is bow the animals and things of the world were named .  then one day , the spanish invaded . from across the cerro           . so be said , \" granddad , there are men there , it looks like they are armed . what are they ? \"  since aviriri was like a god , he could bring something into being by imagining it . so he said , \" those are soldiers . they \\'re invading our land . so we \\'re going to change them . . . into black rocks . \"  in my country , near the perene river , there is a big black rock . when i was growing up , people would place offerings there , sometimes just chewed coca leaves and sometimes more . that was a tribute to aviriri . if something bad happened to you , people would say , \" ah , ha ! you have forgotten to pay tribute to aviriri . \" then , in the 1950s , the evangelists came and convinced people that this was just an idol , that aviriri was superstition . and that \\'s bow the belief in aviriri died out .  near wehre the urubamba meets the tambo , two chocolate-colored rivers           loaded so heavily that each time the sputtering engine chokes , water spills over the sides and soaks our feet . as two of our guides shout at one another , shifting their weight for emphasis -- \" it \\'s that way . .... wrong , it \\'s this way ! \" -the streams become a minor flood and several men begin to bail . the canoe \\'s owner remains impassive , setting a third course , tucking into a hidden tributary and accelerating flat out for a remote indian village called tahuanti .  mishari , perched on the bow of the canoe , peers anxiously over his shoulder and squints into the dense , turquoise foliage along the riverbank . he \\'s on edge , his head swiveling like an owl \\'s , on the lookout for townspeople , army patrols , guerrillas . as we pass a family of refugee indians from the highlands , dressed in traditional clothes and paddling downstream , mishari puts a sharp point on all the talk we \\'ve been hearing from local indians about the threat posed by intruders . these highlanders           townspeople , and mishari says they will be ejected from ashaninka territory as well . but surely they are as needy and desperate as anyone . what will happen to them ? \" we hope they will learn from our example , form a community somewhere else , and file for title from the government , \" mishari says .  after we dock , mishari heads up a beaten trail to tahuanti , trees and shrubs dwarfing him , his shoulders beginning to droop , relaxing now . as he passes through communal cropland-acres of cassava , rice , corn , and sugarcane-his surface submissiveness vanishes in expansive gestures , arms akimbo , his mouth puckered to issue an insistent , shrill whistle , which echoes deep into the jungle . he stops to knock down a few papayas from trees in a lush grove , splitting them with a machete and passing slices around to share .  beyond the papaya grove , small canoes traverse a creek loaded with river shad , men and boys spearing their catch ; women and girls gather the fish and carry them           arrow from his bodyguard and jogs to the water , playful as a five-year-old . in his excitement , he slips and falls in , shoes and all , and takes aim , striking and missing , striking and missing again . \" i \\'m a good marksman . it \\'s only these bad arrows that are to blame , \" he jokes .  that comment draws a laugh , as it is translated from spanish to ashaninka and passed from one villager to another , gathered in a row at creekside . tahuanti \\'s leader , a young , sleek man named seminario marinero camaitei , stands a few feet from the others , shifting his intent gaze from one visitor to the next , as if trying to sense whether anyone brings danger to the village .  underneath a hut on stilts , fish laid beneath palm fronds smoke on an oblong grill . in the open-air kitchen above , women are stirring masato , a brew made from boiled , fermented cassava . mishari sits at the table in a place of honor , while seminario           on the floor between the common room and the kitchen , a mentally retarded girl rolls from side to side-clothed , tended , and emitting periodic squeaks . bowls of fish soup , plates of boiled yucca , and gourds full of masato crowd the table as seminario delivers brief news flashes , as if he were a town crier : \" the colonists are getting out of hand ! they \\'re invading our land ! we \\'ve got to figure out a way to get them out ! \"  lima now seems placed on the wrong end of a telescope , the capital \\'s intertwined problems so remote . no one in tahuanti expresses the slightest interest in the capital or anyplace else beyond the village ; there are no questions about where we \\'ve been or what we \\'ve seen . having visited the nearest town , full of drunks , prostitutes , and angry men , seems contact enough with civilization for the people of tahuanti . \" lots of noise , lots of tin , lots of trouble , \" seminario says of atalaya . here           him in the outside world . it \\'s the only conversation i \\'ll have in peru that does n\\'t end with the anxious question , \" do you think there is a job for me in the united states ? \" or the frightened whisper , \" i \\'m scared . what \\'s going to happen to my kids ? \"  after lunch , seminario mentions tentatively that his father \\'s older brother ( \" we do n\\'t know how old he is , he \\'s probably ninety or so \" ) lives a few hours \\' hike away , near the hacienda where he \\'d worked for more than seventy years . sebastian knows more about the history of the village , and the tribe , than anyone else . he was born on the land where he now lives . would we like to meet him ?  by the time we set off into the jungle , we \\'re followed , and flanked , by twelve escorts , barefooted boys and young men wearing cloth briefs , their chests bare , several with bright red achiote daubed           over us , we trudge for an hour , the thin film of swamp water seeping through our shoes , vines ripping at our legs , twisting and turning , leaping over fallen logs , as the trail grows less and less distinct in perpetual twilight beneath the jungle canopy .  after a two-hour trek and canoe ride , with seminario and his brother poling through a creek that winds around massive tree trunks , uncle sebastian , the elder of tahuanti , is waiting . sebastian \\'s skin has slipped from the bones and he falters twice as he shuffles out of his hut to greet us . he \\'s welcoming , but not eager to be interviewed , especially since mishari has to translate into -- and out of -- ashaninka . but sebastian wants it known that he worked for a big landowner as a virtual slave for many decades . slashing his arm toward the thousands of acres of land adjoining his small plot , sebasti5n says : \" that man did nothing but sit all these years . i did the work . and now           title to the land that should be ours . \"  sebastian \\'s point punctuates a subject now being forcefully debated among indian advocates in the united states and europe . preservation of the amazon is often seen as a fight to resist change , stop development , and save the \" lungs of the world . \" but , as mishari pointed out on our way to sebastian \\'s hut , preservation of the amazon is not as simple as preserving the status quo . for many indians , maintaining the status quo , even fighting to simply conserve as much of the amazon basin as possible , could actually mean maintaining the oppression of centuries .  those who work most closely with aidesep complain that this idea seems beyond the grasp of even the most well-meaning outsiders . soren hvalkof , the danish anthropologist who oversees his government \\'s support for the land-titling project , says : \" it frustrates me that the same bastards who conquered the amazon , and dominated it , now call for conserving it , for keeping the status quo .... it frustrates           , should accept the status quo . if the indians stand up , as at tahuanti and in most of the amazon basin today , make political demands , wear manufactured clothes , sell surplus food for cash , they risk losing their cachet with outsiders . the indians are up against a philosophy , the classic division between god and satan , object and subject , humans and nature . we tend to accept them only as long as they remain savages . \"  \" we need room , \" sebastian is saying , insistently . he looks around at the men and boys who have accompanied us and points at them for emphasis . we need to protect these children , and their children , and the children after that -- or our whole world comes to an end . \"  asked to add to mishari \\'s collection of ritual ashaninka myths ( does he remember the traditional stories of his childhood , about great eagles and wild jaguars ? ) , sebastian chuckles , setting off a round of laughter , which echoes through the encampment           . \" we heard many of those things over the years . from my own people , my parents , i heard that gods were inside the trees and birds , in the sun and moon . those were our gods . and then came intruders with different ideas . they said there was only one god , one man , way up there , all by himself . they said we should worship him . but i did n\\'t believe it . and i do n\\'t believe any of what i learned before . did the birds protect us ? did their god protect us ?  \" no , i do n\\'t believe any of that anymore . i do n\\'t believe in spirits , in gods in trees or birds . i do n\\'t believe in god . i only believe in these young ones , and in what they will do with this land . \"  sebastian stops abruptly . just as suddenly , seminario swivels toward the patron \\'s grazing land , which will soon be part of his village if mishari has his way           . great gobs of words tumble out , bitterness so sharp and anger so pungent that the other men back away from him as he begins growling . seminario \\'s tempo picks up and he punches the air , shouting at the missing landowner . his uncle sits with a slight smile , nodding approvingly . seminario calls on the power of the forest to help expel all invaders , demanding the power to do what \\'s just and right . it \\'s a glimpse of the kind of fury that drove fellow ashaninkas in puerto bermudez to take up their bows and face down soldiers with machine guns . \" the colonists are out of hand ! they \\'re invading our land ! we \\'ve got to get them out ! it \\'s ours ! we want them out ! what we want is this : the land must be pure indigenous . pure ashaninka . pure ! that is all ! es todo ! \"  seminario \\'s fierce tirade , full of grievance and venom , has gone on for several minutes . but having reached his conclusion           a switch , turning back and smiling sheepishly , appearing slightly embarrassed by the passion he has just unleashed . among some amazon tribes , anger like this is avoided as a dangerous thing ; set loose in the world , it could make the skies tumble and the sun fall . seminario steps forward , bending in a courtly way and quietly , politely , asks , \" are you ready to go back now ? \" pablo cabado was selected 1989 young photographer of the year by the international center for photography .  photo ( black &; white ) : surrounded by massive environmental destruction , amazon indians have a new plan to save their river and themselves .  photos ( black &; white ) : six million people are crammed into lima ; millions are unemployed in frontier towns .  photos ( black &; white ) : mishari on his way to tahuanti ( left ) ; village elder sebastian resting in his hammock .  photos ( black &; white ) : swimming in the urubamba river near tahuanti ; an ecology project near           is the editor of mother jones . special thanks to richard smith of oxfam america , in lima , for his assistance .  debt for nature : dirty deals ?  evaristo nugkuag has been standing up to intruders in the amazon basin ever since 1979 , when his tribe ran filmmaker werner herzog out of the area and burned his encampment to the ground .  \" he wanted to make this movie about fitzcarraldo , \" nugkuag remembers . \" but we knew that this project would bring outsiders in-delinquents , prostitutes , liars .... also , fitzcarraldo was a landowner who victimized indians to bring his boat across a mountain , and we did n\\'t want to glamorize him . \"  nugkuag is now president of coica , coordinating body for the indigenous peoples organizations of the amazon basin , a federation that represents indigenous amazonians from five countries . it \\'s not widely understood that while most of the amazon land mass is in brazil , only 300,000 of an estimated 1.2 million indigenous amazonians live there . the rest live scattered along the headwaters           , peru , and bolivia .  from coica headquarters on a quiet residential street in lima , peru , nugkuag monitors the federation \\'s work , traveling often to stitch together traditionally separate tribal cultures , spreading news about widespread efforts to protect indian culture by safeguarding indian land . he is most concerned now about a new wave of well-meaning intruders : large environmental organizations out to preserve the amazon . nugkuag sharply criticizes groups involved in \" debt-for-nature swaps , \" arrangements in which organizations buy a portion of a country \\'s debt from international lenders , and then negotiate to create parks or environmental preserves .  \" the environmentalists talk a lot about butterflies , fish , animals , and trees , \" nugkuag says . \" but in their view of the amazon biosphere , they do n\\'t take human beings , indigenous peoples , into account . we are part of the ecosystem , and our ancestors are the ones who protect its resources . and if we \\'re thrown out , who is going to defend the amazon ? national parks are not           a park created by law can also be done away with by law . this has nothing to do with indigenous peoples-just like the debt-for-nature idea .  \" we think that instead of debt for nature , we should be talking about debt for indigenous control -- creating large new extensions where indians can live , to protect our culture while also protecting the land . \"  gaining support from environmentalists is particularly important as coica \\'s battles cross national borders . \" the common problem we all have is controlling our land , \" says nugkuag . \" we need to safeguard the land and develop in our own way . we do n\\'t want sophisticated technology imposed from the outside . we know how to develop with appropriate technology , in our own way , at our own speed , using minimal resources and causing the least damage to the environment . \"  nugkuag \\'s program for the year ahead includes :  -- in brazil , supporting the efforts of the yanomami tribe to remove gold miners who have wreaked havoc on the environment ;           of one-half of the country \\'s amazon basin , recently ceded to them by the government ;  -- in ecuador , supporting the effort to stop the construction of a conoco oil pipeline through indian land ;  -- in bolivia , supporting efforts to secure government recognition of indigenous territorial rights to the chimanes forest , where five thousand indians are fighting lumber companies for control of the land ;  -- in peru , fighting for the land titling program to continue after the inauguration of a new president this month .  in werner herzog \\'s film , eventually shot in 1981 near the headwaters of the amazon river , klaus kinski played fitzcarraldo , a pioneer who risks his life in efforts to build a railroad , produce ice , and stage an opera to uplift the natives who populate a jungle \" full of lies , demons , and illusions . \" while fitzcarraldo \\'s motivations-symbolized by his attempts to drown out the sounds of indigenous drums by blasting a caruso recording of aida into the jungle-are made clear in the film , the indians           , the backdrop for an adventure story , but not the main event .  nugkuag says outsiders have always treated amazonians that way , alternately exploiting them and romanticizing their experience-sometimes doing both simultaneously . now that there is a federation to speak directly for amazon indians , he hopes there will be more indigenous control over the ways in which outsiders attempt to save the amazon . nugkuag says : \" they \\'ve treated us as if we did n\\'t have a brain , a mouth , eyes , ears . but the world is about to find out that we can see , hear , and that we can speak . outsiders are going to have to deal with us directly from now on . \"  for more information about coica , contact : evaristo nugkuag , president , coordinadora de las organizaciones indigenas de la cuenca amaz6nica , jiron almagro 614 , lima 11 , peru ; fax 51-14-423572 .  office in the united states : 1011 orleans street , new orleans , louisiana 70116 ; fax ( 504 ) 522-7454 .  photo (           up to intruders in the amazon basin ever since 1979 , when his tribe ran filmmaker werner herzog out of the area and burned his encampment to the ground .   [SEP]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOLRljp3gr9T",
        "colab_type": "code",
        "outputId": "a3f9cfd7-894e-4191-d758-34249bc65cfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "print (\"Tokenize the first sentence:\")\n",
        "print (tokenized_texts[0])"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenize the first sentence:\n",
            "['[CLS]', '\"', 'bum', '##s', '.', '\"', 'that', \"'\", 's', 'what', 'radio', 'havana', 'called', 'them', '.', 'and', 'indeed', ',', 'some', 'of', 'the', '125', ',', '000', 'cuban', '##s', 'who', 'fled', 'to', 'the', 'u', '.', 's', '.', '10', 'years', 'ago', 'were', 'thugs', 'and', 'hood', '##lum', '##s', '.', 'but', 'not', 'photo', '##jou', '##rna', '##list', 'eduardo', 'suarez', '.', 'his', 'is', 'a', 'classic', 'american', 'success', 'story', ',', 'even', 'though', 'his', 'finances', 'are', 'a', 'little', 'out', 'of', 'focus', '.', 'the', 'date', ':', 'may', '22', ',', '1980', '.', 'the', 'place', 'the', 'ding', '##y', 'industrial', 'port', 'of', 'marie', '##l', ',', '27', 'miles', 'west', 'of', 'havana', '.', 'the', 'time', 'early', 'evening', '.', 'there', 'on', 'the', 'docks', 'under', 'a', 'cloud', '##less', ',', 'moon', '##lit', 'sky', 'stood', 'television', 'camera', '##man', 'eduardo', 'suarez', ',', '30', ',', 'his', 'bride', 'of', 'seven', 'months', ',', 'betty', ',', '22', ',', 'and', 'her', '55', '-', 'year', '-', 'old', 'mother', 'rosa', '.', 'taking', 'advantage', 'of', 'castro', \"'\", 's', 'temporary', 'liberal', '##ization', 'of', 'cuba', \"'\", 's', 'emigration', 'policy', ',', 'they', 'had', 'come', 'to', 'marie', '##l', 'to', 'embark', 'on', 'a', 'life', 'of', 'freedom', '.', 'in', 'just', '18', 'hours', ',', 'that', 'new', 'life', 'would', 'begin', 'as', 'it', 'has', 'for', 'millions', 'of', 'immigrants', 'in', 'america', ',', 'at', 'the', 'bottom', 'of', 'the', 'social', 'and', 'economic', 'ladder', '.', 'slowly', 'they', 'filed', 'onto', 'a', '50', '-', 'foot', 'shrimp', 'boat', 'named', 'about', '25', 'people', '.', 'that', 'night', 'it', 'sat', 'low', 'in', 'the', 'water', ',', 'jammed', 'with', '80', 'or', 'so', 'refugees', ',', 'some', 'of', 'them', 'just', 'released', 'from', 'cuba', \"'\", 's', 'mental', 'hospitals', 'and', 'jail', '##s', '.', 'money', '?', 'the', 'suarez', 'party', 'had', 'none', ';', 'it', 'was', 'illegal', 'to', 'take', 'cuban', 'pe', '##sos', 'out', 'of', 'the', 'country', '.', 'belongings', '?', 'the', 'clothes', 'on', 'their', 'backs', ',', 'a', 'blanket', 'and', 'a', 'single', 'pillow', '.', 'hopes', 'and', 'dreams', '?', 'of', 'those', 'they', 'had', 'plenty', '.', 'and', 'the', 'next', 'day', ',', 'after', 'a', 'voyage', 'that', 'began', 'calmly', 'and', 'ended', 'with', 'the', 'passengers', 're', '##tch', '##ing', ',', 'salt', '-', 'stained', 'and', 'exhausted', ',', 'eduardo', ',', 'betty', 'and', 'rosa', 'could', 'begin', 'to', 'act', 'on', 'them', ',', 'initially', 'from', 'a', 'reset', '##tlement', 'camp', 'in', 'a', 'world', 'war', 'ii', 'sea', '##plane', 'hangar', 'in', 'key', 'west', '.', 'there', 'to', 'greet', 'them', 'that', 'steam', '##y', 'friday', 'afternoon', 'were', '3', ',', '000', 'or', 'so', 'of', 'the', '125', ',', '000', 'boat', '##lift', 'exiles', 'who', 'came', 'to', 'be', 'branded', '\"', 'marie', '##lit', '##os', '\"', '-', '-', 'a', 'pe', '##jo', '##rative', 'term', 'that', 'literally', 'means', 'little', 'people', 'from', 'the', 'port', 'of', 'marie', '##l', '.', 'fast', 'forward', 'to', 'may', '6', ',', '1990', '.', 'the', 'place', 'the', 'now', 'abandoned', 'sea', '##plane', 'hangar', ',', 'long', 'since', 'cord', '##oned', 'off', 'by', 'barbed', 'sul', '##try', 'late', 'sunday', 'afternoon', ',', 'is', 'eduardo', 'suarez', ',', '40', '.', 'but', 'this', 'time', 'he', 'arrives', 'as', 'a', '13', '-', 'time', 'emmy', 'award', '-', 'winning', 'producer', ',', 'director', ',', 'camera', '##man', 'and', 'operations', 'manager', 'for', 'channel', '51', ',', 'a', 'spanish', '-', 'language', 'television', 'station', 'in', 'greater', 'miami', ',', 'whose', '60', '##5', ',', '000', 'cuban', 'ex', '##pa', '##tri', '##ates', '(', '31', '%', 'of', 'the', 'population', ')', 'make', 'it', 'the', 'second', 'largest', 'cuban', 'city', 'in', 'the', 'world', '.', 'the', 'intense', 'professional', ',', 'who', 'makes', 'about', '$', '65', ',', '000', 'a', 'year', ',', 'has', 'come', 'to', 'film', 'la', 'calm', '##a', 'des', '##pu', '##es', 'de', 'la', 'torment', '##a', '(', 'the', 'calm', 'after', 'the', 'storm', ')', ',', 'a', 'documentary', 'about', 'the', 'marie', '##l', 'refugees', \"'\", 'fate', '.', 'as', 'he', 'gazes', 'at', 'the', 'hangar', ',', 'the', 'hair', 'on', 'his', 'forearms', 'rises', '.', '\"', 'me', 'er', '##ize', '-', '-', 'i', 'have', 'goose', 'bumps', ',', '\"', 'he', 'says', ',', 'then', 'adds', ',', '\"', 'i', 'came', 'through', 'here', '.', '\"', 'and', 'he', 'has', 'barely', 'looked', 'back', '.', 'in', 'the', '10', 'years', 'since', 'the', 'may', '-', 'to', '-', 'september', '1980', 'marie', '##l', 'boat', '##lift', ',', 'eduardo', 'and', 'his', 'shy', ',', 'practical', '-', 'minded', 'wife', 'betty', 'have', 'built', 'an', 'american', 'dream', '.', 'they', 'both', 'have', 'good', 'jobs', 'he', 'at', 'the', 'tv', 'station', ',', 'she', 'as', 'a', '$', '19', ',', '000', '-', 'a', '-', 'year', 'drafts', '##person', 'for', 'the', 'northwest', 'miami', 'engineering', 'ranch', 'house', 'along', 'a', 'canal', 'in', 'a', 'middle', '-', 'class', 'neighborhood', 'in', 'southwest', 'miami', ',', 'complete', 'with', '33', '-', 'foot', 'swimming', 'pool', ',', 'coconut', 'palms', 'and', ',', 'yes', ',', 'a', 'white', 'picket', 'fence', '.', 'cost', ':', '$', '86', ',', '000', ',', 'financed', 'four', 'years', 'ago', 'with', 'a', '30', '-', 'year', ',', '9', '.', '5', '%', 'fixed', '-', 'rate', 'mortgage', '.', 'they', 'have', 'a', 'high', '-', 'spirited', 'and', 'independent', 'two', '-', 'year', '-', 'old', 'daughter', ',', 'andrea', '.', 'and', 'they', 'have', 'all', 'american', 'aspirations', 'a', 'parochial', '-', 'school', 'education', 'for', 'andrea', 'followed', 'by', 'the', 'college', 'of', 'her', 'choice', ',', 'as', 'well', 'as', 'financial', 'security', 'for', 'themselves', '.', 'but', 'to', 'realize', 'their', 'aspirations', ',', 'the', 'suarez', '##es', 'will', 'need', 'to', 'bring', 'their', 'finances', 'into', 'sharpe', '##r', 'focus', '.', 'like', 'many', 'immigrants', ',', 'they', 'have', 'been', 'concentrating', 'on', 'making', 'money', ',', 'not', 'managing', 'it', '-', 'and', 'their', 'spending', 'is', 'so', 'out', 'of', 'control', 'that', 'nearly', '30', '%', 'of', 'their', 'income', 'remains', 'una', '##cco', '##unt', '##ed', 'for', '.', 'then', 'too', ',', 'with', 'andrea', 'to', 'look', 'out', 'for', ',', 'eduardo', 'needs', 'to', 'beef', 'up', 'his', 'life', 'and', 'long', '-', 'term', 'disability', 'insurance', '.', 'and', 'he', 'and', 'betty', '-', '-', 'whose', 'english', ',', 'like', 'eduardo', \"'\", 's', ',', 'is', 'spot', '##ty', '-', 'have', 'yet', 'to', 'take', 'the', 'simplest', 'and', 'least', 'expensive', 'step', 'to', 'protect', 'their', 'daughter', \"'\", 's', 'future', 'making', 'a', 'will', '.', 'marie', '##l', 'refugees', '-', '-', 'bel', '##ies', 'the', 'stigma', 'that', 'the', 'cuban', 'boat', 'people', 'bear', '.', 'of', 'the', '125', ',', '000', 'whom', 'the', 'cuban', 'government', 'allowed', 'to', 'leave', 'in', '1980', ',', 'only', 'some', '2', ',', '000', 'were', 'confirmed', 'criminals', 'or', 'mentally', 'ill', 'persons', 'freed', 'by', 'castro', '.', '\"', 'the', 'u', '.', 's', '.', 'has', 'always', 'wanted', 'to', 'pick', 'the', 'best', 'brains', 'of', 'our', 'people', ',', '\"', 'radio', 'havana', 'ca', '##ckle', '##d', 'at', 'the', 'time', '.', '\"', 'let', 'them', 'also', 'pick', 'up', 'the', 'bum', '##s', '.', '\"', 'although', 'a', 'few', 'exiled', 'fe', '##lon', '##s', 'were', 'app', '##re', '##hend', '##ed', 'by', 'u', '.', 's', '.', 'immigration', 'officials', ',', 'most', 'were', 'not', '.', '\"', 'the', 'conditions', 'were', 'too', 'crowded', ',', '\"', 'says', 'u', '.', 's', '.', 'army', 'col', '.', 'juan', 'armando', 'monte', '##s', ',', 'who', 'coordinated', 'the', 'reception', '.', 'at', 'the', 'peak', 'in', 'early', 'summer', ',', 'he', 'says', ',', 'some', '3', ',', '500', 'people', 'were', 'arriving', 'every', 'day', '.', 'as', 'a', 'result', ',', 'within', 'months', 'of', 'the', 'boat', '##lift', \"'\", 's', 'end', 'in', 'september', ',', 'some', 'marie', '##l', 'refugees', 'had', 'committed', 'murders', ',', 'bu', '##rg', '##lar', '##ies', 'or', 'hi', '##jack', '##ings', '.', 'yet', 'many', 'others', 'became', 'model', 'citizens', '.', 'indeed', ',', 'says', 'lisa', '##nd', '##ro', 'perez', ',', 'chairman', 'of', 'sociology', 'and', 'anthropology', 'at', 'miami', \"'\", 's', 'florida', 'international', 'university', 'and', 'himself', 'a', '1960', 'cuban', 'refugee', ',', 'marie', '##ls', 'were', 'driven', 'less', 'by', 'politics', ',', 'make', 'money', 'and', 'put', 'their', 'kids', 'through', 'college', '.', '\"', 'the', 'suarez', '##es', 'are', 'a', 'perfect', 'example', '.', 'eduardo', 'had', 'worked', 'as', 'a', 'television', 'camera', '##man', 'on', 'and', 'off', 'for', 'seven', 'years', 'in', 'cuba', 'in', 'the', '1970s', '.', 'top', 'pay', 'the', 'equivalent', 'of', 'about', '$', '300', 'a', 'year', '.', 'but', 'he', 'was', 'drawn', 'to', 'the', 'u', '.', 's', '.', 'after', 'a', '1979', 'visit', 'to', 'his', 'ai', '##ling', 'father', 'in', 'miami', '(', 'the', 'elder', 'suarez', 'died', 'last', 'march', ')', '.', '\"', 'in', 'cuba', ',', '\"', 'eduardo', 'explains', ',', '\"', 'you', 'had', 'to', 'work', 'closely', 'with', 'the', 'communist', 'party', 'in', 'order', 'to', 'advance', '.', '\"', 'trouble', 'was', ',', 'eduardo', 'never', 'identified', 'with', 'any', 'regime', '.', '\"', 'i', 'just', 'wanted', 'to', 'live', 'up', 'to', 'my', 'potential', ',', '\"', 'he', 'says', ',', '\"', 'and', 'i', 'needed', 'freedom', 'in', 'order', 'to', 'do', 'it', '.', '\"', 'the', 'decision', 'to', 'emi', '##grate', ',', 'though', ',', 'was', 'ex', '##cr', '##uc', '##iating', '.', 'for', 'eduardo', ',', 'it', 'meant', 'leaving', 'behind', 'his', 'mother', ',', 'brother', 'and', ',', 'hardest', 'of', 'all', ',', 'five', '-', 'and', 'six', '-', 'year', '-', 'old', 'edgar', 'and', 'eli', '##anne', ',', 'his', 'son', 'and', 'daughter', 'from', 'a', 'previous', 'marriage', '.', 'betty', ',', 'though', 'she', 'had', 'an', 'older', 'sister', 'and', 'an', 'aunt', 'in', 'to', 'leave', 'her', 'father', 'roland', '##o', ',', 'a', 'contractor', '.', '\"', 'he', 'could', 'n', \"'\", 't', 'even', 'speak', 'the', 'word', 'good', '-', 'bye', ',', '\"', 'she', 'recalls', '.', 'the', '18', '-', 'hour', ',', '104', '-', 'mile', 'ride', 'on', 'the', 'tr', '##op', '##ic', 'dream', 'was', 'peaceful', 'at', 'first', '.', 'in', 'the', 'middle', 'of', 'the', 'night', ',', 'however', ',', 'the', 'boat', 'began', 'to', 'pitch', 'in', 'rising', 'seas', '.', 'betty', 'and', 'many', 'others', 'became', 'seas', '##ick', '.', 'and', 'all', 'were', 'bed', '##rag', '##gled', 'by', 'the', 'time', 'the', 'captain', '-', '-', 'himself', 'a', 'cuban', 'exile', '-', 'turned', 'them', 'loose', 'in', 'key', 'west', 'the', 'next', 'day', '.', 'the', 'captain', \"'\", 's', 'advice', '?', '\"', 'the', 'first', 'year', 'is', 'the', 'hardest', ',', '\"', 'he', 'told', 'them', '.', '\"', 'you', \"'\", 'll', 'cry', 'a', 'lot', 'and', 'wish', 'you', 'never', 'left', 'cuba', '.', 'but', 'then', 'things', 'will', 'get', 'easier', '.', '\"', 'his', 'words', 'proved', 'prophet', '##ic', '.', 'the', 'suarez', '##es', 'moved', 'into', 'an', 'eight', '-', 'foot', '-', 'by', '-', '10', '-', 'foot', 'bedroom', 'at', 'betty', \"'\", 's', 'aunt', \"'\", 's', 'house', '.', 'eduardo', 'found', 'a', '$', '3', '.', '10', '-', 'an', '-', 'hour', 'job', 'as', 'a', 'photo', 'lab', 'assistant', '.', 'betty', ',', 'who', 'had', 'studied', 'drawing', 'at', 'the', 'instituto', 'te', '##c', '##no', '##logic', '##o', 'oswald', '##o', 'herrera', 'in', 'cuba', ',', 'landed', 'a', 'drafting', 'position', 'that', 'paid', 'a', 'scan', '##t', '$', '4', ',', '99', '##2', 'a', 'year', '.', 'both', 'were', 'lonely', 'and', 'depressed', '.', '\"', 'i', '\"', 'says', 'betty', ',', 'who', 'grew', 'up', 'in', 'a', 'comfortable', 'havana', 'neighborhood', '.', '\"', 'for', 'the', 'first', '12', 'months', ',', 'i', 'even', 'missed', 'the', 'walls', '.', '\"', 'adds', 'eduardo', ':', '\"', 'i', 'thought', 'about', 'my', 'children', 'all', 'the', 'time', '.', '\"', 'their', 'first', 'break', 'came', 'within', 'two', 'months', ',', 'however', ',', 'when', 'eduardo', 'was', 'hired', 'as', 'a', 'camera', '##man', 'for', '$', '4', '.', '50', 'an', 'hour', 'at', 'channel', '23', ',', 'a', 'non', '##uni', '##on', 'spanish', '-', 'language', 'tv', 'station', '.', 'they', 'moved', 'to', 'their', 'own', 'one', '-', 'bedroom', 'apartment', ',', 'paying', '$', '350', 'a', 'month', 'in', 'rent', ',', 'and', 'stayed', 'there', 'two', 'years', 'while', 'eduardo', 'advanced', 'to', 'chief', 'photographer', 'and', 'betty', 'moved', 'to', 'the', 'engineering', 'firm', '.', 'by', '1984', ',', 'they', 'were', 'ready', 'to', 'buy', 'a', '$', '54', ',', '000', 'two', '-', 'bedroom', 'town', '##house', 'in', 'suburban', 'kendall', '(', '\"', 'we', 'thought', 'it', 'was', 'our', 'castle', ',', '\"', 'says', 'eduardo', ')', '.', 'and', 'in', \"'\", '86', ',', 'they', 'closed', 'on', 'their', 'current', 'dwelling', '.', 'call', 'it', 'miami', 'nice', 'the', 'gleaming', 'white', 'interior', 'of', 'the', '20', '-', 'year', '-', 'old', 'home', 'be', '##sp', '##eak', '##s', 'extensive', 'renovation', '.', 'the', 'money', 'for', 'the', '$', '30', ',', '000', 'project', 'came', 'from', 'savings', 'and', 'a', '$', '7', ',', '000', ',', '18', '%', 'consumer', 'loan', 'from', 'southeast', 'bank', 'on', 'which', 'the', 'suarez', '##es', 'still', 'pay', '$', '115', 'a', 'eduardo', \"'\", 's', 'career', 'was', 'booming', '.', 'he', 'was', 'promoted', 'three', 'times', 'at', 'channel', '23', 'before', 'becoming', 'operations', 'manager', 'at', 'rival', 'channel', '51', 'two', 'years', 'ago', '.', 'eduardo', 'has', 'now', 'won', 'more', 'local', 'emmy', 'awards', '-', '-', '13', ',', 'for', 'outstanding', 'camera', 'work', ',', 'producing', 'and', 'directing', '-', '-', 'than', 'he', 'has', 'spent', 'years', 'in', 'this', 'country', '.', 'but', 'which', 'emmy', 'does', 'he', 'cher', '##ish', 'most', '?', 'nu', '##mer', '##o', 'uno', ',', 'for', 'a', 'piece', 'about', 'the', '1982', 'crash', 'in', 'new', 'orleans', 'of', 'a', 'pan', 'am', 'flight', 'that', 'originated', 'in', 'miami', '.', 'he', 'still', 'keeps', 'a', 'yellow', '##ed', 'miami', 'herald', 'clip', 'about', 'the', 'awards', 'ceremony', '.', '\"', 'this', 'award', 'means', 'a', 'lot', 'to', 'me', ',', '\"', 'he', 'told', 'a', 'hushed', 'crowd', 'of', '2', ',', '000', '.', '\"', 'it', 'is', 'true', 'this', 'is', 'the', 'land', 'of', 'opportunity', '.', '\"', 'neither', 'eduardo', 'nor', 'betty', 'would', 'move', 'back', 'to', 'cuba', ',', 'even', 'if', 'castro', 'left', 'power', '.', 'says', 'betty', ':', '\"', 'this', 'is', 'our', 'home', 'now', '.', '\"', 'while', 'the', 'emmy', '##s', 'were', 'impressive', 'signs', 'of', 'success', ',', 'the', 'suarez', '##es', \"'\", 'greatest', 'reward', 'by', 'far', 'since', 'coming', 'to', 'america', 'was', 'the', 'birth', 'of', 'andrea', 'in', 'may', '1988', '.', '\"', 'she', 'is', 'the', 'most', 'eduardo', ',', '\"', 'and', 'i', 'want', 'to', 'give', 'her', 'everything', 'i', 'can', '.', '\"', 'but', 'when', 'it', 'comes', 'to', 'andrea', \"'\", 's', 'education', ',', 'the', 'suarez', '##es', 'will', 'not', 'be', 'able', 'to', 'afford', 'their', 'dreams', 'unless', 'they', 'reform', 'their', 'spending', '.', 'in', 'some', 'ways', ',', 'life', 'at', 'casa', 'suarez', 'seems', 'like', 'one', 'big', 'fiesta', '.', 'con', '##cede', '##s', 'eduardo', ':', '\"', 'i', 'love', 'having', 'many', 'people', 'at', 'my', 'house', '.', '\"', 'most', 'sundays', ',', 'for', 'example', ',', 'eight', 'to', '10', 'members', 'of', 'their', 'large', 'extended', 'families', 'gather', 'at', 'the', 'suarez', \"'\", 's', 'home', 'for', 'one', 'of', 'his', 'special', '##ties', 'chu', '##rra', '##sco', 'al', 'barbecue', ',', 'an', 'argentine', 'beef', '##ste', '##ak', 'dust', '##ed', 'with', 'mojo', 'cr', '##iol', '##lo', '-', '-', 'season', '##ing', 'salt', '-', '-', 'and', 'grille', '##d', 'over', 'hickory', 'chips', '.', 'average', 'cost', 'for', 'the', 'feast', '$', '80', '.', 'the', 'couple', 'also', 'enjoy', 'dining', 'out', 'with', 'friends', 'at', 'places', 'such', 'as', 'malaga', ',', 'a', '$', '15', '-', 'to', '$', '20', '-', 'a', 'meal', 'spanish', 'joint', 'in', 'miami', \"'\", 's', 'pu', '##ls', '##ating', 'little', 'havana', ',', 'and', 'christy', \"'\", 's', ',', 'a', 'club', '##by', 'coral', 'gables', 'steak', '##house', 'where', 'prime', 'rib', 'for', 'two', ',', 'plus', 'wine', ',', 'costs', 'about', '$', '75', '.', 'on', 'top', 'of', 'that', ',', 'they', 'go', 'for', 'lunch', 'every', 'day', 'with', 'colleagues', 'from', 'work', '.', '\"', 'eating', 'in', 'unfortunately', ',', 'it', \"'\", 's', 'also', 'his', 'achilles', 'heel', ',', 'since', 'he', 'and', 'betty', 'spend', 'around', '$', '10', ',', '000', 'a', 'year', 'eating', 'out', '.', 'with', 'this', 'and', 'other', 'spending', ',', 'including', 'the', 'home', 'renovation', ',', 'the', 'suarez', '##es', 'have', 'set', 'aside', 'only', 'about', '$', '11', ',', '700', 'over', 'the', 'past', 'few', 'years', '.', 'that', \"'\", 's', 'simply', 'too', 'little', ',', 'given', 'their', 'approximately', '$', '85', ',', '000', 'combined', 'salaries', '.', 'about', '60', '%', 'of', 'the', 'savings', 'is', 'in', 'a', '5', '1', '/', '4', '%', 'pass', '##book', 'account', 'and', '40', '%', 'in', 'a', '7', '%', 'bank', 'certificate', 'of', 'deposit', '.', 'also', ',', 'on', 'the', 'advice', 'of', 'a', 'colleague', ',', 'eduardo', 'rolled', 'over', '$', '3', ',', '200', 'of', 'his', 'channel', '23', 'profit', '-', 'sharing', 'money', 'into', 'individual', 'retirement', 'accounts', 'invested', 'in', 'a', 'real', 'estate', 'limited', 'partnership', 'and', 'a', 'money', 'fund', 'at', 'dean', 'wit', '##ter', '(', 'recent', 'yield', '7', '.', '8', '%', ')', '.', 'betty', 'has', 'about', '$', '6', ',', '000', 'in', 'the', 'engineering', 'firm', \"'\", 's', 'retirement', 'plan', ',', 'about', '85', '%', 'of', 'it', 'in', 'a', 'money', 'fund', 'and', 'the', 'remainder', 'in', 'a', 'real', 'estate', 'limited', 'partnership', '.', 'as', 'for', 'lia', '##bilities', ',', 'in', 'addition', 'to', 'the', '$', '4', ',', '500', 'they', 'still', 'owe', 'on', 'the', 'home', 'improvement', 'loan', ',', 'eduardo', 'and', 'betty', 'have', '$', '4', ',', '500', 'outstanding', 'on', 'an', '11', '.', '5', '%', 'three', '-', 'year', 'home', '-', 'equity', 'loan', '.', 'they', 'took', 'it', 'out', 'let', 'balloon', 'to', '$', '6', ',', '000', ',', 'mostly', 'to', 'cover', 'the', 'costs', 'of', 'clothing', ',', 'gifts', 'and', 'furniture', '.', 'the', 'monthly', 'payment', 'is', '$', '115', '.', 'auto', 'loan', 'balance', '##s', 'total', '$', '7', ',', '248', 'on', 'her', \"'\", '86', 'thunder', '##bird', 'and', 'his', \"'\", '89', 'mitsubishi', 'gala', '##nt', '.', 'payments', 'on', 'both', 'come', 'to', '$', '600', 'a', 'month', ',', 'and', 'insurance', 'on', 'the', 'cars', 'costs', 'another', '$', '1', ',', '400', 'a', 'year', '.', 'eduardo', \"'\", 's', 'and', 'betty', \"'\", 's', 'life', 'insurance', 'and', 'his', 'disability', 'coverage', '(', 'betty', 'has', 'none', ')', 'come', 'through', 'channel', '51', ';', 'the', 'premium', '##s', ',', 'de', '##ducted', 'from', 'his', 'pay', '##che', '##ck', ',', 'total', '$', '148', 'a', 'year', '.', 'but', 'eduardo', \"'\", 's', 'life', 'insurance', 'would', 'pay', 'only', 'about', '$', '1', '.', '30', ',', '000', 'on', 'his', 'death', '(', '$', '280', ',', '000', 'if', 'he', 'dies', 'in', 'an', 'accident', ')', 'and', '$', '60', ',', '000', 'for', 'betty', 'in', 'the', 'event', 'that', 'her', 'death', 'is', 'accidental', ';', 'otherwise', ',', 'she', 'is', 'uncovered', '.', 'on', 'the', 'disability', 'side', ',', 'eduardo', \"'\", 's', 'policy', 'would', 'pay', 'two', '-', 'thirds', 'of', 'his', 'salary', ',', 'beginning', '26', 'weeks', 'after', 'he', \"'\", 's', 'injured', 'and', 'lasting', 'until', 'age', '65', '.', 'these', 'and', 'other', 'fixed', 'expenses', ',', 'however', ',', 'amount', 'to', 'only', 'about', '70', '%', 'of', 'their', 'income', '.', 'the', 'rest', 'remains', 'una', '##cco', '##unt', '##ed', 'for', ',', 'although', 'they', 'think', 'the', 'betty', \"'\", 's', 'father', 'came', 'over', 'from', 'havana', 'for', 'an', 'extended', 'stay', 'last', 'october', 'and', 'again', 'in', 'april', '-', '-', 'the', 'latest', 'in', 'a', 'stream', 'of', 'visiting', 'in', '-', 'laws', '.', 'the', 'suarez', '##es', 'picked', 'up', 'the', 'tab', ',', 'including', 'air', 'fare', ',', 'of', '$', '2', ',', '500', 'to', '$', '3', ',', '000', '.', 'in', 'addition', ',', 'several', 'times', 'a', 'year', ',', 'eduardo', 'sends', 'packages', 'stuffed', 'with', 'cash', ',', 'nike', '##s', ',', 'nintendo', 'games', 'and', 'the', 'like', 'to', 'edgar', 'and', 'eli', '##anne', ',', 'now', '15', 'and', '16', 'respectively', '(', '\"', 'as', 'the', 'children', 'get', 'bigger', ',', 'the', 'gifts', 'get', 'more', 'expensive', ',', '\"', 'he', 'sighs', ')', '.', 'eduardo', 'says', 'he', 'can', 'not', 'put', 'a', 'figure', 'on', 'how', 'much', 'he', 'and', 'betty', 'have', 'laid', 'out', 'to', 'help', 'their', 'families', 'over', 'the', 'past', '10', 'years', 'except', 'that', 'it', 'amounts', 'to', '\"', 'many', 'thousands', 'of', 'dollars', '.', '\"', 'and', 'every', 'penny', ',', 'he', 'adds', ',', 'was', 'well', 'spent', '.', 'yet', 'some', 'of', 'that', 'money', 'no', 'doubt', 'should', 'have', 'gone', 'into', 'savings', '.', 'and', 'even', 'eduardo', 'seems', 'to', 'realize', 'that', 'now', '.', '\"', 'before', 'andrea', ',', 'i', 'never', 'really', 'thought', 'about', 'the', 'future', ',', '\"', 'he', 'says', '.', '\"', 'i', 'just', 'did', 'what', 'i', 'had', 'to', 'do', 'advice', 'the', 'problems', ':', 'curb', 'spending', ',', 'make', 'an', 'estate', 'plan', 'and', 'save', 'for', 'andrea', \"'\", 's', 'education', '.', 'the', 'solutions', ':', 'start', 'budget', '##ing', ',', 'write', 'wills', ',', 'shore', 'up', 'life', 'and', 'disability', 'coverage', ',', 'and', 'invest', 'in', 'conservative', ',', 'growth', '-', 'oriented', 'mutual', 'funds', '.', 'money', 'asked', 'miami', 'certified', 'financial', 'planners', 'joseph', 'ross', 'of', 'first', 'affiliated', 'securities', 'and', 'harold', 'even', '##sky', 'of', 'even', '##sky', '&', ';', 'brown', 'to', 'advise', 'eduardo', 'and', 'betty', 'suarez', 'on', 'how', 'to', 'get', 'their', 'finances', 'in', 'shape', '.', 'recommendations', ':', 'get', 'a', 'grip', 'on', 'cash', 'flow', '.', 'although', 'the', 'suarez', '##es', 'have', 'fare', '##d', 'exceeding', '##ly', 'well', 'in', 'their', 'careers', ',', 'they', 'wo', 'n', \"'\", 't', 'be', 'able', 'to', 'control', 'their', 'finances', 'until', 'they', 'figure', 'out', 'where', 'their', 'money', 'is', 'going', '.', 'for', 'two', 'months', ',', 'the', 'couple', 'should', 'keep', 'a', 'daily', 'log', 'of', 'all', 'their', 'fixed', 'and', 'incident', '##al', 'expenses', '-', '-', '\"', 'down', 'to', 'the', 'penny', ',', '\"', 'ross', 'says', '.', '\"', 'the', 'more', 'you', 'find', 'to', 'put', 'away', ',', 'the', 'more', 'you', \"'\", 'll', 'have', 'for', 'andrea', \"'\", 's', 'future', '.', '\"', 'one', 'way', 'betty', 'and', 'eduardo', 'can', 'immediately', 'and', 'pain', '##lessly', 'begin', 'to', 'save', 'is', 'by', 'expanding', 'the', 'credit', 'line', 'on', 'their', '11', '.', '5', '%', 'home', '-', 'equity', 'the', 'money', 'to', 'pay', 'off', 'the', '$', '4', ',', '500', 'balance', 'on', 'their', '18', '%', 'consumer', 'loan', '.', 'interest', 'on', 'the', 'home', '-', 'equity', 'loan', 'is', 'fully', 'tax', 'de', '##du', '##ct', '##ible', ';', 'that', 'on', 'the', 'consumer', 'loan', 'is', 'only', '10', '%', 'de', '##du', '##ct', '##ible', 'this', 'year', 'and', 'not', 'at', 'all', 'in', '1991', '.', 'first', '-', 'year', 'after', '-', 'tax', 'savings', ':', '$', '404', '.', 'they', 'should', 'not', ',', 'however', ',', 'use', 'the', 'credit', 'line', 'for', 'extra', 'spending', '.', 'shore', 'up', 'insurance', 'and', 'write', 'a', 'will', '.', 'even', 'before', 'betty', 'and', 'eduardo', 'begin', 'saving', 'for', 'andrea', \"'\", 's', 'education', ',', 'they', 'should', 'set', 'aside', 'enough', 'money', 'to', 'ensure', 'that', 'she', 'is', 'well', 'taken', 'care', 'of', 'if', 'something', 'happens', 'to', 'them', '.', '\"', 'andrea', 'does', 'n', \"'\", 't', 'start', 'school', 'for', 'three', 'more', 'years', ',', '\"', 'ross', 'points', 'out', '.', '\"', 'but', 'one', 'or', 'both', 'of', 'you', 'could', 'die', 'or', 'become', 'disabled', 'tomorrow', '.', '\"', 'with', 'the', 'help', 'of', 'an', 'attorney', ',', 'therefore', ',', 'betty', 'and', 'eduardo', 'should', 'draw', 'up', 'wills', 'immediately', 'and', 'include', 'provisions', 'in', 'them', 'for', 'andrea', \"'\", 's', 'guardians', '##hip', '.', 'probable', 'cost', ':', '$', '400', 'to', '$', '800', '.', 'they', 'should', 'also', 'supplement', 'their', 'life', 'insurance', '.', 'even', '##sky', 'figures', 'eduardo', 'should', 'have', '$', '400', ',', '000', 'of', 'total', 'coverage', 'and', 'betty', '$', '150', ',', '000', 'that', 'will', 'he', 'recommended', 'universal', 'life', 'policies', ',', 'which', 'combine', 'insurance', 'with', 'tax', '-', 'def', '##erre', '##d', 'savings', ',', 'offered', 'by', 'fee', 'for', 'service', ',', 'a', 'no', '-', 'load', 'insurance', 'company', 'in', 'tampa', '.', 'cost', ':', '$', '1', ',', '400', 'a', 'year', 'for', 'eduardo', 'and', '$', '510', 'for', 'betty', '.', 'eduardo', 'should', 'also', 'boost', 'his', 'disability', 'insurance', 'to', 'cover', '80', '%', 'of', 'his', 'salary', ',', 'instead', 'of', '67', '%', ',', 'at', 'an', 'added', 'cost', 'of', '$', '317', 'a', 'year', '.', 'since', 'betty', 'contributes', 'about', 'a', 'fourth', 'of', 'the', 'family', 'income', ',', 'she', 'should', 'also', 'have', '80', '%', 'disability', 'coverage', '.', 'her', 'premium', 'will', 'run', '$', '340', 'a', 'year', '.', 'paying', 'for', 'school', '.', 'even', '##sky', 'estimates', 'that', '13', 'years', 'of', 'parochial', 'school', 'for', 'andrea', '(', 'including', 'kindergarten', ')', 'plus', 'four', 'years', 'of', 'private', 'college', 'will', 'cost', 'more', 'than', '$', '140', ',', '000', 'in', 'today', \"'\", 's', 'dollars', '-', '-', 'or', '$', '36', '##7', ',', '000', 'if', 'college', 'costs', 'rise', 'a', 'plausible', '7', '%', 'a', 'year', '.', '\"', 'if', 'the', 'suarez', '##es', 'set', 'aside', 'about', '$', '1', ',', '000', 'a', 'month', 'and', 'earn', 'a', 'moderate', '7', '%', 'on', 'their', 'money', 'after', 'taxes', ',', '\"', 'he', 'says', ',', '\"', 'they', 'can', 'pay', 'for', 'it', '.', '\"', 'the', '$', '1', ',', '000', 'goal', 'is', 'admitted', '##ly', 'a', 'tough', 'one', ',', 'but', 'the', 'suarez', '##es', 'should', 'aim', 'for', 'it', 'nonetheless', '-', '-', 'putting', 'best', 'returns', 'of', 'any', 'financial', 'asset', '(', 'about', '10', '%', 'a', 'year', 'before', 'taxes', ')', '.', 'the', 'suarez', '##es', ',', 'says', 'even', '##sky', ',', '\"', 'can', 'afford', 'to', 'take', 'some', 'risks', '.', 'in', 'fact', ',', 'with', 'so', 'many', 'expenses', 'ahead', 'of', 'them', ',', 'they', 'ca', 'n', \"'\", 't', 'afford', 'not', 'to', '.', '\"', 'the', 'planners', 'suggest', 'that', 'eduardo', 'and', 'betty', 'begin', 'making', 'regular', 'contributions', 'to', 'a', 'couple', 'of', 'conservative', 'equity', 'funds', 'with', 'strong', 'long', '-', 'term', 'records', '.', 'examples', ':', 'lin', '##dner', 'fund', '(', 'no', 'load', ';', '314', '-', '72', '##7', '-', '530', '##5', ')', 'and', 'math', '##ers', 'fund', '(', 'no', 'load', ';', '800', '-', '96', '##2', '-', '38', '##6', '##3', ')', ',', 'which', 'are', 'up', '97', '%', 'and', '111', '%', 'respectively', 'over', 'the', 'five', 'years', 'that', 'ended', 'on', 'june', '1', '.', 'eduardo', 'and', 'betty', 'suarez', 'were', 'slightly', 'overwhelmed', 'by', 'the', 'planners', \"'\", 'recommendations', '.', 'but', 'they', 'started', 'keeping', 'track', 'of', 'expenses', ',', 'as', 'the', 'advisers', 'suggested', ',', 'and', 'expanded', 'their', 'home', '-', 'equity', ',', 'line', 'of', 'credit', '.', 'they', 'also', 'planned', 'to', 'write', 'wills', 'and', 'aug', '##ment', 'their', 'insurance', 'coverage', '.', '\"', 'we', 'ye', 'lived', 'here', '10', 'years', ',', 'and', 'no', 'one', 'ever', 'told', 'us', 'about', 'these', 'things', ',', '\"', 'said', 'betty', 'of', 'the', 'planners', \"'\", 'advice', '.', 'for', 'his', 'part', ',', 'eduardo', 'was', 'determined', 'to', ',', 'you', 'really', 'have', 'to', 'plan', 'a', 'lot', ',', '\"', 'he', 'observed', '.', '\"', 'we', 'must', '.', 'and', 'we', 'will', '.', '\"', 'the', 'leaking', 'hole', 'in', 'their', 'budget', 'their', 'expenses', 'aside', ',', 'the', 'suarez', '##es', 'are', 'n', \"'\", 't', 'sure', 'where', 'nearly', '30', '%', 'of', 'their', 'income', 'goes', '.', 'table', 'photos', '(', 'color', ')', ':', 'eduardo', 'suarez', 'in', 'a', '19', '76', 'self', '-', 'portrait', '(', 'left', ')', 'as', 'a', 'havana', 'photographer', ';', 'and', 'today', '(', 'right', ')', 'as', 'the', 'emmy', 'award', '-', 'winning', 'operations', 'manager', 'of', 'a', 'miami', 'tv', 'station', 'photo', '(', 'black', '&', ';', 'white', ')', ':', 'andrea', \"'\", 's', 'second', 'birthday', 'party', '(', 'left', ')', ',', 'attended', 'by', '60', 'friends', 'and', 'family', 'members', ',', 'was', 'typical', 'of', 'the', 'generous', 'entertaining', 'the', 'suarez', '##es', 'need', 'to', 'control', '.', 'above', ',', 'the', 'children', 'by', 'an', 'earlier', 'marriage', '-', '-', 'edgar', 'and', 'eli', '##anne', '-', '-', 'whom', 'eduardo', 'had', 'to', 'leave', 'in', 'cuba', '.', 'photo', '(', 'color', ')', ':', 'striking', 'contrasts', 'of', 'today', 'and', 'yesterday', ':', 'above', ',', 'betty', 'at', 'her', 'drafts', '##person', \"'\", 's', 'job', ',', 'with', 'snaps', '##hot', '##s', 'of', 'andrea', 'on', 'the', 'wall', 'photo', '(', 'black', '&', ';', 'white', ')', ':', 'below', ',', 'the', 'refugee', 'center', 'where', 'the', 'suarez', '##es', 'and', 'other', 'marie', '##l', 'refugees', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wywdbBISguFj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n",
        "# In the original paper, the authors used a length of 512.\n",
        "MAX_LEN = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfXhgejEgxRY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ko6fHIlAiHm7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSaU7DbkgzWh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEUBxUm8g0ov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "    seq_mask = [float(i>0) for i in seq]\n",
        "    attention_masks.append(seq_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJEqe9QDQ2A6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0c6260ef-2a18-4944-845d-81e0a831653b"
      },
      "source": [
        "print(len(df), len(input_ids), len(attention_masks))"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3992 3992 3992\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4QPyBRDiIx1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for training\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=2020, test_size=0.1)\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,random_state=2020, test_size=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWLfSUPjiLJA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ0A0HbAiMpc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_in_size = tokenizer.vocab_size\n",
        "embedding_dim = 32\n",
        "unit = 100\n",
        "no_labels = len(np.unique(train_labels))\n",
        "batch_size = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POlslC2AiPAI",
        "colab_type": "code",
        "outputId": "ff31be89-072b-44af-b079-8e5cc500ad8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "model_lstm = Sequential()\n",
        "model_lstm.add(Embedding(vocab_in_size, embedding_dim, input_length=MAX_LEN))\n",
        "model_lstm.add(LSTM(unit))\n",
        "model_lstm.add(Dense(no_labels, activation='softmax'))\n",
        "model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_lstm.summary()"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 128, 32)           976704    \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3)                 303       \n",
            "=================================================================\n",
            "Total params: 1,030,207\n",
            "Trainable params: 1,030,207\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gu2c6CWSiP3D",
        "colab_type": "code",
        "outputId": "4e1c9325-ae98-4422-8f38-3748b184adf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "history_lstm = model_lstm.fit(train_inputs, train_labels, \n",
        "                              epochs=10,batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "3592/3592 [==============================] - 22s 6ms/step - loss: 1.0571 - acc: 0.4518\n",
            "Epoch 2/10\n",
            "3592/3592 [==============================] - 22s 6ms/step - loss: 0.6825 - acc: 0.6754\n",
            "Epoch 3/10\n",
            "3592/3592 [==============================] - 22s 6ms/step - loss: 0.5378 - acc: 0.7742\n",
            "Epoch 4/10\n",
            "3592/3592 [==============================] - 22s 6ms/step - loss: 0.3825 - acc: 0.8705\n",
            "Epoch 5/10\n",
            "3592/3592 [==============================] - 23s 6ms/step - loss: 0.2867 - acc: 0.8998\n",
            "Epoch 6/10\n",
            "3592/3592 [==============================] - 22s 6ms/step - loss: 0.1253 - acc: 0.9649\n",
            "Epoch 7/10\n",
            "3592/3592 [==============================] - 21s 6ms/step - loss: 0.1131 - acc: 0.9702\n",
            "Epoch 8/10\n",
            "3592/3592 [==============================] - 22s 6ms/step - loss: 0.1360 - acc: 0.9708\n",
            "Epoch 9/10\n",
            "1824/3592 [==============>...............] - ETA: 10s - loss: 0.1571 - acc: 0.9556"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7D5zi_0iRqh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6muAlDeiac7",
        "colab_type": "text"
      },
      "source": [
        "### On with BERT!\n",
        "\n",
        "So while Neural Networks can do a good job with some kind of classification tasks, they don't perform too well on intent classification. Let us see how BERT might do. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJDljt39iddQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert all of our data into torch tensors, the required datatype for our model\n",
        "\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgmJ-E_VigTt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
        "batch_size = 32\n",
        "\n",
        "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
        "# with an iterator the entire dataset does not need to be loaded into memory\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eX6t1SIUi22s",
        "colab_type": "text"
      },
      "source": [
        "## Loading our Models\n",
        "\n",
        "### Train Model\n",
        "Now that our input data is properly formatted, it's time to fine tune the BERT model.\n",
        "For this task, we first want to modify the pre-trained BERT model to give outputs for classification, and then we want to continue training the model on our dataset until that the entire model, end-to-end, is well-suited for our task. Thankfully, the huggingface pytorch implementation includes a set of interfaces designed for a variety of NLP tasks. Though these interfaces are all built on top of a trained BERT model, each has different top layers and output types designed to accomodate their specific NLP task.\n",
        "We'll load BertForSequenceClassification. This is the normal BERT model with an added single linear layer on top for classification that we will use as a sentence classifier. As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task.\n",
        "\n",
        "### Structure of Fine-Tuning Model\n",
        "\n",
        "As we've showed beforehand, the first token of every sequence is the special classification token ([CLS]). Unlike the hidden state vector corresponding to a normal word token, the hidden state corresponding to this special token is designated by the authors of BERT as an aggregate representation of the whole sentence used for classification tasks. As such, when we feed in an input sentence to our model during training, the output is the length 768 hidden state vector corresponding to this token. The additional layer that we've added on top consists of untrained linear neurons of size [hidden_state, number_of_labels], so [768,2], meaning that the output of BERT plus our classification layer is a vector of two numbers representing the \"score\" for \"grammatical/non-grammatical\" that are then fed into cross-entropy loss.\n",
        "\n",
        "### The Fine-Tuning Process\n",
        "\n",
        "Because the pre-trained BERT layers already encode a lot of information about the language, training the classifier is relatively inexpensive. Rather than training every layer in a large model from scratch, it's as if we have already trained the bottom layers 95% of where they need to be, and only really need to train the top layer, with a bit of tweaking going on in the lower levels to accomodate our task.\n",
        "Sometimes practicioners will opt to \"freeze\" certain layers when fine-tuning, or to apply different learning rates, apply diminishing learning rates, etc. all in an effort to preserve the good quality weights in the network and speed up training (often considerably). In fact, recent research on BERT specifically has demonstrated that freezing the majority of the weights results in only minimal accuracy declines, but there are exceptions and broader rules of transfer learning that should also be considered. For example, if your task and fine-tuning dataset is very different from the dataset used to train the transfer learning model, freezing the weights may not be a good idea. We'll cover the broader scope of transfer learning in NLP in a future post.\n",
        "OK, let's load BERT! There are a few different pre-trained BERT models available. \"bert-base-uncased\" means the version that has only lowercase letters (\"uncased\") and is the smaller version of the two (\"base\" vs \"large\")."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYkR2tJxiiJk",
        "colab_type": "code",
        "outputId": "6b6e229b-5850-4b37-b48e-3b77c1373cbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "090ee4aa14d84b42aea545b5844da99f",
            "36dc7ffb811548bea912fe1329989a3e",
            "062a2ebce7b749a6bd554b2f5d6fd4b5",
            "d889373684b74108b13c7f79b9067ed0",
            "f52aecb10ee84613b6167b2f6e59eb1f",
            "9c3c477ba27c496185f1ff64ea92836b",
            "5de47dd68bdc4876bac230a768c7ba9f",
            "ff8a5dd805634809b0c275e63bdd4a80",
            "c9e49d0d6da3420d95e93b19e9f7cd97",
            "0d41b8ccf1124d2a923e85d0dcbfcef1",
            "f8653ce07dbd4154a9eec5a750bd68bb",
            "50700167127e482980215f5c9f5a95dd",
            "65b1305bbbd549c098b7f52d7deb4df2",
            "67892a51ee394f68b962e199fbbe8b0a",
            "a21e6c85932143a1b0ed3b4238fab85b",
            "baa24e0a0e3c4711a029f15ebe1885bc"
          ]
        }
      },
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "model.cuda()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "090ee4aa14d84b42aea545b5844da99f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=361, style=ProgressStyle(description_width=…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c9e49d0d6da3420d95e93b19e9f7cd97",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=440473133, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfwPyoh9ii0x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "likqi1-FimRZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This variable contains all of the hyperparemeter information our training loop needs\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJcXKErgioEE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-fvXJW_ip1k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtCJt0A0irEL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnwi-bj1itbL",
        "colab_type": "code",
        "outputId": "eeeb6eab-0148-4d92-caed-1c29dc1884b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:16.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:33.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:49.\n",
            "  Batch   160  of    241.    Elapsed: 0:01:06.\n",
            "  Batch   200  of    241.    Elapsed: 0:01:22.\n",
            "  Batch   240  of    241.    Elapsed: 0:01:38.\n",
            "\n",
            "  Average training loss: 0.50\n",
            "  Training epcoh took: 0:01:38\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.82\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:16.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:33.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:49.\n",
            "  Batch   160  of    241.    Elapsed: 0:01:05.\n",
            "  Batch   200  of    241.    Elapsed: 0:01:22.\n",
            "  Batch   240  of    241.    Elapsed: 0:01:38.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epcoh took: 0:01:38\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.84\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:16.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:33.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:49.\n",
            "  Batch   160  of    241.    Elapsed: 0:01:06.\n",
            "  Batch   200  of    241.    Elapsed: 0:01:22.\n",
            "  Batch   240  of    241.    Elapsed: 0:01:38.\n",
            "\n",
            "  Average training loss: 0.20\n",
            "  Training epcoh took: 0:01:39\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.84\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:16.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:33.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:49.\n",
            "  Batch   160  of    241.    Elapsed: 0:01:06.\n",
            "  Batch   200  of    241.    Elapsed: 0:01:22.\n",
            "  Batch   240  of    241.    Elapsed: 0:01:38.\n",
            "\n",
            "  Average training loss: 0.14\n",
            "  Training epcoh took: 0:01:39\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.86\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZPeyGKYtoCn",
        "colab_type": "code",
        "outputId": "8d4ce1a5-0e4a-4fca-dabb-26f396d7c8d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "loss_values"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4993767683931406,\n",
              " 0.30958858243657345,\n",
              " 0.20123812692293983,\n",
              " 0.14341535634086836]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFogz7jeiycc",
        "colab_type": "code",
        "outputId": "62e85db7-4bc6-4fbf-9912-4b06c77086fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"./cola_public/raw/out_of_domain_dev.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                   )\n",
        "    \n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "    seq_mask = [float(i>0) for i in seq]\n",
        "    attention_masks.append(seq_mask) \n",
        "\n",
        "# Convert to tensors.\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of test sentences: 516\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3S6GaadjK48",
        "colab_type": "code",
        "outputId": "fb09b782-d2b4-42b8-b8c7-3cedf1097327",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "    # Telling the model not to compute or store gradients, saving memory and \n",
        "    # speeding up prediction\n",
        "    with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "    logits = outputs[0]\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    # Store predictions and true labels\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 516 test sentences...\n",
            "    DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_fqVB6CtzTo",
        "colab_type": "code",
        "outputId": "dbb610df-3998-4aeb-f8ff-a9aa8ad6ff14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Positive samples: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive samples: 354 of 516 (68.60%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIoseqqzt1pc",
        "colab_type": "code",
        "outputId": "9bb6b664-1e9a-4d5f-aea9-ebb0869d634d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "\n",
        "    # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
        "    # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "    # in to a list of 0s and 1s.\n",
        "    pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "\n",
        "    # Calculate and store the coef for this batch.  \n",
        "    matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
        "    matthews_set.append(matthews)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating Matthews Corr. Coef. for each batch...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:900: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BreKUJ9t25l",
        "colab_type": "code",
        "outputId": "b5a6c321-adf0-4fb1-d268-9772b4c425a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "matthews_set"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-0.14856415213808927,\n",
              " -0.050964719143762556,\n",
              " 0.4732058754737091,\n",
              " 0.30508307783296046,\n",
              " 0.5222329678670935,\n",
              " 0.7410010097502685,\n",
              " 0.4152273992686999,\n",
              " 0.47519096331149147,\n",
              " 1.0,\n",
              " 0.8246211251235321,\n",
              " 0.7679476477883045,\n",
              " 0.647150228929434,\n",
              " 0.7562449037944323,\n",
              " 0.7141684885491869,\n",
              " 0.2342878320018382,\n",
              " 0.5716350506349809,\n",
              " 0.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20s0Poipt6XT",
        "colab_type": "code",
        "outputId": "9425ca95-0342-4bb6-8274-70d63cdb224f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('MCC: %.3f' % mcc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MCC: 0.545\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpLhjm3D9-vA",
        "colab_type": "text"
      },
      "source": [
        "We now want to save this model to disk. Once you save it to disk, right click on the file and save it to your local computer to use it on your notebook. You would have maybe trained your model on your own dataset, in which case you would also need to upload your data, or load it using wget as we did before.\n",
        "\n",
        "NOTE: the files are accessible, both uploading and downloading, on the left hand side of the page, under the \"folder\" section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJ1Y-b7-90S9",
        "colab_type": "code",
        "outputId": "4009be13-b2e4-4cad-f597-a658627fceab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import os\n",
        "\n",
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = './model_save/'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model to ./model_save/\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./model_save/vocab.txt',\n",
              " './model_save/special_tokens_map.json',\n",
              " './model_save/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vq7G7JbAt9sn",
        "colab_type": "text"
      },
      "source": [
        "### Results\n",
        "\n",
        "We get the same result as our Jupyter Notebook, but our model was trained in just 10 mins, as opposed to 12 hours without GPU. Super neat! We now skip the parts which just use the pre-trained model, as you can follow those in the Jupyter Notebook. The purpose of this notebook is to get you to upload your data, train your models, download them, and use them in your Jupyter Notebooks.\n",
        "\n",
        "### Fine-tuning BERT and GPT\n",
        "\n",
        "We now do our other model fine-tuning.\n",
        "You have to upload your files to the colab file - on the left of the screen, on the files section, click the upload section, and upload test_text_trump, train_text_trump, run_generation.py, and run_language_modelling.py. These files would be on the GitHub repository.\n",
        "\n",
        "We start with training a model on Trump tweets, and then save the model to disk and load it in the Jupyter notebook. The following two lines of code does the language training and then text generation. The important part here is training your model, and then downloading that by right clicking the file name in the files section on the top left of the screen. In my xase, all the files were saved in /content/output_gpt_trump"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XdVsM_cL_3O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.empty_cache() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqiVuxFuLLvu",
        "colab_type": "code",
        "outputId": "c9e3109b-1764-4c8f-e3ce-8096f2886eac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python /content/run_language_modelling.py --output_dir=output_gpt_trump --model_type=gpt2 --model_name_or_path=gpt2 --do_train --train_data_file=/content/train_text_trump --do_eval --eval_data_file=/content/test_text_trump --per_gpu_train_batch_size=1 --per_gpu_eval_batch_size=1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "03/03/2020 00:24:15 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "03/03/2020 00:24:15 - INFO - filelock -   Lock 140067225987672 acquired on /root/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.699bbd1c449e9861456f359d6daa51bd523ac085b4b531ab0aad5a55d091e942.lock\n",
            "03/03/2020 00:24:15 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpcrgu8p9y\n",
            "Downloading: 100% 224/224 [00:00<00:00, 168kB/s]\n",
            "03/03/2020 00:24:16 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json in cache at /root/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.699bbd1c449e9861456f359d6daa51bd523ac085b4b531ab0aad5a55d091e942\n",
            "03/03/2020 00:24:16 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.699bbd1c449e9861456f359d6daa51bd523ac085b4b531ab0aad5a55d091e942\n",
            "03/03/2020 00:24:16 - INFO - filelock -   Lock 140067225987672 released on /root/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.699bbd1c449e9861456f359d6daa51bd523ac085b4b531ab0aad5a55d091e942.lock\n",
            "03/03/2020 00:24:16 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /root/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.699bbd1c449e9861456f359d6daa51bd523ac085b4b531ab0aad5a55d091e942\n",
            "03/03/2020 00:24:16 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "03/03/2020 00:24:17 - INFO - filelock -   Lock 140067598751896 acquired on /root/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71.lock\n",
            "03/03/2020 00:24:17 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpe8paruc0\n",
            "Downloading: 100% 1.04M/1.04M [00:01<00:00, 959kB/s]\n",
            "03/03/2020 00:24:19 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json in cache at /root/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
            "03/03/2020 00:24:19 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
            "03/03/2020 00:24:19 - INFO - filelock -   Lock 140067598751896 released on /root/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71.lock\n",
            "03/03/2020 00:24:19 - INFO - filelock -   Lock 140067598751896 acquired on /root/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\n",
            "03/03/2020 00:24:19 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp3zvxn5x5\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 506kB/s]\n",
            "03/03/2020 00:24:21 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt in cache at /root/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "03/03/2020 00:24:21 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "03/03/2020 00:24:21 - INFO - filelock -   Lock 140067598751896 released on /root/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\n",
            "03/03/2020 00:24:21 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /root/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
            "03/03/2020 00:24:21 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /root/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "03/03/2020 00:24:22 - INFO - filelock -   Lock 140067225951032 acquired on /root/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1.lock\n",
            "03/03/2020 00:24:22 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpmj5iaxiq\n",
            "Downloading: 100% 548M/548M [00:43<00:00, 12.7MB/s]\n",
            "03/03/2020 00:25:06 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin in cache at /root/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n",
            "03/03/2020 00:25:06 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n",
            "03/03/2020 00:25:06 - INFO - filelock -   Lock 140067225951032 released on /root/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1.lock\n",
            "03/03/2020 00:25:06 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin from cache at /root/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n",
            "03/03/2020 00:25:14 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=1024, cache_dir=None, config_name=None, device=device(type='cuda'), do_eval=True, do_train=True, eval_all_checkpoints=False, eval_data_file='/content/test_text_trump', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, line_by_line=False, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_name_or_path='gpt2', model_type='gpt2', n_gpu=1, no_cuda=False, num_train_epochs=1.0, output_dir='output_gpt_trump', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=1, per_gpu_train_batch_size=1, save_steps=500, save_total_limit=None, seed=42, server_ip='', server_port='', should_continue=False, tokenizer_name=None, train_data_file='/content/train_text_trump', warmup_steps=0, weight_decay=0.0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/run_language_modelling.py\", line 799, in <module>\n",
            "    main()\n",
            "  File \"/content/run_language_modelling.py\", line 744, in main\n",
            "    train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False)\n",
            "  File \"/content/run_language_modelling.py\", line 152, in load_and_cache_examples\n",
            "    return TextDataset(tokenizer, args, file_path=file_path, block_size=args.block_size)\n",
            "  File \"/content/run_language_modelling.py\", line 88, in __init__\n",
            "    assert os.path.isfile(file_path)\n",
            "AssertionError\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hympHJXeMIhB",
        "colab_type": "code",
        "outputId": "10486585-c967-45d4-dbae-45e09a5dc2a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!python run_generation.py --model_type=gpt2 --model_name_or_path=/content/output_gpt_trump"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python3: can't open file 'run_generation.py': [Errno 2] No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RF07JuDLUV9V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-CDbhF8SQoN",
        "colab_type": "text"
      },
      "source": [
        "### Tuning RoBERTa on US and UK blog posts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwsrsQ2qSVnN",
        "colab_type": "code",
        "outputId": "283f0254-c965-42e7-901d-54fefc0335b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python run_language_modelling.py --output_dir=output_roberta_US --model_type=roberta --model_name_or_path=roberta-base --do_train --train_data_file=us_blog_train --do_eval --eval_data_file=us_blog_test --mlm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "03/03/2020 00:27:10 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "03/03/2020 00:27:11 - INFO - filelock -   Lock 140105954413144 acquired on /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6.lock\n",
            "03/03/2020 00:27:11 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp3r2blktq\n",
            "Downloading: 100% 524/524 [00:00<00:00, 399kB/s]\n",
            "03/03/2020 00:27:12 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json in cache at /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\n",
            "03/03/2020 00:27:12 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\n",
            "03/03/2020 00:27:12 - INFO - filelock -   Lock 140105954413144 released on /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6.lock\n",
            "03/03/2020 00:27:12 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\n",
            "03/03/2020 00:27:12 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "03/03/2020 00:27:12 - INFO - filelock -   Lock 140105954380824 acquired on /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b.lock\n",
            "03/03/2020 00:27:12 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp8ug_j20f\n",
            "Downloading: 100% 899k/899k [00:01<00:00, 830kB/s]\n",
            "03/03/2020 00:27:14 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json in cache at /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
            "03/03/2020 00:27:14 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
            "03/03/2020 00:27:14 - INFO - filelock -   Lock 140105954380824 released on /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b.lock\n",
            "03/03/2020 00:27:15 - INFO - filelock -   Lock 140105954381384 acquired on /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\n",
            "03/03/2020 00:27:15 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmphc5rx_qr\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 625kB/s]\n",
            "03/03/2020 00:27:17 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt in cache at /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "03/03/2020 00:27:17 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "03/03/2020 00:27:17 - INFO - filelock -   Lock 140105954381384 released on /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\n",
            "03/03/2020 00:27:17 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
            "03/03/2020 00:27:17 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "03/03/2020 00:27:17 - INFO - filelock -   Lock 140105954413704 acquired on /root/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e.lock\n",
            "03/03/2020 00:27:17 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp62qrj_69\n",
            "Downloading: 100% 501M/501M [00:35<00:00, 14.1MB/s]\n",
            "03/03/2020 00:27:54 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin in cache at /root/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
            "03/03/2020 00:27:54 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
            "03/03/2020 00:27:54 - INFO - filelock -   Lock 140105954413704 released on /root/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e.lock\n",
            "03/03/2020 00:27:54 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at /root/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
            "03/03/2020 00:28:00 - INFO - transformers.modeling_utils -   Weights of RobertaForMaskedLM not initialized from pretrained model: ['lm_head.decoder.bias']\n",
            "03/03/2020 00:28:03 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir=None, config_name=None, device=device(type='cuda'), do_eval=True, do_train=True, eval_all_checkpoints=False, eval_data_file='us_blog_test', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, line_by_line=False, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_steps=-1, mlm=True, mlm_probability=0.15, model_name_or_path='roberta-base', model_type='roberta', n_gpu=1, no_cuda=False, num_train_epochs=1.0, output_dir='output_roberta_US', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=4, save_steps=500, save_total_limit=None, seed=42, server_ip='', server_port='', should_continue=False, tokenizer_name=None, train_data_file='us_blog_train', warmup_steps=0, weight_decay=0.0)\n",
            "Traceback (most recent call last):\n",
            "  File \"run_language_modelling.py\", line 799, in <module>\n",
            "    main()\n",
            "  File \"run_language_modelling.py\", line 744, in main\n",
            "    train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False)\n",
            "  File \"run_language_modelling.py\", line 152, in load_and_cache_examples\n",
            "    return TextDataset(tokenizer, args, file_path=file_path, block_size=args.block_size)\n",
            "  File \"run_language_modelling.py\", line 88, in __init__\n",
            "    assert os.path.isfile(file_path)\n",
            "AssertionError\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R80-_g-xpiwx",
        "colab_type": "code",
        "outputId": "4073d3d6-2de7-48fa-b2fd-12452892b998",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python run_language_modelling.py --output_dir=output_roberta_GB --model_type=roberta --model_name_or_path=roberta-base --do_train --train_data_file=gb_blog_train --do_eval --eval_data_file=gb_blog_test --mlm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02/29/2020 07:27:25 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "02/29/2020 07:27:25 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\n",
            "02/29/2020 07:27:25 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "02/29/2020 07:27:26 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
            "02/29/2020 07:27:26 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "02/29/2020 07:27:26 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at /root/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
            "02/29/2020 07:27:45 - INFO - transformers.modeling_utils -   Weights of RobertaForMaskedLM not initialized from pretrained model: ['lm_head.decoder.bias']\n",
            "02/29/2020 07:27:48 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir=None, config_name=None, device=device(type='cuda'), do_eval=True, do_train=True, eval_all_checkpoints=False, eval_data_file='gb_blog_test', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, line_by_line=False, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_steps=-1, mlm=True, mlm_probability=0.15, model_name_or_path='roberta-base', model_type='roberta', n_gpu=1, no_cuda=False, num_train_epochs=1.0, output_dir='output_roberta_GB', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=4, save_steps=500, save_total_limit=None, seed=42, server_ip='', server_port='', should_continue=False, tokenizer_name=None, train_data_file='gb_blog_train', warmup_steps=0, weight_decay=0.0)\n",
            "02/29/2020 07:27:48 - INFO - __main__ -   Creating features from dataset file at \n",
            "02/29/2020 07:28:08 - INFO - __main__ -   Saving features into cached file roberta_cached_lm_510_gb_blog_train\n",
            "02/29/2020 07:28:08 - INFO - __main__ -   ***** Running training *****\n",
            "02/29/2020 07:28:08 - INFO - __main__ -     Num examples = 10241\n",
            "02/29/2020 07:28:08 - INFO - __main__ -     Num Epochs = 1\n",
            "02/29/2020 07:28:08 - INFO - __main__ -     Instantaneous batch size per GPU = 4\n",
            "02/29/2020 07:28:08 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "02/29/2020 07:28:08 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "02/29/2020 07:28:08 - INFO - __main__ -     Total optimization steps = 2561\n",
            "Epoch:   0% 0/1 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/2561 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 1/2561 [00:00<31:38,  1.35it/s]\u001b[A\n",
            "Iteration:   0% 2/2561 [00:01<31:16,  1.36it/s]\u001b[A\n",
            "Iteration:   0% 3/2561 [00:02<30:58,  1.38it/s]\u001b[A\n",
            "Iteration:   0% 4/2561 [00:02<30:54,  1.38it/s]\u001b[A\n",
            "Iteration:   0% 5/2561 [00:03<30:45,  1.39it/s]\u001b[A\n",
            "Iteration:   0% 6/2561 [00:04<30:34,  1.39it/s]\u001b[A\n",
            "Iteration:   0% 7/2561 [00:05<30:35,  1.39it/s]\u001b[A\n",
            "Iteration:   0% 8/2561 [00:05<30:31,  1.39it/s]\u001b[A\n",
            "Iteration:   0% 9/2561 [00:06<30:29,  1.39it/s]\u001b[A\n",
            "Iteration:   0% 10/2561 [00:07<30:18,  1.40it/s]\u001b[A\n",
            "Iteration:   0% 11/2561 [00:07<30:24,  1.40it/s]\u001b[A\n",
            "Iteration:   0% 12/2561 [00:08<30:22,  1.40it/s]\u001b[A\n",
            "Iteration:   1% 13/2561 [00:09<30:23,  1.40it/s]\u001b[A\n",
            "Iteration:   1% 14/2561 [00:10<30:16,  1.40it/s]\u001b[A\n",
            "Iteration:   1% 15/2561 [00:10<30:27,  1.39it/s]\u001b[A\n",
            "Iteration:   1% 16/2561 [00:11<30:22,  1.40it/s]\u001b[A\n",
            "Iteration:   1% 17/2561 [00:12<30:19,  1.40it/s]\u001b[A\n",
            "Iteration:   1% 18/2561 [00:12<30:12,  1.40it/s]\u001b[A\n",
            "Iteration:   1% 19/2561 [00:13<30:17,  1.40it/s]\u001b[A\n",
            "Iteration:   1% 20/2561 [00:14<30:17,  1.40it/s]\u001b[A\n",
            "Iteration:   1% 21/2561 [00:15<30:20,  1.40it/s]\u001b[A\n",
            "Iteration:   1% 22/2561 [00:15<30:35,  1.38it/s]\u001b[A\n",
            "Iteration:   1% 23/2561 [00:16<30:33,  1.38it/s]\u001b[A\n",
            "Iteration:   1% 24/2561 [00:17<30:26,  1.39it/s]\u001b[A\n",
            "Iteration:   1% 25/2561 [00:17<30:19,  1.39it/s]\u001b[A\n",
            "Iteration:   1% 26/2561 [00:18<30:30,  1.39it/s]\u001b[A\n",
            "Iteration:   1% 27/2561 [00:19<30:21,  1.39it/s]\u001b[A\n",
            "Iteration:   1% 28/2561 [00:20<30:22,  1.39it/s]\u001b[A\n",
            "Iteration:   1% 29/2561 [00:20<30:23,  1.39it/s]\u001b[A\n",
            "Iteration:   1% 30/2561 [00:21<30:22,  1.39it/s]\u001b[A\n",
            "Iteration:   1% 31/2561 [00:22<30:18,  1.39it/s]\u001b[A\n",
            "Iteration:   1% 32/2561 [00:22<30:16,  1.39it/s]\u001b[A\n",
            "Iteration:   1% 33/2561 [00:23<30:34,  1.38it/s]\u001b[A\n",
            "Iteration:   1% 34/2561 [00:24<30:29,  1.38it/s]\u001b[A\n",
            "Iteration:   1% 35/2561 [00:25<30:14,  1.39it/s]\u001b[A\n",
            "Iteration:   1% 36/2561 [00:25<30:21,  1.39it/s]\u001b[A\n",
            "Iteration:   1% 37/2561 [00:26<30:24,  1.38it/s]\u001b[A\n",
            "Iteration:   1% 38/2561 [00:27<31:54,  1.32it/s]\u001b[A\n",
            "Iteration:   2% 39/2561 [00:28<31:22,  1.34it/s]\u001b[A\n",
            "Iteration:   2% 40/2561 [00:28<31:19,  1.34it/s]\u001b[A\n",
            "Iteration:   2% 41/2561 [00:29<31:06,  1.35it/s]\u001b[A\n",
            "Iteration:   2% 42/2561 [00:30<30:49,  1.36it/s]\u001b[A\n",
            "Iteration:   2% 43/2561 [00:31<30:55,  1.36it/s]\u001b[A\n",
            "Iteration:   2% 44/2561 [00:31<30:45,  1.36it/s]\u001b[A\n",
            "Iteration:   2% 45/2561 [00:32<30:36,  1.37it/s]\u001b[A\n",
            "Iteration:   2% 46/2561 [00:33<30:40,  1.37it/s]\u001b[A\n",
            "Iteration:   2% 47/2561 [00:33<30:34,  1.37it/s]\u001b[A\n",
            "Iteration:   2% 48/2561 [00:34<30:29,  1.37it/s]\u001b[A\n",
            "Iteration:   2% 49/2561 [00:35<30:33,  1.37it/s]\u001b[A\n",
            "Iteration:   2% 50/2561 [00:36<30:34,  1.37it/s]\u001b[A\n",
            "Iteration:   2% 51/2561 [00:36<30:25,  1.38it/s]\u001b[A\n",
            "Iteration:   2% 52/2561 [00:37<30:35,  1.37it/s]\u001b[A\n",
            "Iteration:   2% 53/2561 [00:38<30:30,  1.37it/s]\u001b[A\n",
            "Iteration:   2% 54/2561 [00:39<30:42,  1.36it/s]\u001b[A\n",
            "Iteration:   2% 55/2561 [00:39<30:38,  1.36it/s]\u001b[A\n",
            "Iteration:   2% 56/2561 [00:40<30:28,  1.37it/s]\u001b[A\n",
            "Iteration:   2% 57/2561 [00:41<30:30,  1.37it/s]\u001b[A\n",
            "Iteration:   2% 58/2561 [00:42<30:32,  1.37it/s]\u001b[A\n",
            "Iteration:   2% 59/2561 [00:42<30:16,  1.38it/s]\u001b[A\n",
            "Iteration:   2% 60/2561 [00:43<30:27,  1.37it/s]\u001b[A\n",
            "Iteration:   2% 61/2561 [00:44<30:30,  1.37it/s]\u001b[A\n",
            "Iteration:   2% 62/2561 [00:44<30:20,  1.37it/s]\u001b[A\n",
            "Iteration:   2% 63/2561 [00:45<30:28,  1.37it/s]\u001b[A\n",
            "Iteration:   2% 64/2561 [00:46<30:21,  1.37it/s]\u001b[A\n",
            "Iteration:   3% 65/2561 [00:47<30:11,  1.38it/s]\u001b[A\n",
            "Iteration:   3% 66/2561 [00:47<30:19,  1.37it/s]\u001b[A\n",
            "Iteration:   3% 67/2561 [00:48<30:22,  1.37it/s]\u001b[A\n",
            "Iteration:   3% 68/2561 [00:49<30:08,  1.38it/s]\u001b[A\n",
            "Iteration:   3% 69/2561 [00:50<30:19,  1.37it/s]\u001b[A\n",
            "Iteration:   3% 70/2561 [00:50<30:19,  1.37it/s]\u001b[A\n",
            "Iteration:   3% 71/2561 [00:51<30:15,  1.37it/s]\u001b[A\n",
            "Iteration:   3% 72/2561 [00:52<30:24,  1.36it/s]\u001b[A\n",
            "Iteration:   3% 73/2561 [00:52<30:25,  1.36it/s]\u001b[A\n",
            "Iteration:   3% 74/2561 [00:53<30:27,  1.36it/s]\u001b[A\n",
            "Iteration:   3% 75/2561 [00:54<30:26,  1.36it/s]\u001b[A\n",
            "Iteration:   3% 76/2561 [00:55<30:17,  1.37it/s]\u001b[A\n",
            "Iteration:   3% 77/2561 [00:55<30:17,  1.37it/s]\u001b[A\n",
            "Iteration:   3% 78/2561 [00:56<30:15,  1.37it/s]\u001b[A\n",
            "Iteration:   3% 79/2561 [00:57<30:11,  1.37it/s]\u001b[A\n",
            "Iteration:   3% 80/2561 [00:58<30:14,  1.37it/s]\u001b[A\n",
            "Iteration:   3% 81/2561 [00:58<30:16,  1.37it/s]\u001b[A\n",
            "Iteration:   3% 82/2561 [00:59<30:04,  1.37it/s]\u001b[A\n",
            "Iteration:   3% 83/2561 [01:00<30:15,  1.36it/s]\u001b[A\n",
            "Iteration:   3% 84/2561 [01:01<30:16,  1.36it/s]\u001b[A\n",
            "Iteration:   3% 85/2561 [01:01<30:12,  1.37it/s]\u001b[A\n",
            "Iteration:   3% 86/2561 [01:02<30:12,  1.37it/s]\u001b[A\n",
            "Iteration:   3% 87/2561 [01:03<30:10,  1.37it/s]\u001b[A\n",
            "Iteration:   3% 88/2561 [01:03<30:16,  1.36it/s]\u001b[A\n",
            "Iteration:   3% 89/2561 [01:04<30:18,  1.36it/s]\u001b[A\n",
            "Iteration:   4% 90/2561 [01:05<30:13,  1.36it/s]\u001b[A\n",
            "Iteration:   4% 91/2561 [01:06<30:23,  1.35it/s]\u001b[A\n",
            "Iteration:   4% 92/2561 [01:06<30:18,  1.36it/s]\u001b[A\n",
            "Iteration:   4% 93/2561 [01:07<30:25,  1.35it/s]\u001b[A\n",
            "Iteration:   4% 94/2561 [01:08<30:26,  1.35it/s]\u001b[A\n",
            "Iteration:   4% 95/2561 [01:09<30:16,  1.36it/s]\u001b[A\n",
            "Iteration:   4% 96/2561 [01:09<30:20,  1.35it/s]\u001b[A\n",
            "Iteration:   4% 97/2561 [01:10<30:16,  1.36it/s]\u001b[A\n",
            "Iteration:   4% 98/2561 [01:11<30:18,  1.35it/s]\u001b[A\n",
            "Iteration:   4% 99/2561 [01:12<30:22,  1.35it/s]\u001b[A\n",
            "Iteration:   4% 100/2561 [01:12<30:14,  1.36it/s]\u001b[A\n",
            "Iteration:   4% 101/2561 [01:13<30:18,  1.35it/s]\u001b[A\n",
            "Iteration:   4% 102/2561 [01:14<30:14,  1.36it/s]\u001b[A\n",
            "Iteration:   4% 103/2561 [01:15<30:22,  1.35it/s]\u001b[A\n",
            "Iteration:   4% 104/2561 [01:15<30:25,  1.35it/s]\u001b[A\n",
            "Iteration:   4% 105/2561 [01:16<30:14,  1.35it/s]\u001b[A\n",
            "Iteration:   4% 106/2561 [01:17<30:15,  1.35it/s]\u001b[A\n",
            "Iteration:   4% 107/2561 [01:18<30:11,  1.35it/s]\u001b[A\n",
            "Iteration:   4% 108/2561 [01:18<30:19,  1.35it/s]\u001b[A\n",
            "Iteration:   4% 109/2561 [01:19<30:17,  1.35it/s]\u001b[A\n",
            "Iteration:   4% 110/2561 [01:20<30:09,  1.35it/s]\u001b[A\n",
            "Iteration:   4% 111/2561 [01:20<30:13,  1.35it/s]\u001b[A\n",
            "Iteration:   4% 112/2561 [01:21<30:08,  1.35it/s]\u001b[A\n",
            "Iteration:   4% 113/2561 [01:22<30:12,  1.35it/s]\u001b[A\n",
            "Iteration:   4% 114/2561 [01:23<30:10,  1.35it/s]\u001b[A\n",
            "Iteration:   4% 115/2561 [01:23<30:04,  1.36it/s]\u001b[A\n",
            "Iteration:   5% 116/2561 [01:24<30:07,  1.35it/s]\u001b[A\n",
            "Iteration:   5% 117/2561 [01:25<30:02,  1.36it/s]\u001b[A\n",
            "Iteration:   5% 118/2561 [01:26<30:11,  1.35it/s]\u001b[A\n",
            "Iteration:   5% 119/2561 [01:26<30:10,  1.35it/s]\u001b[A\n",
            "Iteration:   5% 120/2561 [01:27<30:09,  1.35it/s]\u001b[A\n",
            "Iteration:   5% 121/2561 [01:28<30:08,  1.35it/s]\u001b[A\n",
            "Iteration:   5% 122/2561 [01:29<30:02,  1.35it/s]\u001b[A\n",
            "Iteration:   5% 123/2561 [01:29<30:09,  1.35it/s]\u001b[A\n",
            "Iteration:   5% 124/2561 [01:30<30:07,  1.35it/s]\u001b[A\n",
            "Iteration:   5% 125/2561 [01:31<30:07,  1.35it/s]\u001b[A\n",
            "Iteration:   5% 126/2561 [01:32<30:05,  1.35it/s]\u001b[A\n",
            "Iteration:   5% 127/2561 [01:32<29:59,  1.35it/s]\u001b[A\n",
            "Iteration:   5% 128/2561 [01:33<30:04,  1.35it/s]\u001b[A\n",
            "Iteration:   5% 129/2561 [01:34<29:48,  1.36it/s]\u001b[A\n",
            "Iteration:   5% 130/2561 [01:35<29:44,  1.36it/s]\u001b[A\n",
            "Iteration:   5% 131/2561 [01:35<29:54,  1.35it/s]\u001b[A\n",
            "Iteration:   5% 132/2561 [01:36<29:51,  1.36it/s]\u001b[A\n",
            "Iteration:   5% 133/2561 [01:37<29:59,  1.35it/s]\u001b[A\n",
            "Iteration:   5% 134/2561 [01:37<29:59,  1.35it/s]\u001b[A\n",
            "Iteration:   5% 135/2561 [01:38<29:57,  1.35it/s]\u001b[A\n",
            "Iteration:   5% 136/2561 [01:39<30:02,  1.35it/s]\u001b[A\n",
            "Iteration:   5% 137/2561 [01:40<29:55,  1.35it/s]\u001b[A\n",
            "Iteration:   5% 138/2561 [01:40<30:00,  1.35it/s]\u001b[A\n",
            "Iteration:   5% 139/2561 [01:41<29:54,  1.35it/s]\u001b[A\n",
            "Iteration:   5% 140/2561 [01:42<30:01,  1.34it/s]\u001b[A\n",
            "Iteration:   6% 141/2561 [01:43<30:00,  1.34it/s]\u001b[A\n",
            "Iteration:   6% 142/2561 [01:43<29:50,  1.35it/s]\u001b[A\n",
            "Iteration:   6% 143/2561 [01:44<29:55,  1.35it/s]\u001b[A\n",
            "Iteration:   6% 144/2561 [01:45<29:51,  1.35it/s]\u001b[A\n",
            "Iteration:   6% 145/2561 [01:46<29:57,  1.34it/s]\u001b[A\n",
            "Iteration:   6% 146/2561 [01:46<29:54,  1.35it/s]\u001b[A\n",
            "Iteration:   6% 147/2561 [01:47<29:44,  1.35it/s]\u001b[A\n",
            "Iteration:   6% 148/2561 [01:48<29:47,  1.35it/s]\u001b[A\n",
            "Iteration:   6% 149/2561 [01:49<29:41,  1.35it/s]\u001b[A\n",
            "Iteration:   6% 150/2561 [01:49<29:49,  1.35it/s]\u001b[A\n",
            "Iteration:   6% 151/2561 [01:50<29:47,  1.35it/s]\u001b[A\n",
            "Iteration:   6% 152/2561 [01:51<29:47,  1.35it/s]\u001b[A\n",
            "Iteration:   6% 153/2561 [01:52<29:47,  1.35it/s]\u001b[A\n",
            "Iteration:   6% 154/2561 [01:52<29:42,  1.35it/s]\u001b[A\n",
            "Iteration:   6% 155/2561 [01:53<29:48,  1.35it/s]\u001b[A\n",
            "Iteration:   6% 156/2561 [01:54<29:41,  1.35it/s]\u001b[A\n",
            "Iteration:   6% 157/2561 [01:55<29:49,  1.34it/s]\u001b[A\n",
            "Iteration:   6% 158/2561 [01:55<29:49,  1.34it/s]\u001b[A\n",
            "Iteration:   6% 159/2561 [01:56<29:38,  1.35it/s]\u001b[A\n",
            "Iteration:   6% 160/2561 [01:57<29:43,  1.35it/s]\u001b[A\n",
            "Iteration:   6% 161/2561 [01:58<29:37,  1.35it/s]\u001b[A\n",
            "Iteration:   6% 162/2561 [01:58<29:46,  1.34it/s]\u001b[A\n",
            "Iteration:   6% 163/2561 [01:59<29:44,  1.34it/s]\u001b[A\n",
            "Iteration:   6% 164/2561 [02:00<29:34,  1.35it/s]\u001b[A\n",
            "Iteration:   6% 165/2561 [02:01<29:39,  1.35it/s]\u001b[A\n",
            "Iteration:   6% 166/2561 [02:01<29:32,  1.35it/s]\u001b[A\n",
            "Iteration:   7% 167/2561 [02:02<29:38,  1.35it/s]\u001b[A\n",
            "Iteration:   7% 168/2561 [02:03<29:41,  1.34it/s]\u001b[A\n",
            "Iteration:   7% 169/2561 [02:03<29:46,  1.34it/s]\u001b[A\n",
            "Iteration:   7% 170/2561 [02:04<29:44,  1.34it/s]\u001b[A\n",
            "Iteration:   7% 171/2561 [02:05<29:43,  1.34it/s]\u001b[A\n",
            "Iteration:   7% 172/2561 [02:06<29:41,  1.34it/s]\u001b[A\n",
            "Iteration:   7% 173/2561 [02:06<29:32,  1.35it/s]\u001b[A\n",
            "Iteration:   7% 174/2561 [02:07<29:36,  1.34it/s]\u001b[A\n",
            "Iteration:   7% 175/2561 [02:08<29:32,  1.35it/s]\u001b[A\n",
            "Iteration:   7% 176/2561 [02:09<29:30,  1.35it/s]\u001b[A\n",
            "Iteration:   7% 177/2561 [02:09<29:34,  1.34it/s]\u001b[A\n",
            "Iteration:   7% 178/2561 [02:10<29:25,  1.35it/s]\u001b[A\n",
            "Iteration:   7% 179/2561 [02:11<29:30,  1.35it/s]\u001b[A\n",
            "Iteration:   7% 180/2561 [02:12<29:24,  1.35it/s]\u001b[A\n",
            "Iteration:   7% 181/2561 [02:12<29:34,  1.34it/s]\u001b[A\n",
            "Iteration:   7% 182/2561 [02:13<29:33,  1.34it/s]\u001b[A\n",
            "Iteration:   7% 183/2561 [02:14<29:38,  1.34it/s]\u001b[A\n",
            "Iteration:   7% 184/2561 [02:15<29:36,  1.34it/s]\u001b[A\n",
            "Iteration:   7% 185/2561 [02:15<29:28,  1.34it/s]\u001b[A\n",
            "Iteration:   7% 186/2561 [02:16<29:29,  1.34it/s]\u001b[A\n",
            "Iteration:   7% 187/2561 [02:17<29:23,  1.35it/s]\u001b[A\n",
            "Iteration:   7% 188/2561 [02:18<29:27,  1.34it/s]\u001b[A\n",
            "Iteration:   7% 189/2561 [02:18<29:25,  1.34it/s]\u001b[A\n",
            "Iteration:   7% 190/2561 [02:19<29:23,  1.34it/s]\u001b[A\n",
            "Iteration:   7% 191/2561 [02:20<29:24,  1.34it/s]\u001b[A\n",
            "Iteration:   7% 192/2561 [02:21<29:18,  1.35it/s]\u001b[A\n",
            "Iteration:   8% 193/2561 [02:21<29:19,  1.35it/s]\u001b[A\n",
            "Iteration:   8% 194/2561 [02:22<29:14,  1.35it/s]\u001b[A\n",
            "Iteration:   8% 195/2561 [02:23<29:17,  1.35it/s]\u001b[A\n",
            "Iteration:   8% 196/2561 [02:24<29:20,  1.34it/s]\u001b[A\n",
            "Iteration:   8% 197/2561 [02:24<29:10,  1.35it/s]\u001b[A\n",
            "Iteration:   8% 198/2561 [02:25<29:15,  1.35it/s]\u001b[A\n",
            "Iteration:   8% 199/2561 [02:26<29:08,  1.35it/s]\u001b[A\n",
            "Iteration:   8% 200/2561 [02:27<29:15,  1.35it/s]\u001b[A\n",
            "Iteration:   8% 201/2561 [02:27<29:11,  1.35it/s]\u001b[A\n",
            "Iteration:   8% 202/2561 [02:28<29:02,  1.35it/s]\u001b[A\n",
            "Iteration:   8% 203/2561 [02:29<29:09,  1.35it/s]\u001b[A\n",
            "Iteration:   8% 204/2561 [02:29<29:07,  1.35it/s]\u001b[A\n",
            "Iteration:   8% 205/2561 [02:30<29:14,  1.34it/s]\u001b[A\n",
            "Iteration:   8% 206/2561 [02:31<29:13,  1.34it/s]\u001b[A\n",
            "Iteration:   8% 207/2561 [02:32<29:18,  1.34it/s]\u001b[A\n",
            "Iteration:   8% 208/2561 [02:32<29:17,  1.34it/s]\u001b[A\n",
            "Iteration:   8% 209/2561 [02:33<29:15,  1.34it/s]\u001b[A\n",
            "Iteration:   8% 210/2561 [02:34<29:13,  1.34it/s]\u001b[A\n",
            "Iteration:   8% 211/2561 [02:35<29:06,  1.35it/s]\u001b[A\n",
            "Iteration:   8% 212/2561 [02:35<29:12,  1.34it/s]\u001b[A\n",
            "Iteration:   8% 213/2561 [02:36<29:05,  1.35it/s]\u001b[A\n",
            "Iteration:   8% 214/2561 [02:37<28:57,  1.35it/s]\u001b[A\n",
            "Iteration:   8% 215/2561 [02:38<29:04,  1.34it/s]\u001b[A\n",
            "Iteration:   8% 216/2561 [02:38<28:55,  1.35it/s]\u001b[A\n",
            "Iteration:   8% 217/2561 [02:39<28:58,  1.35it/s]\u001b[A\n",
            "Iteration:   9% 218/2561 [02:40<28:57,  1.35it/s]\u001b[A\n",
            "Iteration:   9% 219/2561 [02:41<29:02,  1.34it/s]\u001b[A\n",
            "Iteration:   9% 220/2561 [02:41<29:01,  1.34it/s]\u001b[A\n",
            "Iteration:   9% 221/2561 [02:42<28:48,  1.35it/s]\u001b[A\n",
            "Iteration:   9% 222/2561 [02:43<28:54,  1.35it/s]\u001b[A\n",
            "Iteration:   9% 223/2561 [02:44<28:51,  1.35it/s]\u001b[A\n",
            "Iteration:   9% 224/2561 [02:44<28:55,  1.35it/s]\u001b[A\n",
            "Iteration:   9% 225/2561 [02:45<28:52,  1.35it/s]\u001b[A\n",
            "Iteration:   9% 226/2561 [02:46<28:54,  1.35it/s]\u001b[A\n",
            "Iteration:   9% 227/2561 [02:47<28:55,  1.34it/s]\u001b[A\n",
            "Iteration:   9% 228/2561 [02:47<28:49,  1.35it/s]\u001b[A\n",
            "Iteration:   9% 229/2561 [02:48<28:55,  1.34it/s]\u001b[A\n",
            "Iteration:   9% 230/2561 [02:49<28:51,  1.35it/s]\u001b[A\n",
            "Iteration:   9% 231/2561 [02:50<28:48,  1.35it/s]\u001b[A\n",
            "Iteration:   9% 232/2561 [02:50<28:47,  1.35it/s]\u001b[A\n",
            "Iteration:   9% 233/2561 [02:51<28:40,  1.35it/s]\u001b[A\n",
            "Iteration:   9% 234/2561 [02:52<28:46,  1.35it/s]\u001b[A\n",
            "Iteration:   9% 235/2561 [02:53<28:44,  1.35it/s]\u001b[A\n",
            "Iteration:   9% 236/2561 [02:53<28:52,  1.34it/s]\u001b[A\n",
            "Iteration:   9% 237/2561 [02:54<28:50,  1.34it/s]\u001b[A\n",
            "Iteration:   9% 238/2561 [02:55<28:42,  1.35it/s]\u001b[A\n",
            "Iteration:   9% 239/2561 [02:55<28:45,  1.35it/s]\u001b[A\n",
            "Iteration:   9% 240/2561 [02:56<28:38,  1.35it/s]\u001b[A\n",
            "Iteration:   9% 241/2561 [02:57<28:43,  1.35it/s]\u001b[A\n",
            "Iteration:   9% 242/2561 [02:58<28:39,  1.35it/s]\u001b[A\n",
            "Iteration:   9% 243/2561 [02:58<28:46,  1.34it/s]\u001b[A\n",
            "Iteration:  10% 244/2561 [02:59<28:44,  1.34it/s]\u001b[A\n",
            "Iteration:  10% 245/2561 [03:00<28:33,  1.35it/s]\u001b[A\n",
            "Iteration:  10% 246/2561 [03:01<28:39,  1.35it/s]\u001b[A\n",
            "Iteration:  10% 247/2561 [03:01<28:34,  1.35it/s]\u001b[A\n",
            "Iteration:  10% 248/2561 [03:02<28:30,  1.35it/s]\u001b[A\n",
            "Iteration:  10% 249/2561 [03:03<28:34,  1.35it/s]\u001b[A\n",
            "Iteration:  10% 250/2561 [03:04<28:30,  1.35it/s]\u001b[A\n",
            "Iteration:  10% 251/2561 [03:04<28:35,  1.35it/s]\u001b[A\n",
            "Iteration:  10% 252/2561 [03:05<28:30,  1.35it/s]\u001b[A\n",
            "Iteration:  10% 253/2561 [03:06<28:38,  1.34it/s]\u001b[A\n",
            "Iteration:  10% 254/2561 [03:07<28:33,  1.35it/s]\u001b[A\n",
            "Iteration:  10% 255/2561 [03:07<28:28,  1.35it/s]\u001b[A\n",
            "Iteration:  10% 256/2561 [03:08<28:33,  1.35it/s]\u001b[A\n",
            "Iteration:  10% 257/2561 [03:09<28:28,  1.35it/s]\u001b[A\n",
            "Iteration:  10% 258/2561 [03:10<28:34,  1.34it/s]\u001b[A\n",
            "Iteration:  10% 259/2561 [03:10<28:30,  1.35it/s]\u001b[A\n",
            "Iteration:  10% 260/2561 [03:11<28:29,  1.35it/s]\u001b[A\n",
            "Iteration:  10% 261/2561 [03:12<28:30,  1.34it/s]\u001b[A\n",
            "Iteration:  10% 262/2561 [03:13<28:22,  1.35it/s]\u001b[A\n",
            "Iteration:  10% 263/2561 [03:13<28:29,  1.34it/s]\u001b[A\n",
            "Iteration:  10% 264/2561 [03:14<28:23,  1.35it/s]\u001b[A\n",
            "Iteration:  10% 265/2561 [03:15<28:32,  1.34it/s]\u001b[A\n",
            "Iteration:  10% 266/2561 [03:16<28:31,  1.34it/s]\u001b[A\n",
            "Iteration:  10% 267/2561 [03:16<28:24,  1.35it/s]\u001b[A\n",
            "Iteration:  10% 268/2561 [03:17<28:28,  1.34it/s]\u001b[A\n",
            "Iteration:  11% 269/2561 [03:18<28:25,  1.34it/s]\u001b[A\n",
            "Iteration:  11% 270/2561 [03:19<28:30,  1.34it/s]\u001b[A\n",
            "Iteration:  11% 271/2561 [03:19<28:26,  1.34it/s]\u001b[A\n",
            "Iteration:  11% 272/2561 [03:20<28:30,  1.34it/s]\u001b[A\n",
            "Iteration:  11% 273/2561 [03:21<28:29,  1.34it/s]\u001b[A\n",
            "Iteration:  11% 274/2561 [03:22<28:19,  1.35it/s]\u001b[A\n",
            "Iteration:  11% 275/2561 [03:22<28:25,  1.34it/s]\u001b[A\n",
            "Iteration:  11% 276/2561 [03:23<28:19,  1.34it/s]\u001b[A\n",
            "Iteration:  11% 277/2561 [03:24<28:22,  1.34it/s]\u001b[A\n",
            "Iteration:  11% 278/2561 [03:24<28:19,  1.34it/s]\u001b[A\n",
            "Iteration:  11% 279/2561 [03:25<28:25,  1.34it/s]\u001b[A\n",
            "Iteration:  11% 280/2561 [03:26<28:26,  1.34it/s]\u001b[A\n",
            "Iteration:  11% 281/2561 [03:27<28:15,  1.34it/s]\u001b[A\n",
            "Iteration:  11% 282/2561 [03:27<28:16,  1.34it/s]\u001b[A\n",
            "Iteration:  11% 283/2561 [03:28<28:09,  1.35it/s]\u001b[A\n",
            "Iteration:  11% 284/2561 [03:29<28:17,  1.34it/s]\u001b[A\n",
            "Iteration:  11% 285/2561 [03:30<28:16,  1.34it/s]\u001b[A\n",
            "Iteration:  11% 286/2561 [03:30<28:21,  1.34it/s]\u001b[A\n",
            "Iteration:  11% 287/2561 [03:31<28:17,  1.34it/s]\u001b[A\n",
            "Iteration:  11% 288/2561 [03:32<28:08,  1.35it/s]\u001b[A\n",
            "Iteration:  11% 289/2561 [03:33<28:10,  1.34it/s]\u001b[A\n",
            "Iteration:  11% 290/2561 [03:33<28:03,  1.35it/s]\u001b[A\n",
            "Iteration:  11% 291/2561 [03:34<28:08,  1.34it/s]\u001b[A\n",
            "Iteration:  11% 292/2561 [03:35<28:05,  1.35it/s]\u001b[A\n",
            "Iteration:  11% 293/2561 [03:36<28:03,  1.35it/s]\u001b[A\n",
            "Iteration:  11% 294/2561 [03:36<28:03,  1.35it/s]\u001b[A\n",
            "Iteration:  12% 295/2561 [03:37<27:59,  1.35it/s]\u001b[A\n",
            "Iteration:  12% 296/2561 [03:38<28:04,  1.34it/s]\u001b[A\n",
            "Iteration:  12% 297/2561 [03:39<27:59,  1.35it/s]\u001b[A\n",
            "Iteration:  12% 298/2561 [03:39<28:05,  1.34it/s]\u001b[A\n",
            "Iteration:  12% 299/2561 [03:40<28:05,  1.34it/s]\u001b[A\n",
            "Iteration:  12% 300/2561 [03:41<27:55,  1.35it/s]\u001b[A\n",
            "Iteration:  12% 301/2561 [03:42<27:57,  1.35it/s]\u001b[A\n",
            "Iteration:  12% 302/2561 [03:42<27:53,  1.35it/s]\u001b[A\n",
            "Iteration:  12% 303/2561 [03:43<27:59,  1.34it/s]\u001b[A\n",
            "Iteration:  12% 304/2561 [03:44<27:56,  1.35it/s]\u001b[A\n",
            "Iteration:  12% 305/2561 [03:45<28:03,  1.34it/s]\u001b[A\n",
            "Iteration:  12% 306/2561 [03:45<28:01,  1.34it/s]\u001b[A\n",
            "Iteration:  12% 307/2561 [03:46<27:54,  1.35it/s]\u001b[A\n",
            "Iteration:  12% 308/2561 [03:47<27:57,  1.34it/s]\u001b[A\n",
            "Iteration:  12% 309/2561 [03:48<27:52,  1.35it/s]\u001b[A\n",
            "Iteration:  12% 310/2561 [03:48<27:56,  1.34it/s]\u001b[A\n",
            "Iteration:  12% 311/2561 [03:49<27:53,  1.34it/s]\u001b[A\n",
            "Iteration:  12% 312/2561 [03:50<27:52,  1.34it/s]\u001b[A\n",
            "Iteration:  12% 313/2561 [03:51<27:51,  1.35it/s]\u001b[A\n",
            "Iteration:  12% 314/2561 [03:51<27:40,  1.35it/s]\u001b[A\n",
            "Iteration:  12% 315/2561 [03:52<27:46,  1.35it/s]\u001b[A\n",
            "Iteration:  12% 316/2561 [03:53<27:47,  1.35it/s]\u001b[A\n",
            "Iteration:  12% 317/2561 [03:54<27:54,  1.34it/s]\u001b[A\n",
            "Iteration:  12% 318/2561 [03:54<27:49,  1.34it/s]\u001b[A\n",
            "Iteration:  12% 319/2561 [03:55<27:41,  1.35it/s]\u001b[A\n",
            "Iteration:  12% 320/2561 [03:56<27:39,  1.35it/s]\u001b[A\n",
            "Iteration:  13% 321/2561 [03:56<27:37,  1.35it/s]\u001b[A\n",
            "Iteration:  13% 322/2561 [03:57<27:44,  1.35it/s]\u001b[A\n",
            "Iteration:  13% 323/2561 [03:58<27:42,  1.35it/s]\u001b[A\n",
            "Iteration:  13% 324/2561 [03:59<27:40,  1.35it/s]\u001b[A\n",
            "Iteration:  13% 325/2561 [03:59<27:42,  1.34it/s]\u001b[A\n",
            "Iteration:  13% 326/2561 [04:00<27:35,  1.35it/s]\u001b[A\n",
            "Iteration:  13% 327/2561 [04:01<27:41,  1.34it/s]\u001b[A\n",
            "Iteration:  13% 328/2561 [04:02<27:38,  1.35it/s]\u001b[A\n",
            "Iteration:  13% 329/2561 [04:02<27:39,  1.35it/s]\u001b[A\n",
            "Iteration:  13% 330/2561 [04:03<27:37,  1.35it/s]\u001b[A\n",
            "Iteration:  13% 331/2561 [04:04<27:31,  1.35it/s]\u001b[A\n",
            "Iteration:  13% 332/2561 [04:05<27:35,  1.35it/s]\u001b[A\n",
            "Iteration:  13% 333/2561 [04:05<27:33,  1.35it/s]\u001b[A\n",
            "Iteration:  13% 334/2561 [04:06<27:40,  1.34it/s]\u001b[A\n",
            "Iteration:  13% 335/2561 [04:07<27:37,  1.34it/s]\u001b[A\n",
            "Iteration:  13% 336/2561 [04:08<27:43,  1.34it/s]\u001b[A\n",
            "Iteration:  13% 337/2561 [04:08<27:41,  1.34it/s]\u001b[A\n",
            "Iteration:  13% 338/2561 [04:09<27:31,  1.35it/s]\u001b[A\n",
            "Iteration:  13% 339/2561 [04:10<27:34,  1.34it/s]\u001b[A\n",
            "Iteration:  13% 340/2561 [04:11<27:29,  1.35it/s]\u001b[A\n",
            "Iteration:  13% 341/2561 [04:11<27:33,  1.34it/s]\u001b[A\n",
            "Iteration:  13% 342/2561 [04:12<27:30,  1.34it/s]\u001b[A\n",
            "Iteration:  13% 343/2561 [04:13<27:18,  1.35it/s]\u001b[A\n",
            "Iteration:  13% 344/2561 [04:14<27:23,  1.35it/s]\u001b[A\n",
            "Iteration:  13% 345/2561 [04:14<27:18,  1.35it/s]\u001b[A\n",
            "Iteration:  14% 346/2561 [04:15<27:26,  1.35it/s]\u001b[A\n",
            "Iteration:  14% 347/2561 [04:16<27:23,  1.35it/s]\u001b[A\n",
            "Iteration:  14% 348/2561 [04:17<27:30,  1.34it/s]\u001b[A\n",
            "Iteration:  14% 349/2561 [04:17<27:29,  1.34it/s]\u001b[A\n",
            "Iteration:  14% 350/2561 [04:18<27:23,  1.35it/s]\u001b[A\n",
            "Iteration:  14% 351/2561 [04:19<27:23,  1.34it/s]\u001b[A\n",
            "Iteration:  14% 352/2561 [04:19<27:18,  1.35it/s]\u001b[A\n",
            "Iteration:  14% 353/2561 [04:20<27:24,  1.34it/s]\u001b[A\n",
            "Iteration:  14% 354/2561 [04:21<27:21,  1.34it/s]\u001b[A\n",
            "Iteration:  14% 355/2561 [04:22<27:16,  1.35it/s]\u001b[A\n",
            "Iteration:  14% 356/2561 [04:22<27:19,  1.34it/s]\u001b[A\n",
            "Iteration:  14% 357/2561 [04:23<27:10,  1.35it/s]\u001b[A\n",
            "Iteration:  14% 358/2561 [04:24<27:16,  1.35it/s]\u001b[A\n",
            "Iteration:  14% 359/2561 [04:25<27:12,  1.35it/s]\u001b[A\n",
            "Iteration:  14% 360/2561 [04:25<27:19,  1.34it/s]\u001b[A\n",
            "Iteration:  14% 361/2561 [04:26<27:20,  1.34it/s]\u001b[A\n",
            "Iteration:  14% 362/2561 [04:27<27:10,  1.35it/s]\u001b[A\n",
            "Iteration:  14% 363/2561 [04:28<27:07,  1.35it/s]\u001b[A\n",
            "Iteration:  14% 364/2561 [04:28<27:05,  1.35it/s]\u001b[A\n",
            "Iteration:  14% 365/2561 [04:29<27:14,  1.34it/s]\u001b[A\n",
            "Iteration:  14% 366/2561 [04:30<27:14,  1.34it/s]\u001b[A\n",
            "Iteration:  14% 367/2561 [04:31<27:11,  1.34it/s]\u001b[A\n",
            "Iteration:  14% 368/2561 [04:31<27:11,  1.34it/s]\u001b[A\n",
            "Iteration:  14% 369/2561 [04:32<27:04,  1.35it/s]\u001b[A\n",
            "Iteration:  14% 370/2561 [04:33<27:09,  1.34it/s]\u001b[A\n",
            "Iteration:  14% 371/2561 [04:34<27:06,  1.35it/s]\u001b[A\n",
            "Iteration:  15% 372/2561 [04:34<27:11,  1.34it/s]\u001b[A\n",
            "Iteration:  15% 373/2561 [04:35<27:08,  1.34it/s]\u001b[A\n",
            "Iteration:  15% 374/2561 [04:36<27:00,  1.35it/s]\u001b[A\n",
            "Iteration:  15% 375/2561 [04:37<27:03,  1.35it/s]\u001b[A\n",
            "Iteration:  15% 376/2561 [04:37<27:00,  1.35it/s]\u001b[A\n",
            "Iteration:  15% 377/2561 [04:38<27:06,  1.34it/s]\u001b[A\n",
            "Iteration:  15% 378/2561 [04:39<27:05,  1.34it/s]\u001b[A\n",
            "Iteration:  15% 379/2561 [04:40<27:09,  1.34it/s]\u001b[A\n",
            "Iteration:  15% 380/2561 [04:40<27:08,  1.34it/s]\u001b[A\n",
            "Iteration:  15% 381/2561 [04:41<26:59,  1.35it/s]\u001b[A\n",
            "Iteration:  15% 382/2561 [04:42<27:01,  1.34it/s]\u001b[A\n",
            "Iteration:  15% 383/2561 [04:43<26:53,  1.35it/s]\u001b[A\n",
            "Iteration:  15% 384/2561 [04:43<27:00,  1.34it/s]\u001b[A\n",
            "Iteration:  15% 385/2561 [04:44<26:58,  1.34it/s]\u001b[A\n",
            "Iteration:  15% 386/2561 [04:45<26:53,  1.35it/s]\u001b[A\n",
            "Iteration:  15% 387/2561 [04:46<26:56,  1.34it/s]\u001b[A\n",
            "Iteration:  15% 388/2561 [04:46<26:50,  1.35it/s]\u001b[A\n",
            "Iteration:  15% 389/2561 [04:47<26:56,  1.34it/s]\u001b[A\n",
            "Iteration:  15% 390/2561 [04:48<26:53,  1.35it/s]\u001b[A\n",
            "Iteration:  15% 391/2561 [04:48<26:58,  1.34it/s]\u001b[A\n",
            "Iteration:  15% 392/2561 [04:49<26:52,  1.35it/s]\u001b[A\n",
            "Iteration:  15% 393/2561 [04:50<26:44,  1.35it/s]\u001b[A\n",
            "Iteration:  15% 394/2561 [04:51<26:47,  1.35it/s]\u001b[A\n",
            "Iteration:  15% 395/2561 [04:51<26:44,  1.35it/s]\u001b[A\n",
            "Iteration:  15% 396/2561 [04:52<26:51,  1.34it/s]\u001b[A\n",
            "Iteration:  16% 397/2561 [04:53<26:50,  1.34it/s]\u001b[A\n",
            "Iteration:  16% 398/2561 [04:54<26:57,  1.34it/s]\u001b[A\n",
            "Iteration:  16% 399/2561 [04:54<26:56,  1.34it/s]\u001b[A\n",
            "Iteration:  16% 400/2561 [04:55<26:43,  1.35it/s]\u001b[A\n",
            "Iteration:  16% 401/2561 [04:56<26:46,  1.34it/s]\u001b[A\n",
            "Iteration:  16% 402/2561 [04:57<26:40,  1.35it/s]\u001b[A\n",
            "Iteration:  16% 403/2561 [04:57<26:45,  1.34it/s]\u001b[A\n",
            "Iteration:  16% 404/2561 [04:58<26:43,  1.35it/s]\u001b[A\n",
            "Iteration:  16% 405/2561 [04:59<26:44,  1.34it/s]\u001b[A\n",
            "Iteration:  16% 406/2561 [05:00<26:42,  1.34it/s]\u001b[A\n",
            "Iteration:  16% 407/2561 [05:00<26:35,  1.35it/s]\u001b[A\n",
            "Iteration:  16% 408/2561 [05:01<26:40,  1.35it/s]\u001b[A\n",
            "Iteration:  16% 409/2561 [05:02<26:37,  1.35it/s]\u001b[A\n",
            "Iteration:  16% 410/2561 [05:03<26:42,  1.34it/s]\u001b[A\n",
            "Iteration:  16% 411/2561 [05:03<26:42,  1.34it/s]\u001b[A\n",
            "Iteration:  16% 412/2561 [05:04<26:34,  1.35it/s]\u001b[A\n",
            "Iteration:  16% 413/2561 [05:05<26:36,  1.35it/s]\u001b[A\n",
            "Iteration:  16% 414/2561 [05:06<26:30,  1.35it/s]\u001b[A\n",
            "Iteration:  16% 415/2561 [05:06<26:35,  1.35it/s]\u001b[A\n",
            "Iteration:  16% 416/2561 [05:07<26:32,  1.35it/s]\u001b[A\n",
            "Iteration:  16% 417/2561 [05:08<26:29,  1.35it/s]\u001b[A\n",
            "Iteration:  16% 418/2561 [05:09<26:26,  1.35it/s]\u001b[A\n",
            "Iteration:  16% 419/2561 [05:09<26:23,  1.35it/s]\u001b[A\n",
            "Iteration:  16% 420/2561 [05:10<26:28,  1.35it/s]\u001b[A\n",
            "Iteration:  16% 421/2561 [05:11<26:28,  1.35it/s]\u001b[A\n",
            "Iteration:  16% 422/2561 [05:12<26:30,  1.35it/s]\u001b[A\n",
            "Iteration:  17% 423/2561 [05:12<26:26,  1.35it/s]\u001b[A\n",
            "Iteration:  17% 424/2561 [05:13<26:21,  1.35it/s]\u001b[A\n",
            "Iteration:  17% 425/2561 [05:14<26:29,  1.34it/s]\u001b[A\n",
            "Iteration:  17% 426/2561 [05:14<26:25,  1.35it/s]\u001b[A\n",
            "Iteration:  17% 427/2561 [05:15<26:32,  1.34it/s]\u001b[A\n",
            "Iteration:  17% 428/2561 [05:16<26:27,  1.34it/s]\u001b[A\n",
            "Iteration:  17% 429/2561 [05:17<26:26,  1.34it/s]\u001b[A\n",
            "Iteration:  17% 430/2561 [05:17<26:26,  1.34it/s]\u001b[A\n",
            "Iteration:  17% 431/2561 [05:18<26:18,  1.35it/s]\u001b[A\n",
            "Iteration:  17% 432/2561 [05:19<26:23,  1.34it/s]\u001b[A\n",
            "Iteration:  17% 433/2561 [05:20<26:19,  1.35it/s]\u001b[A\n",
            "Iteration:  17% 434/2561 [05:20<26:26,  1.34it/s]\u001b[A\n",
            "Iteration:  17% 435/2561 [05:21<26:24,  1.34it/s]\u001b[A\n",
            "Iteration:  17% 436/2561 [05:22<26:27,  1.34it/s]\u001b[A\n",
            "Iteration:  17% 437/2561 [05:23<26:26,  1.34it/s]\u001b[A\n",
            "Iteration:  17% 438/2561 [05:23<26:15,  1.35it/s]\u001b[A\n",
            "Iteration:  17% 439/2561 [05:24<26:19,  1.34it/s]\u001b[A\n",
            "Iteration:  17% 440/2561 [05:25<26:15,  1.35it/s]\u001b[A\n",
            "Iteration:  17% 441/2561 [05:26<26:20,  1.34it/s]\u001b[A\n",
            "Iteration:  17% 442/2561 [05:26<26:20,  1.34it/s]\u001b[A\n",
            "Iteration:  17% 443/2561 [05:27<26:23,  1.34it/s]\u001b[A\n",
            "Iteration:  17% 444/2561 [05:28<26:22,  1.34it/s]\u001b[A\n",
            "Iteration:  17% 445/2561 [05:29<26:10,  1.35it/s]\u001b[A\n",
            "Iteration:  17% 446/2561 [05:29<26:13,  1.34it/s]\u001b[A\n",
            "Iteration:  17% 447/2561 [05:30<26:08,  1.35it/s]\u001b[A\n",
            "Iteration:  17% 448/2561 [05:31<26:16,  1.34it/s]\u001b[A\n",
            "Iteration:  18% 449/2561 [05:32<26:09,  1.35it/s]\u001b[A\n",
            "Iteration:  18% 450/2561 [05:32<26:07,  1.35it/s]\u001b[A\n",
            "Iteration:  18% 451/2561 [05:33<26:08,  1.35it/s]\u001b[A\n",
            "Iteration:  18% 452/2561 [05:34<26:00,  1.35it/s]\u001b[A\n",
            "Iteration:  18% 453/2561 [05:35<26:04,  1.35it/s]\u001b[A\n",
            "Iteration:  18% 454/2561 [05:35<26:03,  1.35it/s]\u001b[A\n",
            "Iteration:  18% 455/2561 [05:36<26:02,  1.35it/s]\u001b[A\n",
            "Iteration:  18% 456/2561 [05:37<26:05,  1.34it/s]\u001b[A\n",
            "Iteration:  18% 457/2561 [05:38<25:58,  1.35it/s]\u001b[A\n",
            "Iteration:  18% 458/2561 [05:38<26:02,  1.35it/s]\u001b[A\n",
            "Iteration:  18% 459/2561 [05:39<25:58,  1.35it/s]\u001b[A\n",
            "Iteration:  18% 460/2561 [05:40<26:03,  1.34it/s]\u001b[A\n",
            "Iteration:  18% 461/2561 [05:41<26:01,  1.35it/s]\u001b[A\n",
            "Iteration:  18% 462/2561 [05:41<26:07,  1.34it/s]\u001b[A\n",
            "Iteration:  18% 463/2561 [05:42<26:04,  1.34it/s]\u001b[A\n",
            "Iteration:  18% 464/2561 [05:43<25:57,  1.35it/s]\u001b[A\n",
            "Iteration:  18% 465/2561 [05:44<26:01,  1.34it/s]\u001b[A\n",
            "Iteration:  18% 466/2561 [05:44<25:56,  1.35it/s]\u001b[A\n",
            "Iteration:  18% 467/2561 [05:45<26:05,  1.34it/s]\u001b[A\n",
            "Iteration:  18% 468/2561 [05:46<26:01,  1.34it/s]\u001b[A\n",
            "Iteration:  18% 469/2561 [05:46<26:00,  1.34it/s]\u001b[A\n",
            "Iteration:  18% 470/2561 [05:47<25:58,  1.34it/s]\u001b[A\n",
            "Iteration:  18% 471/2561 [05:48<25:51,  1.35it/s]\u001b[A\n",
            "Iteration:  18% 472/2561 [05:49<25:52,  1.35it/s]\u001b[A\n",
            "Iteration:  18% 473/2561 [05:49<25:48,  1.35it/s]\u001b[A\n",
            "Iteration:  19% 474/2561 [05:50<25:55,  1.34it/s]\u001b[A\n",
            "Iteration:  19% 475/2561 [05:51<25:54,  1.34it/s]\u001b[A\n",
            "Iteration:  19% 476/2561 [05:52<25:47,  1.35it/s]\u001b[A\n",
            "Iteration:  19% 477/2561 [05:52<25:51,  1.34it/s]\u001b[A\n",
            "Iteration:  19% 478/2561 [05:53<25:50,  1.34it/s]\u001b[A\n",
            "Iteration:  19% 479/2561 [05:54<25:52,  1.34it/s]\u001b[A\n",
            "Iteration:  19% 480/2561 [05:55<25:49,  1.34it/s]\u001b[A\n",
            "Iteration:  19% 481/2561 [05:55<25:55,  1.34it/s]\u001b[A\n",
            "Iteration:  19% 482/2561 [05:56<25:50,  1.34it/s]\u001b[A\n",
            "Iteration:  19% 483/2561 [05:57<25:42,  1.35it/s]\u001b[A\n",
            "Iteration:  19% 484/2561 [05:58<25:45,  1.34it/s]\u001b[A\n",
            "Iteration:  19% 485/2561 [05:58<25:41,  1.35it/s]\u001b[A\n",
            "Iteration:  19% 486/2561 [05:59<25:46,  1.34it/s]\u001b[A\n",
            "Iteration:  19% 487/2561 [06:00<25:41,  1.35it/s]\u001b[A\n",
            "Iteration:  19% 488/2561 [06:01<25:45,  1.34it/s]\u001b[A\n",
            "Iteration:  19% 489/2561 [06:01<25:47,  1.34it/s]\u001b[A\n",
            "Iteration:  19% 490/2561 [06:02<25:39,  1.35it/s]\u001b[A\n",
            "Iteration:  19% 491/2561 [06:03<25:41,  1.34it/s]\u001b[A\n",
            "Iteration:  19% 492/2561 [06:04<25:37,  1.35it/s]\u001b[A\n",
            "Iteration:  19% 493/2561 [06:04<25:42,  1.34it/s]\u001b[A\n",
            "Iteration:  19% 494/2561 [06:05<25:39,  1.34it/s]\u001b[A\n",
            "Iteration:  19% 495/2561 [06:06<25:43,  1.34it/s]\u001b[A\n",
            "Iteration:  19% 496/2561 [06:07<25:42,  1.34it/s]\u001b[A\n",
            "Iteration:  19% 497/2561 [06:07<25:41,  1.34it/s]\u001b[A\n",
            "Iteration:  19% 498/2561 [06:08<25:40,  1.34it/s]\u001b[A\n",
            "Iteration:  19% 499/2561 [06:09<25:32,  1.35it/s]\u001b[A/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "02/29/2020 07:34:18 - INFO - transformers.configuration_utils -   Configuration saved in output_roberta_GB/checkpoint-500/config.json\n",
            "02/29/2020 07:34:20 - INFO - transformers.modeling_utils -   Model weights saved in output_roberta_GB/checkpoint-500/pytorch_model.bin\n",
            "02/29/2020 07:34:20 - INFO - __main__ -   Saving model checkpoint to output_roberta_GB/checkpoint-500\n",
            "02/29/2020 07:34:23 - INFO - __main__ -   Saving optimizer and scheduler states to output_roberta_GB/checkpoint-500\n",
            "\n",
            "Iteration:  20% 500/2561 [06:15<1:19:09,  2.30s/it]\u001b[A\n",
            "Iteration:  20% 501/2561 [06:16<1:03:05,  1.84s/it]\u001b[A\n",
            "Iteration:  20% 502/2561 [06:16<51:51,  1.51s/it]  \u001b[A\n",
            "Iteration:  20% 503/2561 [06:17<43:52,  1.28s/it]\u001b[A\n",
            "Iteration:  20% 504/2561 [06:18<38:24,  1.12s/it]\u001b[A\n",
            "Iteration:  20% 505/2561 [06:19<34:30,  1.01s/it]\u001b[A\n",
            "Iteration:  20% 506/2561 [06:19<31:40,  1.08it/s]\u001b[A\n",
            "Iteration:  20% 507/2561 [06:20<29:51,  1.15it/s]\u001b[A\n",
            "Iteration:  20% 508/2561 [06:21<28:26,  1.20it/s]\u001b[A\n",
            "Iteration:  20% 509/2561 [06:21<27:36,  1.24it/s]\u001b[A\n",
            "Iteration:  20% 510/2561 [06:22<26:54,  1.27it/s]\u001b[A\n",
            "Iteration:  20% 511/2561 [06:23<26:36,  1.28it/s]\u001b[A\n",
            "Iteration:  20% 512/2561 [06:24<26:13,  1.30it/s]\u001b[A\n",
            "Iteration:  20% 513/2561 [06:24<25:52,  1.32it/s]\u001b[A\n",
            "Iteration:  20% 514/2561 [06:25<25:46,  1.32it/s]\u001b[A\n",
            "Iteration:  20% 515/2561 [06:26<25:34,  1.33it/s]\u001b[A\n",
            "Iteration:  20% 516/2561 [06:27<25:36,  1.33it/s]\u001b[A\n",
            "Iteration:  20% 517/2561 [06:27<25:28,  1.34it/s]\u001b[A\n",
            "Iteration:  20% 518/2561 [06:28<25:31,  1.33it/s]\u001b[A\n",
            "Iteration:  20% 519/2561 [06:29<25:28,  1.34it/s]\u001b[A\n",
            "Iteration:  20% 520/2561 [06:30<25:24,  1.34it/s]\u001b[A\n",
            "Iteration:  20% 521/2561 [06:30<25:23,  1.34it/s]\u001b[A\n",
            "Iteration:  20% 522/2561 [06:31<25:16,  1.34it/s]\u001b[A\n",
            "Iteration:  20% 523/2561 [06:32<25:19,  1.34it/s]\u001b[A\n",
            "Iteration:  20% 524/2561 [06:33<25:18,  1.34it/s]\u001b[A\n",
            "Iteration:  20% 525/2561 [06:33<25:21,  1.34it/s]\u001b[A\n",
            "Iteration:  21% 526/2561 [06:34<25:17,  1.34it/s]\u001b[A\n",
            "Iteration:  21% 527/2561 [06:35<25:08,  1.35it/s]\u001b[A\n",
            "Iteration:  21% 528/2561 [06:36<25:12,  1.34it/s]\u001b[A\n",
            "Iteration:  21% 529/2561 [06:36<25:09,  1.35it/s]\u001b[A\n",
            "Iteration:  21% 530/2561 [06:37<25:15,  1.34it/s]\u001b[A\n",
            "Iteration:  21% 531/2561 [06:38<25:13,  1.34it/s]\u001b[A\n",
            "Iteration:  21% 532/2561 [06:39<25:18,  1.34it/s]\u001b[A\n",
            "Iteration:  21% 533/2561 [06:39<25:16,  1.34it/s]\u001b[A\n",
            "Iteration:  21% 534/2561 [06:40<25:07,  1.34it/s]\u001b[A\n",
            "Iteration:  21% 535/2561 [06:41<25:10,  1.34it/s]\u001b[A\n",
            "Iteration:  21% 536/2561 [06:42<25:08,  1.34it/s]\u001b[A\n",
            "Iteration:  21% 537/2561 [06:42<25:13,  1.34it/s]\u001b[A\n",
            "Iteration:  21% 538/2561 [06:43<25:09,  1.34it/s]\u001b[A\n",
            "Iteration:  21% 539/2561 [06:44<25:15,  1.33it/s]\u001b[A\n",
            "Iteration:  21% 540/2561 [06:45<25:15,  1.33it/s]\u001b[A\n",
            "Iteration:  21% 541/2561 [06:45<25:17,  1.33it/s]\u001b[A\n",
            "Iteration:  21% 542/2561 [06:46<25:12,  1.33it/s]\u001b[A\n",
            "Iteration:  21% 543/2561 [06:47<25:04,  1.34it/s]\u001b[A\n",
            "Iteration:  21% 544/2561 [06:48<25:05,  1.34it/s]\u001b[A\n",
            "Iteration:  21% 545/2561 [06:48<25:02,  1.34it/s]\u001b[A\n",
            "Iteration:  21% 546/2561 [06:49<25:09,  1.34it/s]\u001b[A\n",
            "Iteration:  21% 547/2561 [06:50<25:08,  1.33it/s]\u001b[A\n",
            "Iteration:  21% 548/2561 [06:51<25:11,  1.33it/s]\u001b[A\n",
            "Iteration:  21% 549/2561 [06:51<25:07,  1.34it/s]\u001b[A\n",
            "Iteration:  21% 550/2561 [06:52<25:12,  1.33it/s]\u001b[A\n",
            "Iteration:  22% 551/2561 [06:53<25:10,  1.33it/s]\u001b[A\n",
            "Iteration:  22% 552/2561 [06:54<25:00,  1.34it/s]\u001b[A\n",
            "Iteration:  22% 553/2561 [06:54<25:01,  1.34it/s]\u001b[A\n",
            "Iteration:  22% 554/2561 [06:55<24:57,  1.34it/s]\u001b[A\n",
            "Iteration:  22% 555/2561 [06:56<25:01,  1.34it/s]\u001b[A\n",
            "Iteration:  22% 556/2561 [06:57<24:59,  1.34it/s]\u001b[A\n",
            "Iteration:  22% 557/2561 [06:57<25:00,  1.34it/s]\u001b[A\n",
            "Iteration:  22% 558/2561 [06:58<24:57,  1.34it/s]\u001b[A\n",
            "Iteration:  22% 559/2561 [06:59<24:48,  1.34it/s]\u001b[A\n",
            "Iteration:  22% 560/2561 [07:00<24:50,  1.34it/s]\u001b[A\n",
            "Iteration:  22% 561/2561 [07:00<24:48,  1.34it/s]\u001b[A\n",
            "Iteration:  22% 562/2561 [07:01<24:55,  1.34it/s]\u001b[A\n",
            "Iteration:  22% 563/2561 [07:02<24:52,  1.34it/s]\u001b[A\n",
            "Iteration:  22% 564/2561 [07:03<24:49,  1.34it/s]\u001b[A\n",
            "Iteration:  22% 565/2561 [07:03<24:49,  1.34it/s]\u001b[A\n",
            "Iteration:  22% 566/2561 [07:04<24:42,  1.35it/s]\u001b[A\n",
            "Iteration:  22% 567/2561 [07:05<24:47,  1.34it/s]\u001b[A\n",
            "Iteration:  22% 568/2561 [07:06<24:43,  1.34it/s]\u001b[A\n",
            "Iteration:  22% 569/2561 [07:06<24:48,  1.34it/s]\u001b[A\n",
            "Iteration:  22% 570/2561 [07:07<24:46,  1.34it/s]\u001b[A\n",
            "Iteration:  22% 571/2561 [07:08<24:50,  1.33it/s]\u001b[A\n",
            "Iteration:  22% 572/2561 [07:09<24:47,  1.34it/s]\u001b[A\n",
            "Iteration:  22% 573/2561 [07:09<24:38,  1.34it/s]\u001b[A\n",
            "Iteration:  22% 574/2561 [07:10<24:40,  1.34it/s]\u001b[A\n",
            "Iteration:  22% 575/2561 [07:11<24:35,  1.35it/s]\u001b[A\n",
            "Iteration:  22% 576/2561 [07:11<24:40,  1.34it/s]\u001b[A\n",
            "Iteration:  23% 577/2561 [07:12<24:38,  1.34it/s]\u001b[A\n",
            "Iteration:  23% 578/2561 [07:13<24:43,  1.34it/s]\u001b[A\n",
            "Iteration:  23% 579/2561 [07:14<24:43,  1.34it/s]\u001b[A\n",
            "Iteration:  23% 580/2561 [07:14<24:33,  1.34it/s]\u001b[A\n",
            "Iteration:  23% 581/2561 [07:15<24:36,  1.34it/s]\u001b[A\n",
            "Iteration:  23% 582/2561 [07:16<24:29,  1.35it/s]\u001b[A\n",
            "Iteration:  23% 583/2561 [07:17<24:33,  1.34it/s]\u001b[A\n",
            "Iteration:  23% 584/2561 [07:17<24:29,  1.35it/s]\u001b[A\n",
            "Iteration:  23% 585/2561 [07:18<24:30,  1.34it/s]\u001b[A\n",
            "Iteration:  23% 586/2561 [07:19<24:30,  1.34it/s]\u001b[A\n",
            "Iteration:  23% 587/2561 [07:20<24:23,  1.35it/s]\u001b[A\n",
            "Iteration:  23% 588/2561 [07:20<24:28,  1.34it/s]\u001b[A\n",
            "Iteration:  23% 589/2561 [07:21<24:21,  1.35it/s]\u001b[A\n",
            "Iteration:  23% 590/2561 [07:22<24:26,  1.34it/s]\u001b[A\n",
            "Iteration:  23% 591/2561 [07:23<24:25,  1.34it/s]\u001b[A\n",
            "Iteration:  23% 592/2561 [07:23<24:25,  1.34it/s]\u001b[A\n",
            "Iteration:  23% 593/2561 [07:24<24:26,  1.34it/s]\u001b[A\n",
            "Iteration:  23% 594/2561 [07:25<24:19,  1.35it/s]\u001b[A\n",
            "Iteration:  23% 595/2561 [07:26<24:22,  1.34it/s]\u001b[A\n",
            "Iteration:  23% 596/2561 [07:26<24:18,  1.35it/s]\u001b[A\n",
            "Iteration:  23% 597/2561 [07:27<24:24,  1.34it/s]\u001b[A\n",
            "Iteration:  23% 598/2561 [07:28<24:20,  1.34it/s]\u001b[A\n",
            "Iteration:  23% 599/2561 [07:29<24:26,  1.34it/s]\u001b[A\n",
            "Iteration:  23% 600/2561 [07:29<24:29,  1.33it/s]\u001b[A\n",
            "Iteration:  23% 601/2561 [07:30<24:16,  1.35it/s]\u001b[A\n",
            "Iteration:  24% 602/2561 [07:31<24:13,  1.35it/s]\u001b[A\n",
            "Iteration:  24% 603/2561 [07:32<24:13,  1.35it/s]\u001b[A\n",
            "Iteration:  24% 604/2561 [07:32<24:19,  1.34it/s]\u001b[A\n",
            "Iteration:  24% 605/2561 [07:33<24:17,  1.34it/s]\u001b[A\n",
            "Iteration:  24% 606/2561 [07:34<24:20,  1.34it/s]\u001b[A\n",
            "Iteration:  24% 607/2561 [07:35<24:18,  1.34it/s]\u001b[A\n",
            "Iteration:  24% 608/2561 [07:35<24:11,  1.35it/s]\u001b[A\n",
            "Iteration:  24% 609/2561 [07:36<24:13,  1.34it/s]\u001b[A\n",
            "Iteration:  24% 610/2561 [07:37<24:08,  1.35it/s]\u001b[A\n",
            "Iteration:  24% 611/2561 [07:38<24:12,  1.34it/s]\u001b[A\n",
            "Iteration:  24% 612/2561 [07:38<24:08,  1.35it/s]\u001b[A\n",
            "Iteration:  24% 613/2561 [07:39<24:06,  1.35it/s]\u001b[A\n",
            "Iteration:  24% 614/2561 [07:40<24:08,  1.34it/s]\u001b[A\n",
            "Iteration:  24% 615/2561 [07:40<24:02,  1.35it/s]\u001b[A\n",
            "Iteration:  24% 616/2561 [07:41<24:05,  1.35it/s]\u001b[A\n",
            "Iteration:  24% 617/2561 [07:42<23:59,  1.35it/s]\u001b[A\n",
            "Iteration:  24% 618/2561 [07:43<24:05,  1.34it/s]\u001b[A\n",
            "Iteration:  24% 619/2561 [07:43<24:07,  1.34it/s]\u001b[A\n",
            "Iteration:  24% 620/2561 [07:44<24:01,  1.35it/s]\u001b[A\n",
            "Iteration:  24% 621/2561 [07:45<24:05,  1.34it/s]\u001b[A\n",
            "Iteration:  24% 622/2561 [07:46<24:01,  1.34it/s]\u001b[A\n",
            "Iteration:  24% 623/2561 [07:46<24:07,  1.34it/s]\u001b[A\n",
            "Iteration:  24% 624/2561 [07:47<24:03,  1.34it/s]\u001b[A\n",
            "Iteration:  24% 625/2561 [07:48<24:06,  1.34it/s]\u001b[A\n",
            "Iteration:  24% 626/2561 [07:49<24:03,  1.34it/s]\u001b[A\n",
            "Iteration:  24% 627/2561 [07:49<23:54,  1.35it/s]\u001b[A\n",
            "Iteration:  25% 628/2561 [07:50<23:58,  1.34it/s]\u001b[A\n",
            "Iteration:  25% 629/2561 [07:51<23:55,  1.35it/s]\u001b[A\n",
            "Iteration:  25% 630/2561 [07:52<23:59,  1.34it/s]\u001b[A\n",
            "Iteration:  25% 631/2561 [07:52<23:58,  1.34it/s]\u001b[A\n",
            "Iteration:  25% 632/2561 [07:53<24:01,  1.34it/s]\u001b[A\n",
            "Iteration:  25% 633/2561 [07:54<24:02,  1.34it/s]\u001b[A\n",
            "Iteration:  25% 634/2561 [07:55<23:56,  1.34it/s]\u001b[A\n",
            "Iteration:  25% 635/2561 [07:55<23:59,  1.34it/s]\u001b[A\n",
            "Iteration:  25% 636/2561 [07:56<23:51,  1.34it/s]\u001b[A\n",
            "Iteration:  25% 637/2561 [07:57<23:55,  1.34it/s]\u001b[A\n",
            "Iteration:  25% 638/2561 [07:58<23:51,  1.34it/s]\u001b[A\n",
            "Iteration:  25% 639/2561 [07:58<23:54,  1.34it/s]\u001b[A\n",
            "Iteration:  25% 640/2561 [07:59<23:53,  1.34it/s]\u001b[A\n",
            "Iteration:  25% 641/2561 [08:00<23:49,  1.34it/s]\u001b[A\n",
            "Iteration:  25% 642/2561 [08:01<23:50,  1.34it/s]\u001b[A\n",
            "Iteration:  25% 643/2561 [08:01<23:47,  1.34it/s]\u001b[A\n",
            "Iteration:  25% 644/2561 [08:02<23:50,  1.34it/s]\u001b[A\n",
            "Iteration:  25% 645/2561 [08:03<23:44,  1.34it/s]\u001b[A\n",
            "Iteration:  25% 646/2561 [08:04<23:49,  1.34it/s]\u001b[A\n",
            "Iteration:  25% 647/2561 [08:04<23:48,  1.34it/s]\u001b[A\n",
            "Iteration:  25% 648/2561 [08:05<23:53,  1.33it/s]\u001b[A\n",
            "Iteration:  25% 649/2561 [08:06<23:50,  1.34it/s]\u001b[A\n",
            "Iteration:  25% 650/2561 [08:07<23:42,  1.34it/s]\u001b[A\n",
            "Iteration:  25% 651/2561 [08:07<23:43,  1.34it/s]\u001b[A\n",
            "Iteration:  25% 652/2561 [08:08<23:39,  1.35it/s]\u001b[A\n",
            "Iteration:  25% 653/2561 [08:09<23:43,  1.34it/s]\u001b[A\n",
            "Iteration:  26% 654/2561 [08:10<23:41,  1.34it/s]\u001b[A\n",
            "Iteration:  26% 655/2561 [08:10<23:45,  1.34it/s]\u001b[A\n",
            "Iteration:  26% 656/2561 [08:11<23:44,  1.34it/s]\u001b[A\n",
            "Iteration:  26% 657/2561 [08:12<23:36,  1.34it/s]\u001b[A\n",
            "Iteration:  26% 658/2561 [08:13<23:40,  1.34it/s]\u001b[A\n",
            "Iteration:  26% 659/2561 [08:13<23:34,  1.34it/s]\u001b[A\n",
            "Iteration:  26% 660/2561 [08:14<23:38,  1.34it/s]\u001b[A\n",
            "Iteration:  26% 661/2561 [08:15<23:36,  1.34it/s]\u001b[A\n",
            "Iteration:  26% 662/2561 [08:16<23:42,  1.33it/s]\u001b[A\n",
            "Iteration:  26% 663/2561 [08:16<23:39,  1.34it/s]\u001b[A\n",
            "Iteration:  26% 664/2561 [08:17<23:37,  1.34it/s]\u001b[A\n",
            "Iteration:  26% 665/2561 [08:18<23:36,  1.34it/s]\u001b[A\n",
            "Iteration:  26% 666/2561 [08:19<23:29,  1.34it/s]\u001b[A\n",
            "Iteration:  26% 667/2561 [08:19<23:31,  1.34it/s]\u001b[A\n",
            "Iteration:  26% 668/2561 [08:20<23:26,  1.35it/s]\u001b[A\n",
            "Iteration:  26% 669/2561 [08:21<23:31,  1.34it/s]\u001b[A\n",
            "Iteration:  26% 670/2561 [08:22<23:30,  1.34it/s]\u001b[A\n",
            "Iteration:  26% 671/2561 [08:22<23:26,  1.34it/s]\u001b[A\n",
            "Iteration:  26% 672/2561 [08:23<23:28,  1.34it/s]\u001b[A\n",
            "Iteration:  26% 673/2561 [08:24<23:21,  1.35it/s]\u001b[A\n",
            "Iteration:  26% 674/2561 [08:24<23:26,  1.34it/s]\u001b[A\n",
            "Iteration:  26% 675/2561 [08:25<23:23,  1.34it/s]\u001b[A\n",
            "Iteration:  26% 676/2561 [08:26<23:26,  1.34it/s]\u001b[A\n",
            "Iteration:  26% 677/2561 [08:27<23:23,  1.34it/s]\u001b[A\n",
            "Iteration:  26% 678/2561 [08:27<23:16,  1.35it/s]\u001b[A\n",
            "Iteration:  27% 679/2561 [08:28<23:20,  1.34it/s]\u001b[A\n",
            "Iteration:  27% 680/2561 [08:29<23:20,  1.34it/s]\u001b[A\n",
            "Iteration:  27% 681/2561 [08:30<23:24,  1.34it/s]\u001b[A\n",
            "Iteration:  27% 682/2561 [08:30<23:20,  1.34it/s]\u001b[A\n",
            "Iteration:  27% 683/2561 [08:31<23:23,  1.34it/s]\u001b[A\n",
            "Iteration:  27% 684/2561 [08:32<23:22,  1.34it/s]\u001b[A\n",
            "Iteration:  27% 685/2561 [08:33<23:26,  1.33it/s]\u001b[A\n",
            "Iteration:  27% 686/2561 [08:33<23:26,  1.33it/s]\u001b[A\n",
            "Iteration:  27% 687/2561 [08:34<23:16,  1.34it/s]\u001b[A\n",
            "Iteration:  27% 688/2561 [08:35<23:19,  1.34it/s]\u001b[A\n",
            "Iteration:  27% 689/2561 [08:36<23:14,  1.34it/s]\u001b[A\n",
            "Iteration:  27% 690/2561 [08:36<23:17,  1.34it/s]\u001b[A\n",
            "Iteration:  27% 691/2561 [08:37<23:14,  1.34it/s]\u001b[A\n",
            "Iteration:  27% 692/2561 [08:38<23:18,  1.34it/s]\u001b[A\n",
            "Iteration:  27% 693/2561 [08:39<23:14,  1.34it/s]\u001b[A\n",
            "Iteration:  27% 694/2561 [08:39<23:12,  1.34it/s]\u001b[A\n",
            "Iteration:  27% 695/2561 [08:40<23:11,  1.34it/s]\u001b[A\n",
            "Iteration:  27% 696/2561 [08:41<23:06,  1.35it/s]\u001b[A\n",
            "Iteration:  27% 697/2561 [08:42<23:09,  1.34it/s]\u001b[A\n",
            "Iteration:  27% 698/2561 [08:42<23:06,  1.34it/s]\u001b[A\n",
            "Iteration:  27% 699/2561 [08:43<23:12,  1.34it/s]\u001b[A\n",
            "Iteration:  27% 700/2561 [08:44<23:08,  1.34it/s]\u001b[A\n",
            "Iteration:  27% 701/2561 [08:45<23:07,  1.34it/s]\u001b[A\n",
            "Iteration:  27% 702/2561 [08:45<23:06,  1.34it/s]\u001b[A\n",
            "Iteration:  27% 703/2561 [08:46<22:59,  1.35it/s]\u001b[A\n",
            "Iteration:  27% 704/2561 [08:47<23:01,  1.34it/s]\u001b[A\n",
            "Iteration:  28% 705/2561 [08:48<22:56,  1.35it/s]\u001b[A\n",
            "Iteration:  28% 706/2561 [08:48<23:03,  1.34it/s]\u001b[A\n",
            "Iteration:  28% 707/2561 [08:49<23:02,  1.34it/s]\u001b[A\n",
            "Iteration:  28% 708/2561 [08:50<23:02,  1.34it/s]\u001b[A\n",
            "Iteration:  28% 709/2561 [08:51<23:01,  1.34it/s]\u001b[A\n",
            "Iteration:  28% 710/2561 [08:51<22:53,  1.35it/s]\u001b[A\n",
            "Iteration:  28% 711/2561 [08:52<22:58,  1.34it/s]\u001b[A\n",
            "Iteration:  28% 712/2561 [08:53<22:56,  1.34it/s]\u001b[A\n",
            "Iteration:  28% 713/2561 [08:54<23:00,  1.34it/s]\u001b[A\n",
            "Iteration:  28% 714/2561 [08:54<22:56,  1.34it/s]\u001b[A\n",
            "Iteration:  28% 715/2561 [08:55<22:50,  1.35it/s]\u001b[A\n",
            "Iteration:  28% 716/2561 [08:56<22:54,  1.34it/s]\u001b[A\n",
            "Iteration:  28% 717/2561 [08:57<22:46,  1.35it/s]\u001b[A\n",
            "Iteration:  28% 718/2561 [08:57<22:50,  1.34it/s]\u001b[A\n",
            "Iteration:  28% 719/2561 [08:58<22:49,  1.34it/s]\u001b[A\n",
            "Iteration:  28% 720/2561 [08:59<22:57,  1.34it/s]\u001b[A\n",
            "Iteration:  28% 721/2561 [09:00<22:53,  1.34it/s]\u001b[A\n",
            "Iteration:  28% 722/2561 [09:00<22:52,  1.34it/s]\u001b[A\n",
            "Iteration:  28% 723/2561 [09:01<22:53,  1.34it/s]\u001b[A\n",
            "Iteration:  28% 724/2561 [09:02<22:46,  1.34it/s]\u001b[A\n",
            "Iteration:  28% 725/2561 [09:03<22:51,  1.34it/s]\u001b[A\n",
            "Iteration:  28% 726/2561 [09:03<22:46,  1.34it/s]\u001b[A\n",
            "Iteration:  28% 727/2561 [09:04<22:52,  1.34it/s]\u001b[A\n",
            "Iteration:  28% 728/2561 [09:05<22:48,  1.34it/s]\u001b[A\n",
            "Iteration:  28% 729/2561 [09:05<22:49,  1.34it/s]\u001b[A\n",
            "Iteration:  29% 730/2561 [09:06<22:48,  1.34it/s]\u001b[A\n",
            "Iteration:  29% 731/2561 [09:07<22:44,  1.34it/s]\u001b[A\n",
            "Iteration:  29% 732/2561 [09:08<22:46,  1.34it/s]\u001b[A\n",
            "Iteration:  29% 733/2561 [09:08<22:42,  1.34it/s]\u001b[A\n",
            "Iteration:  29% 734/2561 [09:09<22:48,  1.34it/s]\u001b[A\n",
            "Iteration:  29% 735/2561 [09:10<22:42,  1.34it/s]\u001b[A\n",
            "Iteration:  29% 736/2561 [09:11<22:45,  1.34it/s]\u001b[A\n",
            "Iteration:  29% 737/2561 [09:11<22:43,  1.34it/s]\u001b[A\n",
            "Iteration:  29% 738/2561 [09:12<22:38,  1.34it/s]\u001b[A\n",
            "Iteration:  29% 739/2561 [09:13<22:39,  1.34it/s]\u001b[A\n",
            "Iteration:  29% 740/2561 [09:14<22:34,  1.34it/s]\u001b[A\n",
            "Iteration:  29% 741/2561 [09:14<22:36,  1.34it/s]\u001b[A\n",
            "Iteration:  29% 742/2561 [09:15<22:34,  1.34it/s]\u001b[A\n",
            "Iteration:  29% 743/2561 [09:16<22:36,  1.34it/s]\u001b[A\n",
            "Iteration:  29% 744/2561 [09:17<22:35,  1.34it/s]\u001b[A\n",
            "Iteration:  29% 745/2561 [09:17<22:30,  1.35it/s]\u001b[A\n",
            "Iteration:  29% 746/2561 [09:18<22:32,  1.34it/s]\u001b[A\n",
            "Iteration:  29% 747/2561 [09:19<22:27,  1.35it/s]\u001b[A\n",
            "Iteration:  29% 748/2561 [09:20<22:30,  1.34it/s]\u001b[A\n",
            "Iteration:  29% 749/2561 [09:20<22:27,  1.34it/s]\u001b[A\n",
            "Iteration:  29% 750/2561 [09:21<22:33,  1.34it/s]\u001b[A\n",
            "Iteration:  29% 751/2561 [09:22<22:31,  1.34it/s]\u001b[A\n",
            "Iteration:  29% 752/2561 [09:23<22:37,  1.33it/s]\u001b[A\n",
            "Iteration:  29% 753/2561 [09:23<22:35,  1.33it/s]\u001b[A\n",
            "Iteration:  29% 754/2561 [09:24<22:25,  1.34it/s]\u001b[A\n",
            "Iteration:  29% 755/2561 [09:25<22:26,  1.34it/s]\u001b[A\n",
            "Iteration:  30% 756/2561 [09:26<22:23,  1.34it/s]\u001b[A\n",
            "Iteration:  30% 757/2561 [09:26<22:24,  1.34it/s]\u001b[A\n",
            "Iteration:  30% 758/2561 [09:27<22:21,  1.34it/s]\u001b[A\n",
            "Iteration:  30% 759/2561 [09:28<22:26,  1.34it/s]\u001b[A\n",
            "Iteration:  30% 760/2561 [09:29<22:23,  1.34it/s]\u001b[A\n",
            "Iteration:  30% 761/2561 [09:29<22:20,  1.34it/s]\u001b[A\n",
            "Iteration:  30% 762/2561 [09:30<22:21,  1.34it/s]\u001b[A\n",
            "Iteration:  30% 763/2561 [09:31<22:16,  1.35it/s]\u001b[A\n",
            "Iteration:  30% 764/2561 [09:32<22:20,  1.34it/s]\u001b[A\n",
            "Iteration:  30% 765/2561 [09:32<22:17,  1.34it/s]\u001b[A\n",
            "Iteration:  30% 766/2561 [09:33<22:23,  1.34it/s]\u001b[A\n",
            "Iteration:  30% 767/2561 [09:34<22:21,  1.34it/s]\u001b[A\n",
            "Iteration:  30% 768/2561 [09:35<22:17,  1.34it/s]\u001b[A\n",
            "Iteration:  30% 769/2561 [09:35<22:19,  1.34it/s]\u001b[A\n",
            "Iteration:  30% 770/2561 [09:36<22:12,  1.34it/s]\u001b[A\n",
            "Iteration:  30% 771/2561 [09:37<22:18,  1.34it/s]\u001b[A\n",
            "Iteration:  30% 772/2561 [09:38<22:13,  1.34it/s]\u001b[A\n",
            "Iteration:  30% 773/2561 [09:38<22:18,  1.34it/s]\u001b[A\n",
            "Iteration:  30% 774/2561 [09:39<22:14,  1.34it/s]\u001b[A\n",
            "Iteration:  30% 775/2561 [09:40<22:17,  1.34it/s]\u001b[A\n",
            "Iteration:  30% 776/2561 [09:41<22:14,  1.34it/s]\u001b[A\n",
            "Iteration:  30% 777/2561 [09:41<22:06,  1.34it/s]\u001b[A\n",
            "Iteration:  30% 778/2561 [09:42<22:11,  1.34it/s]\u001b[A\n",
            "Iteration:  30% 779/2561 [09:43<22:04,  1.35it/s]\u001b[A\n",
            "Iteration:  30% 780/2561 [09:44<22:08,  1.34it/s]\u001b[A\n",
            "Iteration:  30% 781/2561 [09:44<22:06,  1.34it/s]\u001b[A\n",
            "Iteration:  31% 782/2561 [09:45<22:11,  1.34it/s]\u001b[A\n",
            "Iteration:  31% 783/2561 [09:46<22:08,  1.34it/s]\u001b[A\n",
            "Iteration:  31% 784/2561 [09:47<22:12,  1.33it/s]\u001b[A\n",
            "Iteration:  31% 785/2561 [09:47<22:10,  1.33it/s]\u001b[A\n",
            "Iteration:  31% 786/2561 [09:48<22:00,  1.34it/s]\u001b[A\n",
            "Iteration:  31% 787/2561 [09:49<22:03,  1.34it/s]\u001b[A\n",
            "Iteration:  31% 788/2561 [09:50<21:58,  1.34it/s]\u001b[A\n",
            "Iteration:  31% 789/2561 [09:50<22:03,  1.34it/s]\u001b[A\n",
            "Iteration:  31% 790/2561 [09:51<22:01,  1.34it/s]\u001b[A\n",
            "Iteration:  31% 791/2561 [09:52<22:04,  1.34it/s]\u001b[A\n",
            "Iteration:  31% 792/2561 [09:53<22:04,  1.34it/s]\u001b[A\n",
            "Iteration:  31% 793/2561 [09:53<21:57,  1.34it/s]\u001b[A\n",
            "Iteration:  31% 794/2561 [09:54<21:58,  1.34it/s]\u001b[A\n",
            "Iteration:  31% 795/2561 [09:55<21:53,  1.34it/s]\u001b[A\n",
            "Iteration:  31% 796/2561 [09:55<21:57,  1.34it/s]\u001b[A\n",
            "Iteration:  31% 797/2561 [09:56<21:53,  1.34it/s]\u001b[A\n",
            "Iteration:  31% 798/2561 [09:57<21:59,  1.34it/s]\u001b[A\n",
            "Iteration:  31% 799/2561 [09:58<21:57,  1.34it/s]\u001b[A\n",
            "Iteration:  31% 800/2561 [09:58<21:51,  1.34it/s]\u001b[A\n",
            "Iteration:  31% 801/2561 [09:59<21:51,  1.34it/s]\u001b[A\n",
            "Iteration:  31% 802/2561 [10:00<21:45,  1.35it/s]\u001b[A\n",
            "Iteration:  31% 803/2561 [10:01<21:50,  1.34it/s]\u001b[A\n",
            "Iteration:  31% 804/2561 [10:01<21:46,  1.34it/s]\u001b[A\n",
            "Iteration:  31% 805/2561 [10:02<21:52,  1.34it/s]\u001b[A\n",
            "Iteration:  31% 806/2561 [10:03<21:52,  1.34it/s]\u001b[A\n",
            "Iteration:  32% 807/2561 [10:04<21:46,  1.34it/s]\u001b[A\n",
            "Iteration:  32% 808/2561 [10:04<21:46,  1.34it/s]\u001b[A\n",
            "Iteration:  32% 809/2561 [10:05<21:41,  1.35it/s]\u001b[A\n",
            "Iteration:  32% 810/2561 [10:06<21:45,  1.34it/s]\u001b[A\n",
            "Iteration:  32% 811/2561 [10:07<21:41,  1.34it/s]\u001b[A\n",
            "Iteration:  32% 812/2561 [10:07<21:46,  1.34it/s]\u001b[A\n",
            "Iteration:  32% 813/2561 [10:08<21:44,  1.34it/s]\u001b[A\n",
            "Iteration:  32% 814/2561 [10:09<21:48,  1.33it/s]\u001b[A\n",
            "Iteration:  32% 815/2561 [10:10<21:47,  1.34it/s]\u001b[A\n",
            "Iteration:  32% 816/2561 [10:10<21:38,  1.34it/s]\u001b[A\n",
            "Iteration:  32% 817/2561 [10:11<21:39,  1.34it/s]\u001b[A\n",
            "Iteration:  32% 818/2561 [10:12<21:35,  1.35it/s]\u001b[A\n",
            "Iteration:  32% 819/2561 [10:13<21:37,  1.34it/s]\u001b[A\n",
            "Iteration:  32% 820/2561 [10:13<21:37,  1.34it/s]\u001b[A\n",
            "Iteration:  32% 821/2561 [10:14<21:41,  1.34it/s]\u001b[A\n",
            "Iteration:  32% 822/2561 [10:15<21:42,  1.34it/s]\u001b[A\n",
            "Iteration:  32% 823/2561 [10:16<21:44,  1.33it/s]\u001b[A\n",
            "Iteration:  32% 824/2561 [10:16<21:43,  1.33it/s]\u001b[A\n",
            "Iteration:  32% 825/2561 [10:17<21:31,  1.34it/s]\u001b[A\n",
            "Iteration:  32% 826/2561 [10:18<21:34,  1.34it/s]\u001b[A\n",
            "Iteration:  32% 827/2561 [10:19<21:30,  1.34it/s]\u001b[A\n",
            "Iteration:  32% 828/2561 [10:19<21:35,  1.34it/s]\u001b[A\n",
            "Iteration:  32% 829/2561 [10:20<21:29,  1.34it/s]\u001b[A\n",
            "Iteration:  32% 830/2561 [10:21<21:34,  1.34it/s]\u001b[A\n",
            "Iteration:  32% 831/2561 [10:22<21:32,  1.34it/s]\u001b[A\n",
            "Iteration:  32% 832/2561 [10:22<21:38,  1.33it/s]\u001b[A\n",
            "Iteration:  33% 833/2561 [10:23<21:36,  1.33it/s]\u001b[A\n",
            "Iteration:  33% 834/2561 [10:24<21:27,  1.34it/s]\u001b[A\n",
            "Iteration:  33% 835/2561 [10:25<21:26,  1.34it/s]\u001b[A\n",
            "Iteration:  33% 836/2561 [10:25<21:22,  1.34it/s]\u001b[A\n",
            "Iteration:  33% 837/2561 [10:26<21:28,  1.34it/s]\u001b[A\n",
            "Iteration:  33% 838/2561 [10:27<21:24,  1.34it/s]\u001b[A\n",
            "Iteration:  33% 839/2561 [10:28<21:28,  1.34it/s]\u001b[A\n",
            "Iteration:  33% 840/2561 [10:28<21:26,  1.34it/s]\u001b[A\n",
            "Iteration:  33% 841/2561 [10:29<21:29,  1.33it/s]\u001b[A\n",
            "Iteration:  33% 842/2561 [10:30<21:29,  1.33it/s]\u001b[A\n",
            "Iteration:  33% 843/2561 [10:31<21:21,  1.34it/s]\u001b[A\n",
            "Iteration:  33% 844/2561 [10:31<21:23,  1.34it/s]\u001b[A\n",
            "Iteration:  33% 845/2561 [10:32<21:17,  1.34it/s]\u001b[A\n",
            "Iteration:  33% 846/2561 [10:33<21:22,  1.34it/s]\u001b[A\n",
            "Iteration:  33% 847/2561 [10:34<21:17,  1.34it/s]\u001b[A\n",
            "Iteration:  33% 848/2561 [10:34<21:18,  1.34it/s]\u001b[A\n",
            "Iteration:  33% 849/2561 [10:35<21:19,  1.34it/s]\u001b[A\n",
            "Iteration:  33% 850/2561 [10:36<21:15,  1.34it/s]\u001b[A\n",
            "Iteration:  33% 851/2561 [10:37<21:15,  1.34it/s]\u001b[A\n",
            "Iteration:  33% 852/2561 [10:37<21:12,  1.34it/s]\u001b[A\n",
            "Iteration:  33% 853/2561 [10:38<21:16,  1.34it/s]\u001b[A\n",
            "Iteration:  33% 854/2561 [10:39<21:11,  1.34it/s]\u001b[A\n",
            "Iteration:  33% 855/2561 [10:40<21:14,  1.34it/s]\u001b[A\n",
            "Iteration:  33% 856/2561 [10:40<21:12,  1.34it/s]\u001b[A\n",
            "Iteration:  33% 857/2561 [10:41<21:16,  1.34it/s]\u001b[A\n",
            "Iteration:  34% 858/2561 [10:42<21:15,  1.33it/s]\u001b[A\n",
            "Iteration:  34% 859/2561 [10:43<21:10,  1.34it/s]\u001b[A\n",
            "Iteration:  34% 860/2561 [10:43<21:09,  1.34it/s]\u001b[A\n",
            "Iteration:  34% 861/2561 [10:44<21:02,  1.35it/s]\u001b[A\n",
            "Iteration:  34% 862/2561 [10:45<21:07,  1.34it/s]\u001b[A\n",
            "Iteration:  34% 863/2561 [10:45<21:04,  1.34it/s]\u001b[A\n",
            "Iteration:  34% 864/2561 [10:46<21:08,  1.34it/s]\u001b[A\n",
            "Iteration:  34% 865/2561 [10:47<21:06,  1.34it/s]\u001b[A\n",
            "Iteration:  34% 866/2561 [10:48<21:10,  1.33it/s]\u001b[A\n",
            "Iteration:  34% 867/2561 [10:48<21:09,  1.33it/s]\u001b[A\n",
            "Iteration:  34% 868/2561 [10:49<21:01,  1.34it/s]\u001b[A\n",
            "Iteration:  34% 869/2561 [10:50<21:03,  1.34it/s]\u001b[A\n",
            "Iteration:  34% 870/2561 [10:51<20:58,  1.34it/s]\u001b[A\n",
            "Iteration:  34% 871/2561 [10:51<21:00,  1.34it/s]\u001b[A\n",
            "Iteration:  34% 872/2561 [10:52<21:00,  1.34it/s]\u001b[A\n",
            "Iteration:  34% 873/2561 [10:53<21:03,  1.34it/s]\u001b[A\n",
            "Iteration:  34% 874/2561 [10:54<21:01,  1.34it/s]\u001b[A\n",
            "Iteration:  34% 875/2561 [10:54<20:53,  1.34it/s]\u001b[A\n",
            "Iteration:  34% 876/2561 [10:55<20:56,  1.34it/s]\u001b[A\n",
            "Iteration:  34% 877/2561 [10:56<20:52,  1.34it/s]\u001b[A\n",
            "Iteration:  34% 878/2561 [10:57<20:54,  1.34it/s]\u001b[A\n",
            "Iteration:  34% 879/2561 [10:57<20:50,  1.34it/s]\u001b[A\n",
            "Iteration:  34% 880/2561 [10:58<20:57,  1.34it/s]\u001b[A\n",
            "Iteration:  34% 881/2561 [10:59<20:53,  1.34it/s]\u001b[A\n",
            "Iteration:  34% 882/2561 [11:00<20:58,  1.33it/s]\u001b[A\n",
            "Iteration:  34% 883/2561 [11:00<20:55,  1.34it/s]\u001b[A\n",
            "Iteration:  35% 884/2561 [11:01<20:53,  1.34it/s]\u001b[A\n",
            "Iteration:  35% 885/2561 [11:02<20:53,  1.34it/s]\u001b[A\n",
            "Iteration:  35% 886/2561 [11:03<20:46,  1.34it/s]\u001b[A\n",
            "Iteration:  35% 887/2561 [11:03<20:47,  1.34it/s]\u001b[A\n",
            "Iteration:  35% 888/2561 [11:04<20:40,  1.35it/s]\u001b[A\n",
            "Iteration:  35% 889/2561 [11:05<20:48,  1.34it/s]\u001b[A\n",
            "Iteration:  35% 890/2561 [11:06<20:48,  1.34it/s]\u001b[A\n",
            "Iteration:  35% 891/2561 [11:06<20:51,  1.33it/s]\u001b[A\n",
            "Iteration:  35% 892/2561 [11:07<20:48,  1.34it/s]\u001b[A\n",
            "Iteration:  35% 893/2561 [11:08<20:41,  1.34it/s]\u001b[A\n",
            "Iteration:  35% 894/2561 [11:09<20:44,  1.34it/s]\u001b[A\n",
            "Iteration:  35% 895/2561 [11:09<20:39,  1.34it/s]\u001b[A\n",
            "Iteration:  35% 896/2561 [11:10<20:42,  1.34it/s]\u001b[A\n",
            "Iteration:  35% 897/2561 [11:11<20:38,  1.34it/s]\u001b[A\n",
            "Iteration:  35% 898/2561 [11:12<20:37,  1.34it/s]\u001b[A\n",
            "Iteration:  35% 899/2561 [11:12<20:40,  1.34it/s]\u001b[A\n",
            "Iteration:  35% 900/2561 [11:13<20:34,  1.35it/s]\u001b[A\n",
            "Iteration:  35% 901/2561 [11:14<20:35,  1.34it/s]\u001b[A\n",
            "Iteration:  35% 902/2561 [11:15<20:35,  1.34it/s]\u001b[A\n",
            "Iteration:  35% 903/2561 [11:15<20:40,  1.34it/s]\u001b[A\n",
            "Iteration:  35% 904/2561 [11:16<20:38,  1.34it/s]\u001b[A\n",
            "Iteration:  35% 905/2561 [11:17<20:41,  1.33it/s]\u001b[A\n",
            "Iteration:  35% 906/2561 [11:18<20:40,  1.33it/s]\u001b[A\n",
            "Iteration:  35% 907/2561 [11:18<20:43,  1.33it/s]\u001b[A\n",
            "Iteration:  35% 908/2561 [11:19<20:41,  1.33it/s]\u001b[A\n",
            "Iteration:  35% 909/2561 [11:20<20:32,  1.34it/s]\u001b[A\n",
            "Iteration:  36% 910/2561 [11:21<20:34,  1.34it/s]\u001b[A\n",
            "Iteration:  36% 911/2561 [11:21<20:29,  1.34it/s]\u001b[A\n",
            "Iteration:  36% 912/2561 [11:22<20:30,  1.34it/s]\u001b[A\n",
            "Iteration:  36% 913/2561 [11:23<20:31,  1.34it/s]\u001b[A\n",
            "Iteration:  36% 914/2561 [11:24<20:34,  1.33it/s]\u001b[A\n",
            "Iteration:  36% 915/2561 [11:24<20:31,  1.34it/s]\u001b[A\n",
            "Iteration:  36% 916/2561 [11:25<20:23,  1.34it/s]\u001b[A\n",
            "Iteration:  36% 917/2561 [11:26<20:24,  1.34it/s]\u001b[A\n",
            "Iteration:  36% 918/2561 [11:27<20:22,  1.34it/s]\u001b[A\n",
            "Iteration:  36% 919/2561 [11:27<20:24,  1.34it/s]\u001b[A\n",
            "Iteration:  36% 920/2561 [11:28<20:22,  1.34it/s]\u001b[A\n",
            "Iteration:  36% 921/2561 [11:29<20:27,  1.34it/s]\u001b[A\n",
            "Iteration:  36% 922/2561 [11:30<20:26,  1.34it/s]\u001b[A\n",
            "Iteration:  36% 923/2561 [11:30<20:26,  1.34it/s]\u001b[A\n",
            "Iteration:  36% 924/2561 [11:31<20:26,  1.33it/s]\u001b[A\n",
            "Iteration:  36% 925/2561 [11:32<20:18,  1.34it/s]\u001b[A\n",
            "Iteration:  36% 926/2561 [11:33<20:21,  1.34it/s]\u001b[A\n",
            "Iteration:  36% 927/2561 [11:33<20:16,  1.34it/s]\u001b[A\n",
            "Iteration:  36% 928/2561 [11:34<20:19,  1.34it/s]\u001b[A\n",
            "Iteration:  36% 929/2561 [11:35<20:16,  1.34it/s]\u001b[A\n",
            "Iteration:  36% 930/2561 [11:36<20:15,  1.34it/s]\u001b[A\n",
            "Iteration:  36% 931/2561 [11:36<20:14,  1.34it/s]\u001b[A\n",
            "Iteration:  36% 932/2561 [11:37<20:10,  1.35it/s]\u001b[A\n",
            "Iteration:  36% 933/2561 [11:38<20:13,  1.34it/s]\u001b[A\n",
            "Iteration:  36% 934/2561 [11:38<20:08,  1.35it/s]\u001b[A\n",
            "Iteration:  37% 935/2561 [11:39<20:10,  1.34it/s]\u001b[A\n",
            "Iteration:  37% 936/2561 [11:40<20:07,  1.35it/s]\u001b[A\n",
            "Iteration:  37% 937/2561 [11:41<20:14,  1.34it/s]\u001b[A\n",
            "Iteration:  37% 938/2561 [11:41<20:13,  1.34it/s]\u001b[A\n",
            "Iteration:  37% 939/2561 [11:42<20:08,  1.34it/s]\u001b[A\n",
            "Iteration:  37% 940/2561 [11:43<20:09,  1.34it/s]\u001b[A\n",
            "Iteration:  37% 941/2561 [11:44<20:05,  1.34it/s]\u001b[A\n",
            "Iteration:  37% 942/2561 [11:44<20:03,  1.34it/s]\u001b[A\n",
            "Iteration:  37% 943/2561 [11:45<20:01,  1.35it/s]\u001b[A\n",
            "Iteration:  37% 944/2561 [11:46<20:07,  1.34it/s]\u001b[A\n",
            "Iteration:  37% 945/2561 [11:47<20:07,  1.34it/s]\u001b[A\n",
            "Iteration:  37% 946/2561 [11:47<20:00,  1.35it/s]\u001b[A\n",
            "Iteration:  37% 947/2561 [11:48<20:00,  1.34it/s]\u001b[A\n",
            "Iteration:  37% 948/2561 [11:49<19:58,  1.35it/s]\u001b[A\n",
            "Iteration:  37% 949/2561 [11:50<20:01,  1.34it/s]\u001b[A\n",
            "Iteration:  37% 950/2561 [11:50<19:59,  1.34it/s]\u001b[A\n",
            "Iteration:  37% 951/2561 [11:51<20:05,  1.34it/s]\u001b[A\n",
            "Iteration:  37% 952/2561 [11:52<20:04,  1.34it/s]\u001b[A\n",
            "Iteration:  37% 953/2561 [11:53<20:08,  1.33it/s]\u001b[A\n",
            "Iteration:  37% 954/2561 [11:53<20:06,  1.33it/s]\u001b[A\n",
            "Iteration:  37% 955/2561 [11:54<20:05,  1.33it/s]\u001b[A\n",
            "Iteration:  37% 956/2561 [11:55<20:03,  1.33it/s]\u001b[A\n",
            "Iteration:  37% 957/2561 [11:56<19:57,  1.34it/s]\u001b[A\n",
            "Iteration:  37% 958/2561 [11:56<19:59,  1.34it/s]\u001b[A\n",
            "Iteration:  37% 959/2561 [11:57<19:53,  1.34it/s]\u001b[A\n",
            "Iteration:  37% 960/2561 [11:58<19:55,  1.34it/s]\u001b[A\n",
            "Iteration:  38% 961/2561 [11:59<19:53,  1.34it/s]\u001b[A\n",
            "Iteration:  38% 962/2561 [11:59<19:57,  1.34it/s]\u001b[A\n",
            "Iteration:  38% 963/2561 [12:00<19:55,  1.34it/s]\u001b[A\n",
            "Iteration:  38% 964/2561 [12:01<19:55,  1.34it/s]\u001b[A\n",
            "Iteration:  38% 965/2561 [12:02<19:53,  1.34it/s]\u001b[A\n",
            "Iteration:  38% 966/2561 [12:02<19:46,  1.34it/s]\u001b[A\n",
            "Iteration:  38% 967/2561 [12:03<19:49,  1.34it/s]\u001b[A\n",
            "Iteration:  38% 968/2561 [12:04<19:46,  1.34it/s]\u001b[A\n",
            "Iteration:  38% 969/2561 [12:05<19:50,  1.34it/s]\u001b[A\n",
            "Iteration:  38% 970/2561 [12:05<19:46,  1.34it/s]\u001b[A\n",
            "Iteration:  38% 971/2561 [12:06<19:44,  1.34it/s]\u001b[A\n",
            "Iteration:  38% 972/2561 [12:07<19:45,  1.34it/s]\u001b[A\n",
            "Iteration:  38% 973/2561 [12:08<19:41,  1.34it/s]\u001b[A\n",
            "Iteration:  38% 974/2561 [12:08<19:45,  1.34it/s]\u001b[A\n",
            "Iteration:  38% 975/2561 [12:09<19:41,  1.34it/s]\u001b[A\n",
            "Iteration:  38% 976/2561 [12:10<19:44,  1.34it/s]\u001b[A\n",
            "Iteration:  38% 977/2561 [12:11<19:40,  1.34it/s]\u001b[A\n",
            "Iteration:  38% 978/2561 [12:11<19:43,  1.34it/s]\u001b[A\n",
            "Iteration:  38% 979/2561 [12:12<19:42,  1.34it/s]\u001b[A\n",
            "Iteration:  38% 980/2561 [12:13<19:37,  1.34it/s]\u001b[A\n",
            "Iteration:  38% 981/2561 [12:14<19:37,  1.34it/s]\u001b[A\n",
            "Iteration:  38% 982/2561 [12:14<19:32,  1.35it/s]\u001b[A\n",
            "Iteration:  38% 983/2561 [12:15<19:38,  1.34it/s]\u001b[A\n",
            "Iteration:  38% 984/2561 [12:16<19:35,  1.34it/s]\u001b[A\n",
            "Iteration:  38% 985/2561 [12:17<19:40,  1.34it/s]\u001b[A\n",
            "Iteration:  39% 986/2561 [12:17<19:37,  1.34it/s]\u001b[A\n",
            "Iteration:  39% 987/2561 [12:18<19:40,  1.33it/s]\u001b[A\n",
            "Iteration:  39% 988/2561 [12:19<19:37,  1.34it/s]\u001b[A\n",
            "Iteration:  39% 989/2561 [12:20<19:30,  1.34it/s]\u001b[A\n",
            "Iteration:  39% 990/2561 [12:20<19:31,  1.34it/s]\u001b[A\n",
            "Iteration:  39% 991/2561 [12:21<19:27,  1.35it/s]\u001b[A\n",
            "Iteration:  39% 992/2561 [12:22<19:31,  1.34it/s]\u001b[A\n",
            "Iteration:  39% 993/2561 [12:23<19:31,  1.34it/s]\u001b[A\n",
            "Iteration:  39% 994/2561 [12:23<19:34,  1.33it/s]\u001b[A\n",
            "Iteration:  39% 995/2561 [12:24<19:30,  1.34it/s]\u001b[A\n",
            "Iteration:  39% 996/2561 [12:25<19:31,  1.34it/s]\u001b[A\n",
            "Iteration:  39% 997/2561 [12:26<19:29,  1.34it/s]\u001b[A\n",
            "Iteration:  39% 998/2561 [12:26<19:19,  1.35it/s]\u001b[A\n",
            "Iteration:  39% 999/2561 [12:27<19:18,  1.35it/s]\u001b[A02/29/2020 07:40:36 - INFO - transformers.configuration_utils -   Configuration saved in output_roberta_GB/checkpoint-1000/config.json\n",
            "02/29/2020 07:40:38 - INFO - transformers.modeling_utils -   Model weights saved in output_roberta_GB/checkpoint-1000/pytorch_model.bin\n",
            "02/29/2020 07:40:38 - INFO - __main__ -   Saving model checkpoint to output_roberta_GB/checkpoint-1000\n",
            "02/29/2020 07:40:42 - INFO - __main__ -   Saving optimizer and scheduler states to output_roberta_GB/checkpoint-1000\n",
            "\n",
            "Iteration:  39% 1000/2561 [12:33<59:47,  2.30s/it]\u001b[A\n",
            "Iteration:  39% 1001/2561 [12:34<47:44,  1.84s/it]\u001b[A\n",
            "Iteration:  39% 1002/2561 [12:34<39:10,  1.51s/it]\u001b[A\n",
            "Iteration:  39% 1003/2561 [12:35<33:12,  1.28s/it]\u001b[A\n",
            "Iteration:  39% 1004/2561 [12:36<28:55,  1.11s/it]\u001b[A\n",
            "Iteration:  39% 1005/2561 [12:37<25:57,  1.00s/it]\u001b[A\n",
            "Iteration:  39% 1006/2561 [12:37<24:00,  1.08it/s]\u001b[A\n",
            "Iteration:  39% 1007/2561 [12:38<22:32,  1.15it/s]\u001b[A\n",
            "Iteration:  39% 1008/2561 [12:39<21:35,  1.20it/s]\u001b[A\n",
            "Iteration:  39% 1009/2561 [12:40<20:53,  1.24it/s]\u001b[A\n",
            "Iteration:  39% 1010/2561 [12:40<20:28,  1.26it/s]\u001b[A\n",
            "Iteration:  39% 1011/2561 [12:41<20:06,  1.29it/s]\u001b[A\n",
            "Iteration:  40% 1012/2561 [12:42<19:45,  1.31it/s]\u001b[A\n",
            "Iteration:  40% 1013/2561 [12:43<19:31,  1.32it/s]\u001b[A\n",
            "Iteration:  40% 1014/2561 [12:43<19:24,  1.33it/s]\u001b[A\n",
            "Iteration:  40% 1015/2561 [12:44<19:24,  1.33it/s]\u001b[A\n",
            "Iteration:  40% 1016/2561 [12:45<19:19,  1.33it/s]\u001b[A\n",
            "Iteration:  40% 1017/2561 [12:46<19:21,  1.33it/s]\u001b[A\n",
            "Iteration:  40% 1018/2561 [12:46<19:18,  1.33it/s]\u001b[A\n",
            "Iteration:  40% 1019/2561 [12:47<19:12,  1.34it/s]\u001b[A\n",
            "Iteration:  40% 1020/2561 [12:48<19:07,  1.34it/s]\u001b[A\n",
            "Iteration:  40% 1021/2561 [12:49<19:04,  1.34it/s]\u001b[A\n",
            "Iteration:  40% 1022/2561 [12:49<19:08,  1.34it/s]\u001b[A\n",
            "Iteration:  40% 1023/2561 [12:50<19:06,  1.34it/s]\u001b[A\n",
            "Iteration:  40% 1024/2561 [12:51<19:01,  1.35it/s]\u001b[A\n",
            "Iteration:  40% 1025/2561 [12:52<19:03,  1.34it/s]\u001b[A\n",
            "Iteration:  40% 1026/2561 [12:52<19:01,  1.34it/s]\u001b[A\n",
            "Iteration:  40% 1027/2561 [12:53<19:04,  1.34it/s]\u001b[A\n",
            "Iteration:  40% 1028/2561 [12:54<19:02,  1.34it/s]\u001b[A\n",
            "Iteration:  40% 1029/2561 [12:55<19:06,  1.34it/s]\u001b[A\n",
            "Iteration:  40% 1030/2561 [12:55<19:04,  1.34it/s]\u001b[A\n",
            "Iteration:  40% 1031/2561 [12:56<19:01,  1.34it/s]\u001b[A\n",
            "Iteration:  40% 1032/2561 [12:57<19:01,  1.34it/s]\u001b[A\n",
            "Iteration:  40% 1033/2561 [12:58<18:57,  1.34it/s]\u001b[A\n",
            "Iteration:  40% 1034/2561 [12:58<18:58,  1.34it/s]\u001b[A\n",
            "Iteration:  40% 1035/2561 [12:59<18:55,  1.34it/s]\u001b[A\n",
            "Iteration:  40% 1036/2561 [13:00<18:59,  1.34it/s]\u001b[A\n",
            "Iteration:  40% 1037/2561 [13:00<18:57,  1.34it/s]\u001b[A\n",
            "Iteration:  41% 1038/2561 [13:01<19:01,  1.33it/s]\u001b[A\n",
            "Iteration:  41% 1039/2561 [13:02<18:58,  1.34it/s]\u001b[A\n",
            "Iteration:  41% 1040/2561 [13:03<18:51,  1.34it/s]\u001b[A\n",
            "Iteration:  41% 1041/2561 [13:03<18:55,  1.34it/s]\u001b[A\n",
            "Iteration:  41% 1042/2561 [13:04<18:51,  1.34it/s]\u001b[A\n",
            "Iteration:  41% 1043/2561 [13:05<18:55,  1.34it/s]\u001b[A\n",
            "Iteration:  41% 1044/2561 [13:06<18:51,  1.34it/s]\u001b[A\n",
            "Iteration:  41% 1045/2561 [13:06<18:56,  1.33it/s]\u001b[A\n",
            "Iteration:  41% 1046/2561 [13:07<18:53,  1.34it/s]\u001b[A\n",
            "Iteration:  41% 1047/2561 [13:08<18:56,  1.33it/s]\u001b[A\n",
            "Iteration:  41% 1048/2561 [13:09<18:56,  1.33it/s]\u001b[A\n",
            "Iteration:  41% 1049/2561 [13:09<18:49,  1.34it/s]\u001b[A\n",
            "Iteration:  41% 1050/2561 [13:10<18:50,  1.34it/s]\u001b[A\n",
            "Iteration:  41% 1051/2561 [13:11<18:45,  1.34it/s]\u001b[A\n",
            "Iteration:  41% 1052/2561 [13:12<18:50,  1.34it/s]\u001b[A\n",
            "Iteration:  41% 1053/2561 [13:12<18:48,  1.34it/s]\u001b[A\n",
            "Iteration:  41% 1054/2561 [13:13<18:52,  1.33it/s]\u001b[A\n",
            "Iteration:  41% 1055/2561 [13:14<18:48,  1.33it/s]\u001b[A\n",
            "Iteration:  41% 1056/2561 [13:15<18:53,  1.33it/s]\u001b[A\n",
            "Iteration:  41% 1057/2561 [13:15<18:51,  1.33it/s]\u001b[A\n",
            "Iteration:  41% 1058/2561 [13:16<18:52,  1.33it/s]\u001b[A\n",
            "Iteration:  41% 1059/2561 [13:17<18:49,  1.33it/s]\u001b[A\n",
            "Iteration:  41% 1060/2561 [13:18<18:51,  1.33it/s]\u001b[A\n",
            "Iteration:  41% 1061/2561 [13:18<18:46,  1.33it/s]\u001b[A\n",
            "Iteration:  41% 1062/2561 [13:19<18:40,  1.34it/s]\u001b[A\n",
            "Iteration:  42% 1063/2561 [13:20<18:42,  1.33it/s]\u001b[A\n",
            "Iteration:  42% 1064/2561 [13:21<18:37,  1.34it/s]\u001b[A\n",
            "Iteration:  42% 1065/2561 [13:21<18:40,  1.34it/s]\u001b[A\n",
            "Iteration:  42% 1066/2561 [13:22<18:36,  1.34it/s]\u001b[A\n",
            "Iteration:  42% 1067/2561 [13:23<18:43,  1.33it/s]\u001b[A\n",
            "Iteration:  42% 1068/2561 [13:24<18:38,  1.33it/s]\u001b[A\n",
            "Iteration:  42% 1069/2561 [13:24<18:44,  1.33it/s]\u001b[A\n",
            "Iteration:  42% 1070/2561 [13:25<18:42,  1.33it/s]\u001b[A\n",
            "Iteration:  42% 1071/2561 [13:26<18:43,  1.33it/s]\u001b[A\n",
            "Iteration:  42% 1072/2561 [13:27<18:39,  1.33it/s]\u001b[A\n",
            "Iteration:  42% 1073/2561 [13:27<18:31,  1.34it/s]\u001b[A\n",
            "Iteration:  42% 1074/2561 [13:28<18:34,  1.33it/s]\u001b[A\n",
            "Iteration:  42% 1075/2561 [13:29<18:31,  1.34it/s]\u001b[A\n",
            "Iteration:  42% 1076/2561 [13:30<18:32,  1.33it/s]\u001b[A\n",
            "Iteration:  42% 1077/2561 [13:30<18:29,  1.34it/s]\u001b[A\n",
            "Iteration:  42% 1078/2561 [13:31<18:30,  1.33it/s]\u001b[A\n",
            "Iteration:  42% 1079/2561 [13:32<18:27,  1.34it/s]\u001b[A\n",
            "Iteration:  42% 1080/2561 [13:33<18:30,  1.33it/s]\u001b[A\n",
            "Iteration:  42% 1081/2561 [13:33<18:28,  1.33it/s]\u001b[A\n",
            "Iteration:  42% 1082/2561 [13:34<18:21,  1.34it/s]\u001b[A\n",
            "Iteration:  42% 1083/2561 [13:35<18:24,  1.34it/s]\u001b[A\n",
            "Iteration:  42% 1084/2561 [13:36<18:21,  1.34it/s]\u001b[A\n",
            "Iteration:  42% 1085/2561 [13:36<18:23,  1.34it/s]\u001b[A\n",
            "Iteration:  42% 1086/2561 [13:37<18:19,  1.34it/s]\u001b[A\n",
            "Iteration:  42% 1087/2561 [13:38<18:21,  1.34it/s]\u001b[A\n",
            "Iteration:  42% 1088/2561 [13:39<18:20,  1.34it/s]\u001b[A\n",
            "Iteration:  43% 1089/2561 [13:39<18:19,  1.34it/s]\u001b[A\n",
            "Iteration:  43% 1090/2561 [13:40<18:18,  1.34it/s]\u001b[A\n",
            "Iteration:  43% 1091/2561 [13:41<18:11,  1.35it/s]\u001b[A\n",
            "Iteration:  43% 1092/2561 [13:42<18:13,  1.34it/s]\u001b[A\n",
            "Iteration:  43% 1093/2561 [13:42<18:10,  1.35it/s]\u001b[A\n",
            "Iteration:  43% 1094/2561 [13:43<18:14,  1.34it/s]\u001b[A\n",
            "Iteration:  43% 1095/2561 [13:44<18:13,  1.34it/s]\u001b[A\n",
            "Iteration:  43% 1096/2561 [13:45<18:10,  1.34it/s]\u001b[A\n",
            "Iteration:  43% 1097/2561 [13:45<18:11,  1.34it/s]\u001b[A\n",
            "Iteration:  43% 1098/2561 [13:46<18:06,  1.35it/s]\u001b[A\n",
            "Iteration:  43% 1099/2561 [13:47<18:08,  1.34it/s]\u001b[A\n",
            "Iteration:  43% 1100/2561 [13:48<18:05,  1.35it/s]\u001b[A\n",
            "Iteration:  43% 1101/2561 [13:48<18:09,  1.34it/s]\u001b[A\n",
            "Iteration:  43% 1102/2561 [13:49<18:09,  1.34it/s]\u001b[A\n",
            "Iteration:  43% 1103/2561 [13:50<18:13,  1.33it/s]\u001b[A\n",
            "Iteration:  43% 1104/2561 [13:51<18:11,  1.33it/s]\u001b[A\n",
            "Iteration:  43% 1105/2561 [13:51<18:04,  1.34it/s]\u001b[A\n",
            "Iteration:  43% 1106/2561 [13:52<18:09,  1.34it/s]\u001b[A\n",
            "Iteration:  43% 1107/2561 [13:53<18:03,  1.34it/s]\u001b[A\n",
            "Iteration:  43% 1108/2561 [13:54<18:05,  1.34it/s]\u001b[A\n",
            "Iteration:  43% 1109/2561 [13:54<18:02,  1.34it/s]\u001b[A\n",
            "Iteration:  43% 1110/2561 [13:55<18:06,  1.34it/s]\u001b[A\n",
            "Iteration:  43% 1111/2561 [13:56<18:05,  1.34it/s]\u001b[A\n",
            "Iteration:  43% 1112/2561 [13:57<18:06,  1.33it/s]\u001b[A\n",
            "Iteration:  43% 1113/2561 [13:57<18:05,  1.33it/s]\u001b[A\n",
            "Iteration:  43% 1114/2561 [13:58<17:57,  1.34it/s]\u001b[A\n",
            "Iteration:  44% 1115/2561 [13:59<17:58,  1.34it/s]\u001b[A\n",
            "Iteration:  44% 1116/2561 [14:00<17:55,  1.34it/s]\u001b[A\n",
            "Iteration:  44% 1117/2561 [14:00<17:57,  1.34it/s]\u001b[A\n",
            "Iteration:  44% 1118/2561 [14:01<17:57,  1.34it/s]\u001b[A\n",
            "Iteration:  44% 1119/2561 [14:02<17:59,  1.34it/s]\u001b[A\n",
            "Iteration:  44% 1120/2561 [14:03<17:57,  1.34it/s]\u001b[A\n",
            "Iteration:  44% 1121/2561 [14:03<17:56,  1.34it/s]\u001b[A\n",
            "Iteration:  44% 1122/2561 [14:04<17:55,  1.34it/s]\u001b[A\n",
            "Iteration:  44% 1123/2561 [14:05<17:51,  1.34it/s]\u001b[A\n",
            "Iteration:  44% 1124/2561 [14:06<17:52,  1.34it/s]\u001b[A\n",
            "Iteration:  44% 1125/2561 [14:06<17:49,  1.34it/s]\u001b[A\n",
            "Iteration:  44% 1126/2561 [14:07<17:51,  1.34it/s]\u001b[A\n",
            "Iteration:  44% 1127/2561 [14:08<17:49,  1.34it/s]\u001b[A\n",
            "Iteration:  44% 1128/2561 [14:09<17:52,  1.34it/s]\u001b[A\n",
            "Iteration:  44% 1129/2561 [14:09<17:50,  1.34it/s]\u001b[A\n",
            "Iteration:  44% 1130/2561 [14:10<17:49,  1.34it/s]\u001b[A\n",
            "Iteration:  44% 1131/2561 [14:11<17:47,  1.34it/s]\u001b[A\n",
            "Iteration:  44% 1132/2561 [14:12<17:42,  1.35it/s]\u001b[A\n",
            "Iteration:  44% 1133/2561 [14:12<17:45,  1.34it/s]\u001b[A\n",
            "Iteration:  44% 1134/2561 [14:13<17:42,  1.34it/s]\u001b[A\n",
            "Iteration:  44% 1135/2561 [14:14<17:46,  1.34it/s]\u001b[A\n",
            "Iteration:  44% 1136/2561 [14:15<17:45,  1.34it/s]\u001b[A\n",
            "Iteration:  44% 1137/2561 [14:15<17:48,  1.33it/s]\u001b[A\n",
            "Iteration:  44% 1138/2561 [14:16<17:45,  1.34it/s]\u001b[A\n",
            "Iteration:  44% 1139/2561 [14:17<17:41,  1.34it/s]\u001b[A\n",
            "Iteration:  45% 1140/2561 [14:18<17:42,  1.34it/s]\u001b[A\n",
            "Iteration:  45% 1141/2561 [14:18<17:37,  1.34it/s]\u001b[A\n",
            "Iteration:  45% 1142/2561 [14:19<17:40,  1.34it/s]\u001b[A\n",
            "Iteration:  45% 1143/2561 [14:20<17:39,  1.34it/s]\u001b[A\n",
            "Iteration:  45% 1144/2561 [14:20<17:41,  1.33it/s]\u001b[A\n",
            "Iteration:  45% 1145/2561 [14:21<17:39,  1.34it/s]\u001b[A\n",
            "Iteration:  45% 1146/2561 [14:22<17:41,  1.33it/s]\u001b[A\n",
            "Iteration:  45% 1147/2561 [14:23<17:41,  1.33it/s]\u001b[A\n",
            "Iteration:  45% 1148/2561 [14:23<17:37,  1.34it/s]\u001b[A\n",
            "Iteration:  45% 1149/2561 [14:24<17:36,  1.34it/s]\u001b[A\n",
            "Iteration:  45% 1150/2561 [14:25<17:32,  1.34it/s]\u001b[A\n",
            "Iteration:  45% 1151/2561 [14:26<17:34,  1.34it/s]\u001b[A\n",
            "Iteration:  45% 1152/2561 [14:26<17:31,  1.34it/s]\u001b[A\n",
            "Iteration:  45% 1153/2561 [14:27<17:34,  1.34it/s]\u001b[A\n",
            "Iteration:  45% 1154/2561 [14:28<17:32,  1.34it/s]\u001b[A\n",
            "Iteration:  45% 1155/2561 [14:29<17:35,  1.33it/s]\u001b[A\n",
            "Iteration:  45% 1156/2561 [14:29<17:37,  1.33it/s]\u001b[A\n",
            "Iteration:  45% 1157/2561 [14:30<17:40,  1.32it/s]\u001b[A\n",
            "Iteration:  45% 1158/2561 [14:31<17:38,  1.33it/s]\u001b[A\n",
            "Iteration:  45% 1159/2561 [14:32<17:33,  1.33it/s]\u001b[A\n",
            "Iteration:  45% 1160/2561 [14:32<17:30,  1.33it/s]\u001b[A\n",
            "Iteration:  45% 1161/2561 [14:33<17:25,  1.34it/s]\u001b[A\n",
            "Iteration:  45% 1162/2561 [14:34<17:27,  1.34it/s]\u001b[A\n",
            "Iteration:  45% 1163/2561 [14:35<17:23,  1.34it/s]\u001b[A\n",
            "Iteration:  45% 1164/2561 [14:35<17:26,  1.34it/s]\u001b[A\n",
            "Iteration:  45% 1165/2561 [14:36<17:22,  1.34it/s]\u001b[A\n",
            "Iteration:  46% 1166/2561 [14:37<17:25,  1.33it/s]\u001b[A\n",
            "Iteration:  46% 1167/2561 [14:38<17:24,  1.33it/s]\u001b[A\n",
            "Iteration:  46% 1168/2561 [14:38<17:27,  1.33it/s]\u001b[A\n",
            "Iteration:  46% 1169/2561 [14:39<17:24,  1.33it/s]\u001b[A\n",
            "Iteration:  46% 1170/2561 [14:40<17:16,  1.34it/s]\u001b[A\n",
            "Iteration:  46% 1171/2561 [14:41<17:19,  1.34it/s]\u001b[A\n",
            "Iteration:  46% 1172/2561 [14:41<17:16,  1.34it/s]\u001b[A\n",
            "Iteration:  46% 1173/2561 [14:42<17:18,  1.34it/s]\u001b[A\n",
            "Iteration:  46% 1174/2561 [14:43<17:16,  1.34it/s]\u001b[A\n",
            "Iteration:  46% 1175/2561 [14:44<17:18,  1.33it/s]\u001b[A\n",
            "Iteration:  46% 1176/2561 [14:44<17:15,  1.34it/s]\u001b[A\n",
            "Iteration:  46% 1177/2561 [14:45<17:18,  1.33it/s]\u001b[A\n",
            "Iteration:  46% 1178/2561 [14:46<17:15,  1.34it/s]\u001b[A\n",
            "Iteration:  46% 1179/2561 [14:47<17:09,  1.34it/s]\u001b[A\n",
            "Iteration:  46% 1180/2561 [14:47<17:11,  1.34it/s]\u001b[A\n",
            "Iteration:  46% 1181/2561 [14:48<17:07,  1.34it/s]\u001b[A\n",
            "Iteration:  46% 1182/2561 [14:49<17:10,  1.34it/s]\u001b[A\n",
            "Iteration:  46% 1183/2561 [14:50<17:08,  1.34it/s]\u001b[A\n",
            "Iteration:  46% 1184/2561 [14:50<17:11,  1.33it/s]\u001b[A\n",
            "Iteration:  46% 1185/2561 [14:51<17:08,  1.34it/s]\u001b[A\n",
            "Iteration:  46% 1186/2561 [14:52<17:11,  1.33it/s]\u001b[A\n",
            "Iteration:  46% 1187/2561 [14:53<17:10,  1.33it/s]\u001b[A\n",
            "Iteration:  46% 1188/2561 [14:53<17:04,  1.34it/s]\u001b[A\n",
            "Iteration:  46% 1189/2561 [14:54<17:05,  1.34it/s]\u001b[A\n",
            "Iteration:  46% 1190/2561 [14:55<17:00,  1.34it/s]\u001b[A\n",
            "Iteration:  47% 1191/2561 [14:56<17:03,  1.34it/s]\u001b[A\n",
            "Iteration:  47% 1192/2561 [14:56<17:01,  1.34it/s]\u001b[A\n",
            "Iteration:  47% 1193/2561 [14:57<17:03,  1.34it/s]\u001b[A\n",
            "Iteration:  47% 1194/2561 [14:58<17:01,  1.34it/s]\u001b[A\n",
            "Iteration:  47% 1195/2561 [14:59<17:04,  1.33it/s]\u001b[A\n",
            "Iteration:  47% 1196/2561 [14:59<17:03,  1.33it/s]\u001b[A\n",
            "Iteration:  47% 1197/2561 [15:00<16:56,  1.34it/s]\u001b[A\n",
            "Iteration:  47% 1198/2561 [15:01<16:59,  1.34it/s]\u001b[A\n",
            "Iteration:  47% 1199/2561 [15:02<16:55,  1.34it/s]\u001b[A\n",
            "Iteration:  47% 1200/2561 [15:02<16:57,  1.34it/s]\u001b[A\n",
            "Iteration:  47% 1201/2561 [15:03<16:52,  1.34it/s]\u001b[A\n",
            "Iteration:  47% 1202/2561 [15:04<16:57,  1.34it/s]\u001b[A\n",
            "Iteration:  47% 1203/2561 [15:05<16:54,  1.34it/s]\u001b[A\n",
            "Iteration:  47% 1204/2561 [15:05<16:50,  1.34it/s]\u001b[A\n",
            "Iteration:  47% 1205/2561 [15:06<16:50,  1.34it/s]\u001b[A\n",
            "Iteration:  47% 1206/2561 [15:07<16:46,  1.35it/s]\u001b[A\n",
            "Iteration:  47% 1207/2561 [15:08<16:49,  1.34it/s]\u001b[A\n",
            "Iteration:  47% 1208/2561 [15:08<16:46,  1.34it/s]\u001b[A\n",
            "Iteration:  47% 1209/2561 [15:09<16:50,  1.34it/s]\u001b[A\n",
            "Iteration:  47% 1210/2561 [15:10<16:50,  1.34it/s]\u001b[A\n",
            "Iteration:  47% 1211/2561 [15:11<16:46,  1.34it/s]\u001b[A\n",
            "Iteration:  47% 1212/2561 [15:11<16:45,  1.34it/s]\u001b[A\n",
            "Iteration:  47% 1213/2561 [15:12<16:40,  1.35it/s]\u001b[A\n",
            "Iteration:  47% 1214/2561 [15:13<16:43,  1.34it/s]\u001b[A\n",
            "Iteration:  47% 1215/2561 [15:14<16:41,  1.34it/s]\u001b[A\n",
            "Iteration:  47% 1216/2561 [15:14<16:45,  1.34it/s]\u001b[A\n",
            "Iteration:  48% 1217/2561 [15:15<16:45,  1.34it/s]\u001b[A\n",
            "Iteration:  48% 1218/2561 [15:16<16:48,  1.33it/s]\u001b[A\n",
            "Iteration:  48% 1219/2561 [15:17<16:45,  1.33it/s]\u001b[A\n",
            "Iteration:  48% 1220/2561 [15:17<16:39,  1.34it/s]\u001b[A\n",
            "Iteration:  48% 1221/2561 [15:18<16:40,  1.34it/s]\u001b[A\n",
            "Iteration:  48% 1222/2561 [15:19<16:36,  1.34it/s]\u001b[A\n",
            "Iteration:  48% 1223/2561 [15:20<16:39,  1.34it/s]\u001b[A\n",
            "Iteration:  48% 1224/2561 [15:20<16:37,  1.34it/s]\u001b[A\n",
            "Iteration:  48% 1225/2561 [15:21<16:41,  1.33it/s]\u001b[A\n",
            "Iteration:  48% 1226/2561 [15:22<16:39,  1.34it/s]\u001b[A\n",
            "Iteration:  48% 1227/2561 [15:23<16:33,  1.34it/s]\u001b[A\n",
            "Iteration:  48% 1228/2561 [15:23<16:35,  1.34it/s]\u001b[A\n",
            "Iteration:  48% 1229/2561 [15:24<16:30,  1.34it/s]\u001b[A\n",
            "Iteration:  48% 1230/2561 [15:25<16:33,  1.34it/s]\u001b[A\n",
            "Iteration:  48% 1231/2561 [15:26<16:30,  1.34it/s]\u001b[A\n",
            "Iteration:  48% 1232/2561 [15:26<16:34,  1.34it/s]\u001b[A\n",
            "Iteration:  48% 1233/2561 [15:27<16:32,  1.34it/s]\u001b[A\n",
            "Iteration:  48% 1234/2561 [15:28<16:34,  1.33it/s]\u001b[A\n",
            "Iteration:  48% 1235/2561 [15:29<16:32,  1.34it/s]\u001b[A\n",
            "Iteration:  48% 1236/2561 [15:29<16:30,  1.34it/s]\u001b[A\n",
            "Iteration:  48% 1237/2561 [15:30<16:31,  1.34it/s]\u001b[A\n",
            "Iteration:  48% 1238/2561 [15:31<16:27,  1.34it/s]\u001b[A\n",
            "Iteration:  48% 1239/2561 [15:32<16:28,  1.34it/s]\u001b[A\n",
            "Iteration:  48% 1240/2561 [15:32<16:25,  1.34it/s]\u001b[A\n",
            "Iteration:  48% 1241/2561 [15:33<16:29,  1.33it/s]\u001b[A\n",
            "Iteration:  48% 1242/2561 [15:34<16:26,  1.34it/s]\u001b[A\n",
            "Iteration:  49% 1243/2561 [15:35<16:25,  1.34it/s]\u001b[A\n",
            "Iteration:  49% 1244/2561 [15:35<16:24,  1.34it/s]\u001b[A\n",
            "Iteration:  49% 1245/2561 [15:36<16:19,  1.34it/s]\u001b[A\n",
            "Iteration:  49% 1246/2561 [15:37<16:22,  1.34it/s]\u001b[A\n",
            "Iteration:  49% 1247/2561 [15:37<16:21,  1.34it/s]\u001b[A\n",
            "Iteration:  49% 1248/2561 [15:38<16:22,  1.34it/s]\u001b[A\n",
            "Iteration:  49% 1249/2561 [15:39<16:19,  1.34it/s]\u001b[A\n",
            "Iteration:  49% 1250/2561 [15:40<16:22,  1.33it/s]\u001b[A\n",
            "Iteration:  49% 1251/2561 [15:40<16:20,  1.34it/s]\u001b[A\n",
            "Iteration:  49% 1252/2561 [15:41<16:15,  1.34it/s]\u001b[A\n",
            "Iteration:  49% 1253/2561 [15:42<16:15,  1.34it/s]\u001b[A\n",
            "Iteration:  49% 1254/2561 [15:43<16:12,  1.34it/s]\u001b[A\n",
            "Iteration:  49% 1255/2561 [15:43<16:15,  1.34it/s]\u001b[A\n",
            "Iteration:  49% 1256/2561 [15:44<16:14,  1.34it/s]\u001b[A\n",
            "Iteration:  49% 1257/2561 [15:45<16:15,  1.34it/s]\u001b[A\n",
            "Iteration:  49% 1258/2561 [15:46<16:13,  1.34it/s]\u001b[A\n",
            "Iteration:  49% 1259/2561 [15:46<16:17,  1.33it/s]\u001b[A\n",
            "Iteration:  49% 1260/2561 [15:47<16:15,  1.33it/s]\u001b[A\n",
            "Iteration:  49% 1261/2561 [15:48<16:08,  1.34it/s]\u001b[A\n",
            "Iteration:  49% 1262/2561 [15:49<16:08,  1.34it/s]\u001b[A\n",
            "Iteration:  49% 1263/2561 [15:49<16:05,  1.34it/s]\u001b[A\n",
            "Iteration:  49% 1264/2561 [15:50<16:07,  1.34it/s]\u001b[A\n",
            "Iteration:  49% 1265/2561 [15:51<16:06,  1.34it/s]\u001b[A\n",
            "Iteration:  49% 1266/2561 [15:52<16:08,  1.34it/s]\u001b[A\n",
            "Iteration:  49% 1267/2561 [15:52<16:09,  1.33it/s]\u001b[A\n",
            "Iteration:  50% 1268/2561 [15:53<16:01,  1.35it/s]\u001b[A\n",
            "Iteration:  50% 1269/2561 [15:54<15:57,  1.35it/s]\u001b[A\n",
            "Iteration:  50% 1270/2561 [15:55<15:56,  1.35it/s]\u001b[A\n",
            "Iteration:  50% 1271/2561 [15:55<16:02,  1.34it/s]\u001b[A\n",
            "Iteration:  50% 1272/2561 [15:56<16:00,  1.34it/s]\u001b[A\n",
            "Iteration:  50% 1273/2561 [15:57<15:57,  1.34it/s]\u001b[A\n",
            "Iteration:  50% 1274/2561 [15:58<15:59,  1.34it/s]\u001b[A\n",
            "Iteration:  50% 1275/2561 [15:58<15:54,  1.35it/s]\u001b[A\n",
            "Iteration:  50% 1276/2561 [15:59<15:56,  1.34it/s]\u001b[A\n",
            "Iteration:  50% 1277/2561 [16:00<15:54,  1.35it/s]\u001b[A\n",
            "Iteration:  50% 1278/2561 [16:01<15:57,  1.34it/s]\u001b[A\n",
            "Iteration:  50% 1279/2561 [16:01<15:55,  1.34it/s]\u001b[A\n",
            "Iteration:  50% 1280/2561 [16:02<15:53,  1.34it/s]\u001b[A\n",
            "Iteration:  50% 1281/2561 [16:03<15:53,  1.34it/s]\u001b[A\n",
            "Iteration:  50% 1282/2561 [16:04<15:49,  1.35it/s]\u001b[A\n",
            "Iteration:  50% 1283/2561 [16:04<15:52,  1.34it/s]\u001b[A\n",
            "Iteration:  50% 1284/2561 [16:05<15:50,  1.34it/s]\u001b[A\n",
            "Iteration:  50% 1285/2561 [16:06<15:53,  1.34it/s]\u001b[A\n",
            "Iteration:  50% 1286/2561 [16:07<15:52,  1.34it/s]\u001b[A\n",
            "Iteration:  50% 1287/2561 [16:07<15:55,  1.33it/s]\u001b[A\n",
            "Iteration:  50% 1288/2561 [16:08<15:53,  1.34it/s]\u001b[A\n",
            "Iteration:  50% 1289/2561 [16:09<15:47,  1.34it/s]\u001b[A\n",
            "Iteration:  50% 1290/2561 [16:10<15:48,  1.34it/s]\u001b[A\n",
            "Iteration:  50% 1291/2561 [16:10<15:44,  1.34it/s]\u001b[A\n",
            "Iteration:  50% 1292/2561 [16:11<15:47,  1.34it/s]\u001b[A\n",
            "Iteration:  50% 1293/2561 [16:12<15:45,  1.34it/s]\u001b[A\n",
            "Iteration:  51% 1294/2561 [16:13<15:48,  1.34it/s]\u001b[A\n",
            "Iteration:  51% 1295/2561 [16:13<15:47,  1.34it/s]\u001b[A\n",
            "Iteration:  51% 1296/2561 [16:14<15:40,  1.34it/s]\u001b[A\n",
            "Iteration:  51% 1297/2561 [16:15<15:40,  1.34it/s]\u001b[A\n",
            "Iteration:  51% 1298/2561 [16:16<15:40,  1.34it/s]\u001b[A\n",
            "Iteration:  51% 1299/2561 [16:16<15:42,  1.34it/s]\u001b[A\n",
            "Iteration:  51% 1300/2561 [16:17<15:40,  1.34it/s]\u001b[A\n",
            "Iteration:  51% 1301/2561 [16:18<15:43,  1.34it/s]\u001b[A\n",
            "Iteration:  51% 1302/2561 [16:19<15:42,  1.34it/s]\u001b[A\n",
            "Iteration:  51% 1303/2561 [16:19<15:42,  1.34it/s]\u001b[A\n",
            "Iteration:  51% 1304/2561 [16:20<15:40,  1.34it/s]\u001b[A\n",
            "Iteration:  51% 1305/2561 [16:21<15:34,  1.34it/s]\u001b[A\n",
            "Iteration:  51% 1306/2561 [16:22<15:37,  1.34it/s]\u001b[A\n",
            "Iteration:  51% 1307/2561 [16:22<15:34,  1.34it/s]\u001b[A\n",
            "Iteration:  51% 1308/2561 [16:23<15:37,  1.34it/s]\u001b[A\n",
            "Iteration:  51% 1309/2561 [16:24<15:34,  1.34it/s]\u001b[A\n",
            "Iteration:  51% 1310/2561 [16:25<15:36,  1.34it/s]\u001b[A\n",
            "Iteration:  51% 1311/2561 [16:25<15:36,  1.33it/s]\u001b[A\n",
            "Iteration:  51% 1312/2561 [16:26<15:34,  1.34it/s]\u001b[A\n",
            "Iteration:  51% 1313/2561 [16:27<15:33,  1.34it/s]\u001b[A\n",
            "Iteration:  51% 1314/2561 [16:27<15:28,  1.34it/s]\u001b[A\n",
            "Iteration:  51% 1315/2561 [16:28<15:29,  1.34it/s]\u001b[A\n",
            "Iteration:  51% 1316/2561 [16:29<15:27,  1.34it/s]\u001b[A\n",
            "Iteration:  51% 1317/2561 [16:30<15:30,  1.34it/s]\u001b[A\n",
            "Iteration:  51% 1318/2561 [16:30<15:26,  1.34it/s]\u001b[A\n",
            "Iteration:  52% 1319/2561 [16:31<15:25,  1.34it/s]\u001b[A\n",
            "Iteration:  52% 1320/2561 [16:32<15:25,  1.34it/s]\u001b[A\n",
            "Iteration:  52% 1321/2561 [16:33<15:21,  1.35it/s]\u001b[A\n",
            "Iteration:  52% 1322/2561 [16:33<15:23,  1.34it/s]\u001b[A\n",
            "Iteration:  52% 1323/2561 [16:34<15:20,  1.34it/s]\u001b[A\n",
            "Iteration:  52% 1324/2561 [16:35<15:23,  1.34it/s]\u001b[A\n",
            "Iteration:  52% 1325/2561 [16:36<15:22,  1.34it/s]\u001b[A\n",
            "Iteration:  52% 1326/2561 [16:36<15:25,  1.34it/s]\u001b[A\n",
            "Iteration:  52% 1327/2561 [16:37<15:23,  1.34it/s]\u001b[A\n",
            "Iteration:  52% 1328/2561 [16:38<15:25,  1.33it/s]\u001b[A\n",
            "Iteration:  52% 1329/2561 [16:39<15:23,  1.33it/s]\u001b[A\n",
            "Iteration:  52% 1330/2561 [16:39<15:18,  1.34it/s]\u001b[A\n",
            "Iteration:  52% 1331/2561 [16:40<15:19,  1.34it/s]\u001b[A\n",
            "Iteration:  52% 1332/2561 [16:41<15:17,  1.34it/s]\u001b[A\n",
            "Iteration:  52% 1333/2561 [16:42<15:20,  1.33it/s]\u001b[A\n",
            "Iteration:  52% 1334/2561 [16:42<15:17,  1.34it/s]\u001b[A\n",
            "Iteration:  52% 1335/2561 [16:43<15:19,  1.33it/s]\u001b[A\n",
            "Iteration:  52% 1336/2561 [16:44<15:17,  1.33it/s]\u001b[A\n",
            "Iteration:  52% 1337/2561 [16:45<15:18,  1.33it/s]\u001b[A\n",
            "Iteration:  52% 1338/2561 [16:45<15:17,  1.33it/s]\u001b[A\n",
            "Iteration:  52% 1339/2561 [16:46<15:11,  1.34it/s]\u001b[A\n",
            "Iteration:  52% 1340/2561 [16:47<15:12,  1.34it/s]\u001b[A\n",
            "Iteration:  52% 1341/2561 [16:48<15:08,  1.34it/s]\u001b[A\n",
            "Iteration:  52% 1342/2561 [16:48<15:11,  1.34it/s]\u001b[A\n",
            "Iteration:  52% 1343/2561 [16:49<15:09,  1.34it/s]\u001b[A\n",
            "Iteration:  52% 1344/2561 [16:50<15:12,  1.33it/s]\u001b[A\n",
            "Iteration:  53% 1345/2561 [16:51<15:10,  1.34it/s]\u001b[A\n",
            "Iteration:  53% 1346/2561 [16:51<15:12,  1.33it/s]\u001b[A\n",
            "Iteration:  53% 1347/2561 [16:52<15:13,  1.33it/s]\u001b[A\n",
            "Iteration:  53% 1348/2561 [16:53<15:14,  1.33it/s]\u001b[A\n",
            "Iteration:  53% 1349/2561 [16:54<15:13,  1.33it/s]\u001b[A\n",
            "Iteration:  53% 1350/2561 [16:54<15:06,  1.34it/s]\u001b[A\n",
            "Iteration:  53% 1351/2561 [16:55<15:06,  1.34it/s]\u001b[A\n",
            "Iteration:  53% 1352/2561 [16:56<15:02,  1.34it/s]\u001b[A\n",
            "Iteration:  53% 1353/2561 [16:57<15:04,  1.34it/s]\u001b[A\n",
            "Iteration:  53% 1354/2561 [16:57<15:01,  1.34it/s]\u001b[A\n",
            "Iteration:  53% 1355/2561 [16:58<15:03,  1.34it/s]\u001b[A\n",
            "Iteration:  53% 1356/2561 [16:59<15:02,  1.34it/s]\u001b[A\n",
            "Iteration:  53% 1357/2561 [17:00<15:00,  1.34it/s]\u001b[A\n",
            "Iteration:  53% 1358/2561 [17:00<15:02,  1.33it/s]\u001b[A\n",
            "Iteration:  53% 1359/2561 [17:01<14:56,  1.34it/s]\u001b[A\n",
            "Iteration:  53% 1360/2561 [17:02<14:57,  1.34it/s]\u001b[A\n",
            "Iteration:  53% 1361/2561 [17:03<14:54,  1.34it/s]\u001b[A\n",
            "Iteration:  53% 1362/2561 [17:03<14:57,  1.34it/s]\u001b[A\n",
            "Iteration:  53% 1363/2561 [17:04<14:52,  1.34it/s]\u001b[A\n",
            "Iteration:  53% 1364/2561 [17:05<14:55,  1.34it/s]\u001b[A\n",
            "Iteration:  53% 1365/2561 [17:06<14:54,  1.34it/s]\u001b[A\n",
            "Iteration:  53% 1366/2561 [17:06<14:55,  1.33it/s]\u001b[A\n",
            "Iteration:  53% 1367/2561 [17:07<14:54,  1.33it/s]\u001b[A\n",
            "Iteration:  53% 1368/2561 [17:08<14:51,  1.34it/s]\u001b[A\n",
            "Iteration:  53% 1369/2561 [17:09<14:50,  1.34it/s]\u001b[A\n",
            "Iteration:  53% 1370/2561 [17:09<14:47,  1.34it/s]\u001b[A\n",
            "Iteration:  54% 1371/2561 [17:10<14:49,  1.34it/s]\u001b[A\n",
            "Iteration:  54% 1372/2561 [17:11<14:45,  1.34it/s]\u001b[A\n",
            "Iteration:  54% 1373/2561 [17:12<14:48,  1.34it/s]\u001b[A\n",
            "Iteration:  54% 1374/2561 [17:12<14:48,  1.34it/s]\u001b[A\n",
            "Iteration:  54% 1375/2561 [17:13<14:48,  1.33it/s]\u001b[A\n",
            "Iteration:  54% 1376/2561 [17:14<14:47,  1.34it/s]\u001b[A\n",
            "Iteration:  54% 1377/2561 [17:15<14:45,  1.34it/s]\u001b[A\n",
            "Iteration:  54% 1378/2561 [17:15<14:46,  1.33it/s]\u001b[A\n",
            "Iteration:  54% 1379/2561 [17:16<14:41,  1.34it/s]\u001b[A\n",
            "Iteration:  54% 1380/2561 [17:17<14:43,  1.34it/s]\u001b[A\n",
            "Iteration:  54% 1381/2561 [17:18<14:39,  1.34it/s]\u001b[A\n",
            "Iteration:  54% 1382/2561 [17:18<14:41,  1.34it/s]\u001b[A\n",
            "Iteration:  54% 1383/2561 [17:19<14:40,  1.34it/s]\u001b[A\n",
            "Iteration:  54% 1384/2561 [17:20<14:37,  1.34it/s]\u001b[A\n",
            "Iteration:  54% 1385/2561 [17:21<14:38,  1.34it/s]\u001b[A\n",
            "Iteration:  54% 1386/2561 [17:21<14:34,  1.34it/s]\u001b[A\n",
            "Iteration:  54% 1387/2561 [17:22<14:35,  1.34it/s]\u001b[A\n",
            "Iteration:  54% 1388/2561 [17:23<14:34,  1.34it/s]\u001b[A\n",
            "Iteration:  54% 1389/2561 [17:24<14:36,  1.34it/s]\u001b[A\n",
            "Iteration:  54% 1390/2561 [17:24<14:35,  1.34it/s]\u001b[A\n",
            "Iteration:  54% 1391/2561 [17:25<14:37,  1.33it/s]\u001b[A\n",
            "Iteration:  54% 1392/2561 [17:26<14:35,  1.34it/s]\u001b[A\n",
            "Iteration:  54% 1393/2561 [17:27<14:38,  1.33it/s]\u001b[A\n",
            "Iteration:  54% 1394/2561 [17:27<14:35,  1.33it/s]\u001b[A\n",
            "Iteration:  54% 1395/2561 [17:28<14:31,  1.34it/s]\u001b[A\n",
            "Iteration:  55% 1396/2561 [17:29<14:32,  1.34it/s]\u001b[A\n",
            "Iteration:  55% 1397/2561 [17:30<14:29,  1.34it/s]\u001b[A\n",
            "Iteration:  55% 1398/2561 [17:30<14:30,  1.34it/s]\u001b[A\n",
            "Iteration:  55% 1399/2561 [17:31<14:27,  1.34it/s]\u001b[A\n",
            "Iteration:  55% 1400/2561 [17:32<14:30,  1.33it/s]\u001b[A\n",
            "Iteration:  55% 1401/2561 [17:33<14:27,  1.34it/s]\u001b[A\n",
            "Iteration:  55% 1402/2561 [17:33<14:29,  1.33it/s]\u001b[A\n",
            "Iteration:  55% 1403/2561 [17:34<14:28,  1.33it/s]\u001b[A\n",
            "Iteration:  55% 1404/2561 [17:35<14:30,  1.33it/s]\u001b[A\n",
            "Iteration:  55% 1405/2561 [17:36<14:28,  1.33it/s]\u001b[A\n",
            "Iteration:  55% 1406/2561 [17:36<14:22,  1.34it/s]\u001b[A\n",
            "Iteration:  55% 1407/2561 [17:37<14:22,  1.34it/s]\u001b[A\n",
            "Iteration:  55% 1408/2561 [17:38<14:19,  1.34it/s]\u001b[A\n",
            "Iteration:  55% 1409/2561 [17:39<14:21,  1.34it/s]\u001b[A\n",
            "Iteration:  55% 1410/2561 [17:39<14:19,  1.34it/s]\u001b[A\n",
            "Iteration:  55% 1411/2561 [17:40<14:22,  1.33it/s]\u001b[A\n",
            "Iteration:  55% 1412/2561 [17:41<14:19,  1.34it/s]\u001b[A\n",
            "Iteration:  55% 1413/2561 [17:42<14:22,  1.33it/s]\u001b[A\n",
            "Iteration:  55% 1414/2561 [17:42<14:20,  1.33it/s]\u001b[A\n",
            "Iteration:  55% 1415/2561 [17:43<14:18,  1.34it/s]\u001b[A\n",
            "Iteration:  55% 1416/2561 [17:44<14:18,  1.33it/s]\u001b[A\n",
            "Iteration:  55% 1417/2561 [17:45<14:13,  1.34it/s]\u001b[A\n",
            "Iteration:  55% 1418/2561 [17:45<14:14,  1.34it/s]\u001b[A\n",
            "Iteration:  55% 1419/2561 [17:46<14:11,  1.34it/s]\u001b[A\n",
            "Iteration:  55% 1420/2561 [17:47<14:14,  1.34it/s]\u001b[A\n",
            "Iteration:  55% 1421/2561 [17:48<14:11,  1.34it/s]\u001b[A\n",
            "Iteration:  56% 1422/2561 [17:48<14:13,  1.33it/s]\u001b[A\n",
            "Iteration:  56% 1423/2561 [17:49<14:12,  1.34it/s]\u001b[A\n",
            "Iteration:  56% 1424/2561 [17:50<14:14,  1.33it/s]\u001b[A\n",
            "Iteration:  56% 1425/2561 [17:51<14:12,  1.33it/s]\u001b[A\n",
            "Iteration:  56% 1426/2561 [17:51<14:13,  1.33it/s]\u001b[A\n",
            "Iteration:  56% 1427/2561 [17:52<14:11,  1.33it/s]\u001b[A\n",
            "Iteration:  56% 1428/2561 [17:53<14:14,  1.33it/s]\u001b[A\n",
            "Iteration:  56% 1429/2561 [17:54<14:12,  1.33it/s]\u001b[A\n",
            "Iteration:  56% 1430/2561 [17:54<14:06,  1.34it/s]\u001b[A\n",
            "Iteration:  56% 1431/2561 [17:55<14:06,  1.33it/s]\u001b[A\n",
            "Iteration:  56% 1432/2561 [17:56<14:01,  1.34it/s]\u001b[A\n",
            "Iteration:  56% 1433/2561 [17:57<14:03,  1.34it/s]\u001b[A\n",
            "Iteration:  56% 1434/2561 [17:57<14:01,  1.34it/s]\u001b[A\n",
            "Iteration:  56% 1435/2561 [17:58<14:03,  1.33it/s]\u001b[A\n",
            "Iteration:  56% 1436/2561 [17:59<14:01,  1.34it/s]\u001b[A\n",
            "Iteration:  56% 1437/2561 [18:00<14:02,  1.33it/s]\u001b[A\n",
            "Iteration:  56% 1438/2561 [18:00<14:00,  1.34it/s]\u001b[A\n",
            "Iteration:  56% 1439/2561 [18:01<13:56,  1.34it/s]\u001b[A\n",
            "Iteration:  56% 1440/2561 [18:02<13:57,  1.34it/s]\u001b[A\n",
            "Iteration:  56% 1441/2561 [18:02<13:54,  1.34it/s]\u001b[A\n",
            "Iteration:  56% 1442/2561 [18:03<13:56,  1.34it/s]\u001b[A\n",
            "Iteration:  56% 1443/2561 [18:04<13:54,  1.34it/s]\u001b[A\n",
            "Iteration:  56% 1444/2561 [18:05<13:56,  1.34it/s]\u001b[A\n",
            "Iteration:  56% 1445/2561 [18:05<13:54,  1.34it/s]\u001b[A\n",
            "Iteration:  56% 1446/2561 [18:06<13:56,  1.33it/s]\u001b[A\n",
            "Iteration:  57% 1447/2561 [18:07<13:55,  1.33it/s]\u001b[A\n",
            "Iteration:  57% 1448/2561 [18:08<13:50,  1.34it/s]\u001b[A\n",
            "Iteration:  57% 1449/2561 [18:08<13:51,  1.34it/s]\u001b[A\n",
            "Iteration:  57% 1450/2561 [18:09<13:48,  1.34it/s]\u001b[A\n",
            "Iteration:  57% 1451/2561 [18:10<13:50,  1.34it/s]\u001b[A\n",
            "Iteration:  57% 1452/2561 [18:11<13:47,  1.34it/s]\u001b[A\n",
            "Iteration:  57% 1453/2561 [18:11<13:49,  1.34it/s]\u001b[A\n",
            "Iteration:  57% 1454/2561 [18:12<13:47,  1.34it/s]\u001b[A\n",
            "Iteration:  57% 1455/2561 [18:13<13:45,  1.34it/s]\u001b[A\n",
            "Iteration:  57% 1456/2561 [18:14<13:45,  1.34it/s]\u001b[A\n",
            "Iteration:  57% 1457/2561 [18:14<13:42,  1.34it/s]\u001b[A\n",
            "Iteration:  57% 1458/2561 [18:15<13:44,  1.34it/s]\u001b[A\n",
            "Iteration:  57% 1459/2561 [18:16<13:41,  1.34it/s]\u001b[A\n",
            "Iteration:  57% 1460/2561 [18:17<13:44,  1.34it/s]\u001b[A\n",
            "Iteration:  57% 1461/2561 [18:17<13:42,  1.34it/s]\u001b[A\n",
            "Iteration:  57% 1462/2561 [18:18<13:44,  1.33it/s]\u001b[A\n",
            "Iteration:  57% 1463/2561 [18:19<13:43,  1.33it/s]\u001b[A\n",
            "Iteration:  57% 1464/2561 [18:20<13:44,  1.33it/s]\u001b[A\n",
            "Iteration:  57% 1465/2561 [18:20<13:43,  1.33it/s]\u001b[A\n",
            "Iteration:  57% 1466/2561 [18:21<13:39,  1.34it/s]\u001b[A\n",
            "Iteration:  57% 1467/2561 [18:22<13:38,  1.34it/s]\u001b[A\n",
            "Iteration:  57% 1468/2561 [18:23<13:34,  1.34it/s]\u001b[A\n",
            "Iteration:  57% 1469/2561 [18:23<13:36,  1.34it/s]\u001b[A\n",
            "Iteration:  57% 1470/2561 [18:24<13:34,  1.34it/s]\u001b[A\n",
            "Iteration:  57% 1471/2561 [18:25<13:36,  1.34it/s]\u001b[A\n",
            "Iteration:  57% 1472/2561 [18:26<13:34,  1.34it/s]\u001b[A\n",
            "Iteration:  58% 1473/2561 [18:26<13:36,  1.33it/s]\u001b[A\n",
            "Iteration:  58% 1474/2561 [18:27<13:34,  1.33it/s]\u001b[A\n",
            "Iteration:  58% 1475/2561 [18:28<13:33,  1.33it/s]\u001b[A\n",
            "Iteration:  58% 1476/2561 [18:29<13:32,  1.34it/s]\u001b[A\n",
            "Iteration:  58% 1477/2561 [18:29<13:29,  1.34it/s]\u001b[A\n",
            "Iteration:  58% 1478/2561 [18:30<13:30,  1.34it/s]\u001b[A\n",
            "Iteration:  58% 1479/2561 [18:31<13:28,  1.34it/s]\u001b[A\n",
            "Iteration:  58% 1480/2561 [18:32<13:29,  1.33it/s]\u001b[A\n",
            "Iteration:  58% 1481/2561 [18:32<13:26,  1.34it/s]\u001b[A\n",
            "Iteration:  58% 1482/2561 [18:33<13:28,  1.33it/s]\u001b[A\n",
            "Iteration:  58% 1483/2561 [18:34<13:26,  1.34it/s]\u001b[A\n",
            "Iteration:  58% 1484/2561 [18:35<13:26,  1.34it/s]\u001b[A\n",
            "Iteration:  58% 1485/2561 [18:35<13:25,  1.34it/s]\u001b[A\n",
            "Iteration:  58% 1486/2561 [18:36<13:21,  1.34it/s]\u001b[A\n",
            "Iteration:  58% 1487/2561 [18:37<13:23,  1.34it/s]\u001b[A\n",
            "Iteration:  58% 1488/2561 [18:38<13:21,  1.34it/s]\u001b[A\n",
            "Iteration:  58% 1489/2561 [18:38<13:23,  1.33it/s]\u001b[A\n",
            "Iteration:  58% 1490/2561 [18:39<13:18,  1.34it/s]\u001b[A\n",
            "Iteration:  58% 1491/2561 [18:40<13:21,  1.33it/s]\u001b[A\n",
            "Iteration:  58% 1492/2561 [18:41<13:19,  1.34it/s]\u001b[A\n",
            "Iteration:  58% 1493/2561 [18:41<13:17,  1.34it/s]\u001b[A\n",
            "Iteration:  58% 1494/2561 [18:42<13:17,  1.34it/s]\u001b[A\n",
            "Iteration:  58% 1495/2561 [18:43<13:13,  1.34it/s]\u001b[A\n",
            "Iteration:  58% 1496/2561 [18:44<13:14,  1.34it/s]\u001b[A\n",
            "Iteration:  58% 1497/2561 [18:44<13:12,  1.34it/s]\u001b[A\n",
            "Iteration:  58% 1498/2561 [18:45<13:13,  1.34it/s]\u001b[A\n",
            "Iteration:  59% 1499/2561 [18:46<13:13,  1.34it/s]\u001b[A02/29/2020 07:46:55 - INFO - transformers.configuration_utils -   Configuration saved in output_roberta_GB/checkpoint-1500/config.json\n",
            "02/29/2020 07:46:57 - INFO - transformers.modeling_utils -   Model weights saved in output_roberta_GB/checkpoint-1500/pytorch_model.bin\n",
            "02/29/2020 07:46:57 - INFO - __main__ -   Saving model checkpoint to output_roberta_GB/checkpoint-1500\n",
            "02/29/2020 07:47:01 - INFO - __main__ -   Saving optimizer and scheduler states to output_roberta_GB/checkpoint-1500\n",
            "\n",
            "Iteration:  59% 1500/2561 [18:52<41:30,  2.35s/it]\u001b[A\n",
            "Iteration:  59% 1501/2561 [18:53<33:06,  1.87s/it]\u001b[A\n",
            "Iteration:  59% 1502/2561 [18:53<27:03,  1.53s/it]\u001b[A\n",
            "Iteration:  59% 1503/2561 [18:54<22:51,  1.30s/it]\u001b[A\n",
            "Iteration:  59% 1504/2561 [18:55<19:54,  1.13s/it]\u001b[A\n",
            "Iteration:  59% 1505/2561 [18:56<17:54,  1.02s/it]\u001b[A\n",
            "Iteration:  59% 1506/2561 [18:56<16:26,  1.07it/s]\u001b[A\n",
            "Iteration:  59% 1507/2561 [18:57<15:26,  1.14it/s]\u001b[A\n",
            "Iteration:  59% 1508/2561 [18:58<14:44,  1.19it/s]\u001b[A\n",
            "Iteration:  59% 1509/2561 [18:59<14:10,  1.24it/s]\u001b[A\n",
            "Iteration:  59% 1510/2561 [18:59<13:50,  1.26it/s]\u001b[A\n",
            "Iteration:  59% 1511/2561 [19:00<13:34,  1.29it/s]\u001b[A\n",
            "Iteration:  59% 1512/2561 [19:01<13:27,  1.30it/s]\u001b[A\n",
            "Iteration:  59% 1513/2561 [19:02<13:18,  1.31it/s]\u001b[A\n",
            "Iteration:  59% 1514/2561 [19:02<13:14,  1.32it/s]\u001b[A\n",
            "Iteration:  59% 1515/2561 [19:03<13:10,  1.32it/s]\u001b[A\n",
            "Iteration:  59% 1516/2561 [19:04<13:09,  1.32it/s]\u001b[A\n",
            "Iteration:  59% 1517/2561 [19:05<13:06,  1.33it/s]\u001b[A\n",
            "Iteration:  59% 1518/2561 [19:05<13:00,  1.34it/s]\u001b[A\n",
            "Iteration:  59% 1519/2561 [19:06<13:00,  1.34it/s]\u001b[A\n",
            "Iteration:  59% 1520/2561 [19:07<12:57,  1.34it/s]\u001b[A\n",
            "Iteration:  59% 1521/2561 [19:08<12:59,  1.33it/s]\u001b[A\n",
            "Iteration:  59% 1522/2561 [19:08<12:55,  1.34it/s]\u001b[A\n",
            "Iteration:  59% 1523/2561 [19:09<12:56,  1.34it/s]\u001b[A\n",
            "Iteration:  60% 1524/2561 [19:10<12:54,  1.34it/s]\u001b[A\n",
            "Iteration:  60% 1525/2561 [19:11<12:54,  1.34it/s]\u001b[A\n",
            "Iteration:  60% 1526/2561 [19:11<12:54,  1.34it/s]\u001b[A\n",
            "Iteration:  60% 1527/2561 [19:12<12:51,  1.34it/s]\u001b[A\n",
            "Iteration:  60% 1528/2561 [19:13<12:52,  1.34it/s]\u001b[A\n",
            "Iteration:  60% 1529/2561 [19:14<12:48,  1.34it/s]\u001b[A\n",
            "Iteration:  60% 1530/2561 [19:14<12:50,  1.34it/s]\u001b[A\n",
            "Iteration:  60% 1531/2561 [19:15<12:49,  1.34it/s]\u001b[A\n",
            "Iteration:  60% 1532/2561 [19:16<12:51,  1.33it/s]\u001b[A\n",
            "Iteration:  60% 1533/2561 [19:17<12:49,  1.34it/s]\u001b[A\n",
            "Iteration:  60% 1534/2561 [19:17<12:48,  1.34it/s]\u001b[A\n",
            "Iteration:  60% 1535/2561 [19:18<12:48,  1.33it/s]\u001b[A\n",
            "Iteration:  60% 1536/2561 [19:19<12:44,  1.34it/s]\u001b[A\n",
            "Iteration:  60% 1537/2561 [19:20<12:44,  1.34it/s]\u001b[A\n",
            "Iteration:  60% 1538/2561 [19:20<12:41,  1.34it/s]\u001b[A\n",
            "Iteration:  60% 1539/2561 [19:21<12:43,  1.34it/s]\u001b[A\n",
            "Iteration:  60% 1540/2561 [19:22<12:41,  1.34it/s]\u001b[A\n",
            "Iteration:  60% 1541/2561 [19:23<12:46,  1.33it/s]\u001b[A\n",
            "Iteration:  60% 1542/2561 [19:23<12:47,  1.33it/s]\u001b[A\n",
            "Iteration:  60% 1543/2561 [19:24<12:48,  1.32it/s]\u001b[A\n",
            "Iteration:  60% 1544/2561 [19:25<12:46,  1.33it/s]\u001b[A\n",
            "Iteration:  60% 1545/2561 [19:26<12:47,  1.32it/s]\u001b[A\n",
            "Iteration:  60% 1546/2561 [19:26<12:44,  1.33it/s]\u001b[A\n",
            "Iteration:  60% 1547/2561 [19:27<12:43,  1.33it/s]\u001b[A\n",
            "Iteration:  60% 1548/2561 [19:28<12:40,  1.33it/s]\u001b[A\n",
            "Iteration:  60% 1549/2561 [19:29<12:36,  1.34it/s]\u001b[A\n",
            "Iteration:  61% 1550/2561 [19:29<12:39,  1.33it/s]\u001b[A\n",
            "Iteration:  61% 1551/2561 [19:30<12:34,  1.34it/s]\u001b[A\n",
            "Iteration:  61% 1552/2561 [19:31<12:37,  1.33it/s]\u001b[A\n",
            "Iteration:  61% 1553/2561 [19:32<12:34,  1.34it/s]\u001b[A\n",
            "Iteration:  61% 1554/2561 [19:32<12:36,  1.33it/s]\u001b[A\n",
            "Iteration:  61% 1555/2561 [19:33<12:35,  1.33it/s]\u001b[A\n",
            "Iteration:  61% 1556/2561 [19:34<12:36,  1.33it/s]\u001b[A\n",
            "Iteration:  61% 1557/2561 [19:35<12:35,  1.33it/s]\u001b[A\n",
            "Iteration:  61% 1558/2561 [19:35<12:33,  1.33it/s]\u001b[A\n",
            "Iteration:  61% 1559/2561 [19:36<12:33,  1.33it/s]\u001b[A\n",
            "Iteration:  61% 1560/2561 [19:37<12:28,  1.34it/s]\u001b[A\n",
            "Iteration:  61% 1561/2561 [19:38<12:29,  1.33it/s]\u001b[A\n",
            "Iteration:  61% 1562/2561 [19:38<12:26,  1.34it/s]\u001b[A\n",
            "Iteration:  61% 1563/2561 [19:39<12:27,  1.33it/s]\u001b[A\n",
            "Iteration:  61% 1564/2561 [19:40<12:25,  1.34it/s]\u001b[A\n",
            "Iteration:  61% 1565/2561 [19:41<12:27,  1.33it/s]\u001b[A\n",
            "Iteration:  61% 1566/2561 [19:41<12:26,  1.33it/s]\u001b[A\n",
            "Iteration:  61% 1567/2561 [19:42<12:24,  1.33it/s]\u001b[A\n",
            "Iteration:  61% 1568/2561 [19:43<12:23,  1.34it/s]\u001b[A\n",
            "Iteration:  61% 1569/2561 [19:44<12:19,  1.34it/s]\u001b[A\n",
            "Iteration:  61% 1570/2561 [19:44<12:20,  1.34it/s]\u001b[A\n",
            "Iteration:  61% 1571/2561 [19:45<12:18,  1.34it/s]\u001b[A\n",
            "Iteration:  61% 1572/2561 [19:46<12:19,  1.34it/s]\u001b[A\n",
            "Iteration:  61% 1573/2561 [19:47<12:18,  1.34it/s]\u001b[A\n",
            "Iteration:  61% 1574/2561 [19:47<12:20,  1.33it/s]\u001b[A\n",
            "Iteration:  61% 1575/2561 [19:48<12:19,  1.33it/s]\u001b[A\n",
            "Iteration:  62% 1576/2561 [19:49<12:20,  1.33it/s]\u001b[A\n",
            "Iteration:  62% 1577/2561 [19:50<12:19,  1.33it/s]\u001b[A\n",
            "Iteration:  62% 1578/2561 [19:50<12:14,  1.34it/s]\u001b[A\n",
            "Iteration:  62% 1579/2561 [19:51<12:14,  1.34it/s]\u001b[A\n",
            "Iteration:  62% 1580/2561 [19:52<12:10,  1.34it/s]\u001b[A\n",
            "Iteration:  62% 1581/2561 [19:53<12:13,  1.34it/s]\u001b[A\n",
            "Iteration:  62% 1582/2561 [19:53<12:11,  1.34it/s]\u001b[A\n",
            "Iteration:  62% 1583/2561 [19:54<12:13,  1.33it/s]\u001b[A\n",
            "Iteration:  62% 1584/2561 [19:55<12:10,  1.34it/s]\u001b[A\n",
            "Iteration:  62% 1585/2561 [19:56<12:12,  1.33it/s]\u001b[A\n",
            "Iteration:  62% 1586/2561 [19:56<12:10,  1.33it/s]\u001b[A\n",
            "Iteration:  62% 1587/2561 [19:57<12:11,  1.33it/s]\u001b[A\n",
            "Iteration:  62% 1588/2561 [19:58<12:10,  1.33it/s]\u001b[A\n",
            "Iteration:  62% 1589/2561 [19:59<12:04,  1.34it/s]\u001b[A\n",
            "Iteration:  62% 1590/2561 [19:59<12:05,  1.34it/s]\u001b[A\n",
            "Iteration:  62% 1591/2561 [20:00<12:02,  1.34it/s]\u001b[A\n",
            "Iteration:  62% 1592/2561 [20:01<12:05,  1.34it/s]\u001b[A\n",
            "Iteration:  62% 1593/2561 [20:02<12:02,  1.34it/s]\u001b[A\n",
            "Iteration:  62% 1594/2561 [20:02<12:04,  1.33it/s]\u001b[A\n",
            "Iteration:  62% 1595/2561 [20:03<12:03,  1.34it/s]\u001b[A\n",
            "Iteration:  62% 1596/2561 [20:04<12:05,  1.33it/s]\u001b[A\n",
            "Iteration:  62% 1597/2561 [20:05<12:02,  1.33it/s]\u001b[A\n",
            "Iteration:  62% 1598/2561 [20:05<12:05,  1.33it/s]\u001b[A\n",
            "Iteration:  62% 1599/2561 [20:06<12:04,  1.33it/s]\u001b[A\n",
            "Iteration:  62% 1600/2561 [20:07<11:58,  1.34it/s]\u001b[A\n",
            "Iteration:  63% 1601/2561 [20:08<11:58,  1.34it/s]\u001b[A\n",
            "Iteration:  63% 1602/2561 [20:08<11:55,  1.34it/s]\u001b[A\n",
            "Iteration:  63% 1603/2561 [20:09<11:57,  1.33it/s]\u001b[A\n",
            "Iteration:  63% 1604/2561 [20:10<11:54,  1.34it/s]\u001b[A\n",
            "Iteration:  63% 1605/2561 [20:11<11:56,  1.33it/s]\u001b[A\n",
            "Iteration:  63% 1606/2561 [20:11<11:54,  1.34it/s]\u001b[A\n",
            "Iteration:  63% 1607/2561 [20:12<11:55,  1.33it/s]\u001b[A\n",
            "Iteration:  63% 1608/2561 [20:13<11:54,  1.33it/s]\u001b[A\n",
            "Iteration:  63% 1609/2561 [20:14<11:55,  1.33it/s]\u001b[A\n",
            "Iteration:  63% 1610/2561 [20:14<11:53,  1.33it/s]\u001b[A\n",
            "Iteration:  63% 1611/2561 [20:15<11:50,  1.34it/s]\u001b[A\n",
            "Iteration:  63% 1612/2561 [20:16<11:51,  1.33it/s]\u001b[A\n",
            "Iteration:  63% 1613/2561 [20:17<11:48,  1.34it/s]\u001b[A\n",
            "Iteration:  63% 1614/2561 [20:17<11:48,  1.34it/s]\u001b[A\n",
            "Iteration:  63% 1615/2561 [20:18<11:46,  1.34it/s]\u001b[A\n",
            "Iteration:  63% 1616/2561 [20:19<11:48,  1.33it/s]\u001b[A\n",
            "Iteration:  63% 1617/2561 [20:20<11:46,  1.34it/s]\u001b[A\n",
            "Iteration:  63% 1618/2561 [20:20<11:41,  1.34it/s]\u001b[A\n",
            "Iteration:  63% 1619/2561 [20:21<11:42,  1.34it/s]\u001b[A\n",
            "Iteration:  63% 1620/2561 [20:22<11:39,  1.34it/s]\u001b[A\n",
            "Iteration:  63% 1621/2561 [20:23<11:43,  1.34it/s]\u001b[A\n",
            "Iteration:  63% 1622/2561 [20:23<11:42,  1.34it/s]\u001b[A\n",
            "Iteration:  63% 1623/2561 [20:24<11:44,  1.33it/s]\u001b[A\n",
            "Iteration:  63% 1624/2561 [20:25<11:43,  1.33it/s]\u001b[A\n",
            "Iteration:  63% 1625/2561 [20:26<11:44,  1.33it/s]\u001b[A\n",
            "Iteration:  63% 1626/2561 [20:26<11:42,  1.33it/s]\u001b[A\n",
            "Iteration:  64% 1627/2561 [20:27<11:42,  1.33it/s]\u001b[A\n",
            "Iteration:  64% 1628/2561 [20:28<11:41,  1.33it/s]\u001b[A\n",
            "Iteration:  64% 1629/2561 [20:29<11:45,  1.32it/s]\u001b[A\n",
            "Iteration:  64% 1630/2561 [20:29<11:41,  1.33it/s]\u001b[A\n",
            "Iteration:  64% 1631/2561 [20:30<11:42,  1.32it/s]\u001b[A\n",
            "Iteration:  64% 1632/2561 [20:31<11:39,  1.33it/s]\u001b[A\n",
            "Iteration:  64% 1633/2561 [20:32<11:40,  1.32it/s]\u001b[A\n",
            "Iteration:  64% 1634/2561 [20:32<11:37,  1.33it/s]\u001b[A\n",
            "Iteration:  64% 1635/2561 [20:33<11:32,  1.34it/s]\u001b[A\n",
            "Iteration:  64% 1636/2561 [20:34<11:32,  1.33it/s]\u001b[A\n",
            "Iteration:  64% 1637/2561 [20:35<11:29,  1.34it/s]\u001b[A\n",
            "Iteration:  64% 1638/2561 [20:35<11:29,  1.34it/s]\u001b[A\n",
            "Iteration:  64% 1639/2561 [20:36<11:28,  1.34it/s]\u001b[A\n",
            "Iteration:  64% 1640/2561 [20:37<11:30,  1.33it/s]\u001b[A\n",
            "Iteration:  64% 1641/2561 [20:38<11:28,  1.34it/s]\u001b[A\n",
            "Iteration:  64% 1642/2561 [20:38<11:29,  1.33it/s]\u001b[A\n",
            "Iteration:  64% 1643/2561 [20:39<11:27,  1.33it/s]\u001b[A\n",
            "Iteration:  64% 1644/2561 [20:40<11:22,  1.34it/s]\u001b[A\n",
            "Iteration:  64% 1645/2561 [20:41<11:23,  1.34it/s]\u001b[A\n",
            "Iteration:  64% 1646/2561 [20:41<11:21,  1.34it/s]\u001b[A\n",
            "Iteration:  64% 1647/2561 [20:42<11:24,  1.34it/s]\u001b[A\n",
            "Iteration:  64% 1648/2561 [20:43<11:21,  1.34it/s]\u001b[A\n",
            "Iteration:  64% 1649/2561 [20:44<11:22,  1.34it/s]\u001b[A\n",
            "Iteration:  64% 1650/2561 [20:44<11:21,  1.34it/s]\u001b[A\n",
            "Iteration:  64% 1651/2561 [20:45<11:23,  1.33it/s]\u001b[A\n",
            "Iteration:  65% 1652/2561 [20:46<11:23,  1.33it/s]\u001b[A\n",
            "Iteration:  65% 1653/2561 [20:47<11:24,  1.33it/s]\u001b[A\n",
            "Iteration:  65% 1654/2561 [20:47<11:22,  1.33it/s]\u001b[A\n",
            "Iteration:  65% 1655/2561 [20:48<11:23,  1.32it/s]\u001b[A\n",
            "Iteration:  65% 1656/2561 [20:49<11:20,  1.33it/s]\u001b[A\n",
            "Iteration:  65% 1657/2561 [20:50<11:15,  1.34it/s]\u001b[A\n",
            "Iteration:  65% 1658/2561 [20:50<11:16,  1.33it/s]\u001b[A\n",
            "Iteration:  65% 1659/2561 [20:51<11:14,  1.34it/s]\u001b[A\n",
            "Iteration:  65% 1660/2561 [20:52<11:15,  1.33it/s]\u001b[A\n",
            "Iteration:  65% 1661/2561 [20:53<11:14,  1.33it/s]\u001b[A\n",
            "Iteration:  65% 1662/2561 [20:53<11:15,  1.33it/s]\u001b[A\n",
            "Iteration:  65% 1663/2561 [20:54<11:12,  1.33it/s]\u001b[A\n",
            "Iteration:  65% 1664/2561 [20:55<11:14,  1.33it/s]\u001b[A\n",
            "Iteration:  65% 1665/2561 [20:56<11:12,  1.33it/s]\u001b[A\n",
            "Iteration:  65% 1666/2561 [20:56<11:15,  1.33it/s]\u001b[A\n",
            "Iteration:  65% 1667/2561 [20:57<11:12,  1.33it/s]\u001b[A\n",
            "Iteration:  65% 1668/2561 [20:58<11:07,  1.34it/s]\u001b[A\n",
            "Iteration:  65% 1669/2561 [20:59<11:08,  1.33it/s]\u001b[A\n",
            "Iteration:  65% 1670/2561 [20:59<11:05,  1.34it/s]\u001b[A\n",
            "Iteration:  65% 1671/2561 [21:00<11:07,  1.33it/s]\u001b[A\n",
            "Iteration:  65% 1672/2561 [21:01<11:04,  1.34it/s]\u001b[A\n",
            "Iteration:  65% 1673/2561 [21:02<11:05,  1.33it/s]\u001b[A\n",
            "Iteration:  65% 1674/2561 [21:02<11:03,  1.34it/s]\u001b[A\n",
            "Iteration:  65% 1675/2561 [21:03<11:04,  1.33it/s]\u001b[A\n",
            "Iteration:  65% 1676/2561 [21:04<11:02,  1.33it/s]\u001b[A\n",
            "Iteration:  65% 1677/2561 [21:05<11:04,  1.33it/s]\u001b[A\n",
            "Iteration:  66% 1678/2561 [21:05<11:02,  1.33it/s]\u001b[A\n",
            "Iteration:  66% 1679/2561 [21:06<11:03,  1.33it/s]\u001b[A\n",
            "Iteration:  66% 1680/2561 [21:07<11:03,  1.33it/s]\u001b[A\n",
            "Iteration:  66% 1681/2561 [21:08<10:59,  1.33it/s]\u001b[A\n",
            "Iteration:  66% 1682/2561 [21:08<10:58,  1.33it/s]\u001b[A\n",
            "Iteration:  66% 1683/2561 [21:09<10:55,  1.34it/s]\u001b[A\n",
            "Iteration:  66% 1684/2561 [21:10<10:56,  1.34it/s]\u001b[A\n",
            "Iteration:  66% 1685/2561 [21:11<10:52,  1.34it/s]\u001b[A\n",
            "Iteration:  66% 1686/2561 [21:11<10:54,  1.34it/s]\u001b[A\n",
            "Iteration:  66% 1687/2561 [21:12<10:52,  1.34it/s]\u001b[A\n",
            "Iteration:  66% 1688/2561 [21:13<10:55,  1.33it/s]\u001b[A\n",
            "Iteration:  66% 1689/2561 [21:14<10:53,  1.33it/s]\u001b[A\n",
            "Iteration:  66% 1690/2561 [21:14<10:49,  1.34it/s]\u001b[A\n",
            "Iteration:  66% 1691/2561 [21:15<10:50,  1.34it/s]\u001b[A\n",
            "Iteration:  66% 1692/2561 [21:16<10:48,  1.34it/s]\u001b[A\n",
            "Iteration:  66% 1693/2561 [21:17<10:49,  1.34it/s]\u001b[A\n",
            "Iteration:  66% 1694/2561 [21:17<10:47,  1.34it/s]\u001b[A\n",
            "Iteration:  66% 1695/2561 [21:18<10:48,  1.34it/s]\u001b[A\n",
            "Iteration:  66% 1696/2561 [21:19<10:46,  1.34it/s]\u001b[A\n",
            "Iteration:  66% 1697/2561 [21:20<10:48,  1.33it/s]\u001b[A\n",
            "Iteration:  66% 1698/2561 [21:20<10:46,  1.33it/s]\u001b[A\n",
            "Iteration:  66% 1699/2561 [21:21<10:47,  1.33it/s]\u001b[A\n",
            "Iteration:  66% 1700/2561 [21:22<10:46,  1.33it/s]\u001b[A\n",
            "Iteration:  66% 1701/2561 [21:23<10:42,  1.34it/s]\u001b[A\n",
            "Iteration:  66% 1702/2561 [21:23<10:43,  1.33it/s]\u001b[A\n",
            "Iteration:  66% 1703/2561 [21:24<10:40,  1.34it/s]\u001b[A\n",
            "Iteration:  67% 1704/2561 [21:25<10:41,  1.33it/s]\u001b[A\n",
            "Iteration:  67% 1705/2561 [21:25<10:38,  1.34it/s]\u001b[A\n",
            "Iteration:  67% 1706/2561 [21:26<10:40,  1.34it/s]\u001b[A\n",
            "Iteration:  67% 1707/2561 [21:27<10:37,  1.34it/s]\u001b[A\n",
            "Iteration:  67% 1708/2561 [21:28<10:38,  1.34it/s]\u001b[A\n",
            "Iteration:  67% 1709/2561 [21:28<10:38,  1.34it/s]\u001b[A\n",
            "Iteration:  67% 1710/2561 [21:29<10:41,  1.33it/s]\u001b[A\n",
            "Iteration:  67% 1711/2561 [21:30<10:39,  1.33it/s]\u001b[A\n",
            "Iteration:  67% 1712/2561 [21:31<10:40,  1.33it/s]\u001b[A\n",
            "Iteration:  67% 1713/2561 [21:32<10:38,  1.33it/s]\u001b[A\n",
            "Iteration:  67% 1714/2561 [21:32<10:38,  1.33it/s]\u001b[A\n",
            "Iteration:  67% 1715/2561 [21:33<10:35,  1.33it/s]\u001b[A\n",
            "Iteration:  67% 1716/2561 [21:34<10:31,  1.34it/s]\u001b[A\n",
            "Iteration:  67% 1717/2561 [21:35<10:31,  1.34it/s]\u001b[A\n",
            "Iteration:  67% 1718/2561 [21:35<10:28,  1.34it/s]\u001b[A\n",
            "Iteration:  67% 1719/2561 [21:36<10:30,  1.34it/s]\u001b[A\n",
            "Iteration:  67% 1720/2561 [21:37<10:27,  1.34it/s]\u001b[A\n",
            "Iteration:  67% 1721/2561 [21:37<10:29,  1.33it/s]\u001b[A\n",
            "Iteration:  67% 1722/2561 [21:38<10:27,  1.34it/s]\u001b[A\n",
            "Iteration:  67% 1723/2561 [21:39<10:28,  1.33it/s]\u001b[A\n",
            "Iteration:  67% 1724/2561 [21:40<10:27,  1.33it/s]\u001b[A\n",
            "Iteration:  67% 1725/2561 [21:40<10:22,  1.34it/s]\u001b[A\n",
            "Iteration:  67% 1726/2561 [21:41<10:23,  1.34it/s]\u001b[A\n",
            "Iteration:  67% 1727/2561 [21:42<10:20,  1.34it/s]\u001b[A\n",
            "Iteration:  67% 1728/2561 [21:43<10:21,  1.34it/s]\u001b[A\n",
            "Iteration:  68% 1729/2561 [21:43<10:20,  1.34it/s]\u001b[A\n",
            "Iteration:  68% 1730/2561 [21:44<10:21,  1.34it/s]\u001b[A\n",
            "Iteration:  68% 1731/2561 [21:45<10:19,  1.34it/s]\u001b[A\n",
            "Iteration:  68% 1732/2561 [21:46<10:21,  1.33it/s]\u001b[A\n",
            "Iteration:  68% 1733/2561 [21:46<10:21,  1.33it/s]\u001b[A\n",
            "Iteration:  68% 1734/2561 [21:47<10:16,  1.34it/s]\u001b[A\n",
            "Iteration:  68% 1735/2561 [21:48<10:16,  1.34it/s]\u001b[A\n",
            "Iteration:  68% 1736/2561 [21:49<10:14,  1.34it/s]\u001b[A\n",
            "Iteration:  68% 1737/2561 [21:49<10:15,  1.34it/s]\u001b[A\n",
            "Iteration:  68% 1738/2561 [21:50<10:13,  1.34it/s]\u001b[A\n",
            "Iteration:  68% 1739/2561 [21:51<10:15,  1.34it/s]\u001b[A\n",
            "Iteration:  68% 1740/2561 [21:52<10:14,  1.34it/s]\u001b[A\n",
            "Iteration:  68% 1741/2561 [21:52<10:17,  1.33it/s]\u001b[A\n",
            "Iteration:  68% 1742/2561 [21:53<10:15,  1.33it/s]\u001b[A\n",
            "Iteration:  68% 1743/2561 [21:54<10:16,  1.33it/s]\u001b[A\n",
            "Iteration:  68% 1744/2561 [21:55<10:14,  1.33it/s]\u001b[A\n",
            "Iteration:  68% 1745/2561 [21:55<10:10,  1.34it/s]\u001b[A\n",
            "Iteration:  68% 1746/2561 [21:56<10:09,  1.34it/s]\u001b[A\n",
            "Iteration:  68% 1747/2561 [21:57<10:06,  1.34it/s]\u001b[A\n",
            "Iteration:  68% 1748/2561 [21:58<10:08,  1.34it/s]\u001b[A\n",
            "Iteration:  68% 1749/2561 [21:58<10:05,  1.34it/s]\u001b[A\n",
            "Iteration:  68% 1750/2561 [21:59<10:07,  1.34it/s]\u001b[A\n",
            "Iteration:  68% 1751/2561 [22:00<10:05,  1.34it/s]\u001b[A\n",
            "Iteration:  68% 1752/2561 [22:01<10:04,  1.34it/s]\u001b[A\n",
            "Iteration:  68% 1753/2561 [22:01<10:03,  1.34it/s]\u001b[A\n",
            "Iteration:  68% 1754/2561 [22:02<09:59,  1.35it/s]\u001b[A\n",
            "Iteration:  69% 1755/2561 [22:03<10:01,  1.34it/s]\u001b[A\n",
            "Iteration:  69% 1756/2561 [22:04<09:59,  1.34it/s]\u001b[A\n",
            "Iteration:  69% 1757/2561 [22:04<10:00,  1.34it/s]\u001b[A\n",
            "Iteration:  69% 1758/2561 [22:05<09:59,  1.34it/s]\u001b[A\n",
            "Iteration:  69% 1759/2561 [22:06<10:00,  1.34it/s]\u001b[A\n",
            "Iteration:  69% 1760/2561 [22:07<09:59,  1.34it/s]\u001b[A\n",
            "Iteration:  69% 1761/2561 [22:07<09:58,  1.34it/s]\u001b[A\n",
            "Iteration:  69% 1762/2561 [22:08<09:58,  1.34it/s]\u001b[A\n",
            "Iteration:  69% 1763/2561 [22:09<09:54,  1.34it/s]\u001b[A\n",
            "Iteration:  69% 1764/2561 [22:10<09:54,  1.34it/s]\u001b[A\n",
            "Iteration:  69% 1765/2561 [22:10<09:52,  1.34it/s]\u001b[A\n",
            "Iteration:  69% 1766/2561 [22:11<09:54,  1.34it/s]\u001b[A\n",
            "Iteration:  69% 1767/2561 [22:12<09:52,  1.34it/s]\u001b[A\n",
            "Iteration:  69% 1768/2561 [22:13<09:53,  1.34it/s]\u001b[A\n",
            "Iteration:  69% 1769/2561 [22:13<09:51,  1.34it/s]\u001b[A\n",
            "Iteration:  69% 1770/2561 [22:14<09:50,  1.34it/s]\u001b[A\n",
            "Iteration:  69% 1771/2561 [22:15<09:50,  1.34it/s]\u001b[A\n",
            "Iteration:  69% 1772/2561 [22:16<09:47,  1.34it/s]\u001b[A\n",
            "Iteration:  69% 1773/2561 [22:16<09:49,  1.34it/s]\u001b[A\n",
            "Iteration:  69% 1774/2561 [22:17<09:46,  1.34it/s]\u001b[A\n",
            "Iteration:  69% 1775/2561 [22:18<09:47,  1.34it/s]\u001b[A\n",
            "Iteration:  69% 1776/2561 [22:19<09:45,  1.34it/s]\u001b[A\n",
            "Iteration:  69% 1777/2561 [22:19<09:47,  1.33it/s]\u001b[A\n",
            "Iteration:  69% 1778/2561 [22:20<09:46,  1.33it/s]\u001b[A\n",
            "Iteration:  69% 1779/2561 [22:21<09:43,  1.34it/s]\u001b[A\n",
            "Iteration:  70% 1780/2561 [22:22<09:43,  1.34it/s]\u001b[A\n",
            "Iteration:  70% 1781/2561 [22:22<09:41,  1.34it/s]\u001b[A\n",
            "Iteration:  70% 1782/2561 [22:23<09:42,  1.34it/s]\u001b[A\n",
            "Iteration:  70% 1783/2561 [22:24<09:40,  1.34it/s]\u001b[A\n",
            "Iteration:  70% 1784/2561 [22:25<09:41,  1.34it/s]\u001b[A\n",
            "Iteration:  70% 1785/2561 [22:25<09:39,  1.34it/s]\u001b[A\n",
            "Iteration:  70% 1786/2561 [22:26<09:41,  1.33it/s]\u001b[A\n",
            "Iteration:  70% 1787/2561 [22:27<09:40,  1.33it/s]\u001b[A\n",
            "Iteration:  70% 1788/2561 [22:28<09:36,  1.34it/s]\u001b[A\n",
            "Iteration:  70% 1789/2561 [22:28<09:36,  1.34it/s]\u001b[A\n",
            "Iteration:  70% 1790/2561 [22:29<09:35,  1.34it/s]\u001b[A\n",
            "Iteration:  70% 1791/2561 [22:30<09:36,  1.34it/s]\u001b[A\n",
            "Iteration:  70% 1792/2561 [22:31<09:33,  1.34it/s]\u001b[A\n",
            "Iteration:  70% 1793/2561 [22:31<09:34,  1.34it/s]\u001b[A\n",
            "Iteration:  70% 1794/2561 [22:32<09:33,  1.34it/s]\u001b[A\n",
            "Iteration:  70% 1795/2561 [22:33<09:35,  1.33it/s]\u001b[A\n",
            "Iteration:  70% 1796/2561 [22:34<09:34,  1.33it/s]\u001b[A\n",
            "Iteration:  70% 1797/2561 [22:34<09:29,  1.34it/s]\u001b[A\n",
            "Iteration:  70% 1798/2561 [22:35<09:29,  1.34it/s]\u001b[A\n",
            "Iteration:  70% 1799/2561 [22:36<09:27,  1.34it/s]\u001b[A\n",
            "Iteration:  70% 1800/2561 [22:37<09:28,  1.34it/s]\u001b[A\n",
            "Iteration:  70% 1801/2561 [22:37<09:27,  1.34it/s]\u001b[A\n",
            "Iteration:  70% 1802/2561 [22:38<09:28,  1.34it/s]\u001b[A\n",
            "Iteration:  70% 1803/2561 [22:39<09:26,  1.34it/s]\u001b[A\n",
            "Iteration:  70% 1804/2561 [22:40<09:28,  1.33it/s]\u001b[A\n",
            "Iteration:  70% 1805/2561 [22:40<09:27,  1.33it/s]\u001b[A\n",
            "Iteration:  71% 1806/2561 [22:41<09:28,  1.33it/s]\u001b[A\n",
            "Iteration:  71% 1807/2561 [22:42<09:26,  1.33it/s]\u001b[A\n",
            "Iteration:  71% 1808/2561 [22:43<09:21,  1.34it/s]\u001b[A\n",
            "Iteration:  71% 1809/2561 [22:43<09:21,  1.34it/s]\u001b[A\n",
            "Iteration:  71% 1810/2561 [22:44<09:18,  1.34it/s]\u001b[A\n",
            "Iteration:  71% 1811/2561 [22:45<09:20,  1.34it/s]\u001b[A\n",
            "Iteration:  71% 1812/2561 [22:46<09:18,  1.34it/s]\u001b[A\n",
            "Iteration:  71% 1813/2561 [22:46<09:19,  1.34it/s]\u001b[A\n",
            "Iteration:  71% 1814/2561 [22:47<09:17,  1.34it/s]\u001b[A\n",
            "Iteration:  71% 1815/2561 [22:48<09:18,  1.33it/s]\u001b[A\n",
            "Iteration:  71% 1816/2561 [22:49<09:19,  1.33it/s]\u001b[A\n",
            "Iteration:  71% 1817/2561 [22:49<09:19,  1.33it/s]\u001b[A\n",
            "Iteration:  71% 1818/2561 [22:50<09:18,  1.33it/s]\u001b[A\n",
            "Iteration:  71% 1819/2561 [22:51<09:13,  1.34it/s]\u001b[A\n",
            "Iteration:  71% 1820/2561 [22:52<09:13,  1.34it/s]\u001b[A\n",
            "Iteration:  71% 1821/2561 [22:52<09:12,  1.34it/s]\u001b[A\n",
            "Iteration:  71% 1822/2561 [22:53<09:13,  1.33it/s]\u001b[A\n",
            "Iteration:  71% 1823/2561 [22:54<09:11,  1.34it/s]\u001b[A\n",
            "Iteration:  71% 1824/2561 [22:55<09:12,  1.33it/s]\u001b[A\n",
            "Iteration:  71% 1825/2561 [22:55<09:10,  1.34it/s]\u001b[A\n",
            "Iteration:  71% 1826/2561 [22:56<09:11,  1.33it/s]\u001b[A\n",
            "Iteration:  71% 1827/2561 [22:57<09:09,  1.34it/s]\u001b[A\n",
            "Iteration:  71% 1828/2561 [22:58<09:10,  1.33it/s]\u001b[A\n",
            "Iteration:  71% 1829/2561 [22:58<09:08,  1.33it/s]\u001b[A\n",
            "Iteration:  71% 1830/2561 [22:59<09:04,  1.34it/s]\u001b[A\n",
            "Iteration:  71% 1831/2561 [23:00<09:05,  1.34it/s]\u001b[A\n",
            "Iteration:  72% 1832/2561 [23:00<09:02,  1.34it/s]\u001b[A\n",
            "Iteration:  72% 1833/2561 [23:01<09:03,  1.34it/s]\u001b[A\n",
            "Iteration:  72% 1834/2561 [23:02<09:01,  1.34it/s]\u001b[A\n",
            "Iteration:  72% 1835/2561 [23:03<09:03,  1.34it/s]\u001b[A\n",
            "Iteration:  72% 1836/2561 [23:03<09:02,  1.34it/s]\u001b[A\n",
            "Iteration:  72% 1837/2561 [23:04<09:01,  1.34it/s]\u001b[A\n",
            "Iteration:  72% 1838/2561 [23:05<09:00,  1.34it/s]\u001b[A\n",
            "Iteration:  72% 1839/2561 [23:06<08:58,  1.34it/s]\u001b[A\n",
            "Iteration:  72% 1840/2561 [23:06<08:59,  1.34it/s]\u001b[A\n",
            "Iteration:  72% 1841/2561 [23:07<08:56,  1.34it/s]\u001b[A\n",
            "Iteration:  72% 1842/2561 [23:08<08:57,  1.34it/s]\u001b[A\n",
            "Iteration:  72% 1843/2561 [23:09<08:54,  1.34it/s]\u001b[A\n",
            "Iteration:  72% 1844/2561 [23:09<08:56,  1.34it/s]\u001b[A\n",
            "Iteration:  72% 1845/2561 [23:10<08:55,  1.34it/s]\u001b[A\n",
            "Iteration:  72% 1846/2561 [23:11<08:54,  1.34it/s]\u001b[A\n",
            "Iteration:  72% 1847/2561 [23:12<08:54,  1.34it/s]\u001b[A\n",
            "Iteration:  72% 1848/2561 [23:12<08:51,  1.34it/s]\u001b[A\n",
            "Iteration:  72% 1849/2561 [23:13<08:51,  1.34it/s]\u001b[A\n",
            "Iteration:  72% 1850/2561 [23:14<08:49,  1.34it/s]\u001b[A\n",
            "Iteration:  72% 1851/2561 [23:15<08:51,  1.34it/s]\u001b[A\n",
            "Iteration:  72% 1852/2561 [23:15<08:49,  1.34it/s]\u001b[A\n",
            "Iteration:  72% 1853/2561 [23:16<08:51,  1.33it/s]\u001b[A\n",
            "Iteration:  72% 1854/2561 [23:17<08:50,  1.33it/s]\u001b[A\n",
            "Iteration:  72% 1855/2561 [23:18<08:50,  1.33it/s]\u001b[A\n",
            "Iteration:  72% 1856/2561 [23:18<08:49,  1.33it/s]\u001b[A\n",
            "Iteration:  73% 1857/2561 [23:19<08:46,  1.34it/s]\u001b[A\n",
            "Iteration:  73% 1858/2561 [23:20<08:46,  1.34it/s]\u001b[A\n",
            "Iteration:  73% 1859/2561 [23:21<08:42,  1.34it/s]\u001b[A\n",
            "Iteration:  73% 1860/2561 [23:21<08:43,  1.34it/s]\u001b[A\n",
            "Iteration:  73% 1861/2561 [23:22<08:42,  1.34it/s]\u001b[A\n",
            "Iteration:  73% 1862/2561 [23:23<08:44,  1.33it/s]\u001b[A\n",
            "Iteration:  73% 1863/2561 [23:24<08:41,  1.34it/s]\u001b[A\n",
            "Iteration:  73% 1864/2561 [23:24<08:42,  1.33it/s]\u001b[A\n",
            "Iteration:  73% 1865/2561 [23:25<08:41,  1.34it/s]\u001b[A\n",
            "Iteration:  73% 1866/2561 [23:26<08:39,  1.34it/s]\u001b[A\n",
            "Iteration:  73% 1867/2561 [23:27<08:38,  1.34it/s]\u001b[A\n",
            "Iteration:  73% 1868/2561 [23:27<08:36,  1.34it/s]\u001b[A\n",
            "Iteration:  73% 1869/2561 [23:28<08:38,  1.34it/s]\u001b[A\n",
            "Iteration:  73% 1870/2561 [23:29<08:36,  1.34it/s]\u001b[A\n",
            "Iteration:  73% 1871/2561 [23:30<08:37,  1.33it/s]\u001b[A\n",
            "Iteration:  73% 1872/2561 [23:30<08:34,  1.34it/s]\u001b[A\n",
            "Iteration:  73% 1873/2561 [23:31<08:36,  1.33it/s]\u001b[A\n",
            "Iteration:  73% 1874/2561 [23:32<08:34,  1.34it/s]\u001b[A\n",
            "Iteration:  73% 1875/2561 [23:33<08:35,  1.33it/s]\u001b[A\n",
            "Iteration:  73% 1876/2561 [23:33<08:34,  1.33it/s]\u001b[A\n",
            "Iteration:  73% 1877/2561 [23:34<08:30,  1.34it/s]\u001b[A\n",
            "Iteration:  73% 1878/2561 [23:35<08:30,  1.34it/s]\u001b[A\n",
            "Iteration:  73% 1879/2561 [23:36<08:27,  1.34it/s]\u001b[A\n",
            "Iteration:  73% 1880/2561 [23:36<08:28,  1.34it/s]\u001b[A\n",
            "Iteration:  73% 1881/2561 [23:37<08:27,  1.34it/s]\u001b[A\n",
            "Iteration:  73% 1882/2561 [23:38<08:28,  1.34it/s]\u001b[A\n",
            "Iteration:  74% 1883/2561 [23:39<08:26,  1.34it/s]\u001b[A\n",
            "Iteration:  74% 1884/2561 [23:39<08:28,  1.33it/s]\u001b[A\n",
            "Iteration:  74% 1885/2561 [23:40<08:27,  1.33it/s]\u001b[A\n",
            "Iteration:  74% 1886/2561 [23:41<08:27,  1.33it/s]\u001b[A\n",
            "Iteration:  74% 1887/2561 [23:42<08:25,  1.33it/s]\u001b[A\n",
            "Iteration:  74% 1888/2561 [23:42<08:22,  1.34it/s]\u001b[A\n",
            "Iteration:  74% 1889/2561 [23:43<08:23,  1.34it/s]\u001b[A\n",
            "Iteration:  74% 1890/2561 [23:44<08:20,  1.34it/s]\u001b[A\n",
            "Iteration:  74% 1891/2561 [23:45<08:20,  1.34it/s]\u001b[A\n",
            "Iteration:  74% 1892/2561 [23:45<08:18,  1.34it/s]\u001b[A\n",
            "Iteration:  74% 1893/2561 [23:46<08:19,  1.34it/s]\u001b[A\n",
            "Iteration:  74% 1894/2561 [23:47<08:18,  1.34it/s]\u001b[A\n",
            "Iteration:  74% 1895/2561 [23:48<08:19,  1.33it/s]\u001b[A\n",
            "Iteration:  74% 1896/2561 [23:48<08:18,  1.33it/s]\u001b[A\n",
            "Iteration:  74% 1897/2561 [23:49<08:14,  1.34it/s]\u001b[A\n",
            "Iteration:  74% 1898/2561 [23:50<08:15,  1.34it/s]\u001b[A\n",
            "Iteration:  74% 1899/2561 [23:51<08:13,  1.34it/s]\u001b[A\n",
            "Iteration:  74% 1900/2561 [23:51<08:14,  1.34it/s]\u001b[A\n",
            "Iteration:  74% 1901/2561 [23:52<08:12,  1.34it/s]\u001b[A\n",
            "Iteration:  74% 1902/2561 [23:53<08:14,  1.33it/s]\u001b[A\n",
            "Iteration:  74% 1903/2561 [23:54<08:12,  1.34it/s]\u001b[A\n",
            "Iteration:  74% 1904/2561 [23:54<08:11,  1.34it/s]\u001b[A\n",
            "Iteration:  74% 1905/2561 [23:55<08:11,  1.34it/s]\u001b[A\n",
            "Iteration:  74% 1906/2561 [23:56<08:08,  1.34it/s]\u001b[A\n",
            "Iteration:  74% 1907/2561 [23:57<08:08,  1.34it/s]\u001b[A\n",
            "Iteration:  75% 1908/2561 [23:57<08:06,  1.34it/s]\u001b[A\n",
            "Iteration:  75% 1909/2561 [23:58<08:08,  1.34it/s]\u001b[A\n",
            "Iteration:  75% 1910/2561 [23:59<08:06,  1.34it/s]\u001b[A\n",
            "Iteration:  75% 1911/2561 [24:00<08:06,  1.34it/s]\u001b[A\n",
            "Iteration:  75% 1912/2561 [24:00<08:06,  1.33it/s]\u001b[A\n",
            "Iteration:  75% 1913/2561 [24:01<08:04,  1.34it/s]\u001b[A\n",
            "Iteration:  75% 1914/2561 [24:02<08:04,  1.33it/s]\u001b[A\n",
            "Iteration:  75% 1915/2561 [24:03<08:02,  1.34it/s]\u001b[A\n",
            "Iteration:  75% 1916/2561 [24:03<08:03,  1.34it/s]\u001b[A\n",
            "Iteration:  75% 1917/2561 [24:04<07:59,  1.34it/s]\u001b[A\n",
            "Iteration:  75% 1918/2561 [24:05<08:00,  1.34it/s]\u001b[A\n",
            "Iteration:  75% 1919/2561 [24:06<07:59,  1.34it/s]\u001b[A\n",
            "Iteration:  75% 1920/2561 [24:06<08:00,  1.34it/s]\u001b[A\n",
            "Iteration:  75% 1921/2561 [24:07<07:58,  1.34it/s]\u001b[A\n",
            "Iteration:  75% 1922/2561 [24:08<07:59,  1.33it/s]\u001b[A\n",
            "Iteration:  75% 1923/2561 [24:09<07:57,  1.33it/s]\u001b[A\n",
            "Iteration:  75% 1924/2561 [24:09<07:57,  1.33it/s]\u001b[A\n",
            "Iteration:  75% 1925/2561 [24:10<07:56,  1.33it/s]\u001b[A\n",
            "Iteration:  75% 1926/2561 [24:11<07:54,  1.34it/s]\u001b[A\n",
            "Iteration:  75% 1927/2561 [24:12<07:53,  1.34it/s]\u001b[A\n",
            "Iteration:  75% 1928/2561 [24:12<07:51,  1.34it/s]\u001b[A\n",
            "Iteration:  75% 1929/2561 [24:13<07:52,  1.34it/s]\u001b[A\n",
            "Iteration:  75% 1930/2561 [24:14<07:50,  1.34it/s]\u001b[A\n",
            "Iteration:  75% 1931/2561 [24:15<07:51,  1.34it/s]\u001b[A\n",
            "Iteration:  75% 1932/2561 [24:15<07:50,  1.34it/s]\u001b[A\n",
            "Iteration:  75% 1933/2561 [24:16<07:50,  1.33it/s]\u001b[A\n",
            "Iteration:  76% 1934/2561 [24:17<07:49,  1.34it/s]\u001b[A\n",
            "Iteration:  76% 1935/2561 [24:18<07:45,  1.34it/s]\u001b[A\n",
            "Iteration:  76% 1936/2561 [24:18<07:46,  1.34it/s]\u001b[A\n",
            "Iteration:  76% 1937/2561 [24:19<07:44,  1.34it/s]\u001b[A\n",
            "Iteration:  76% 1938/2561 [24:20<07:44,  1.34it/s]\u001b[A\n",
            "Iteration:  76% 1939/2561 [24:20<07:43,  1.34it/s]\u001b[A\n",
            "Iteration:  76% 1940/2561 [24:21<07:42,  1.34it/s]\u001b[A\n",
            "Iteration:  76% 1941/2561 [24:22<07:41,  1.34it/s]\u001b[A\n",
            "Iteration:  76% 1942/2561 [24:23<07:39,  1.35it/s]\u001b[A\n",
            "Iteration:  76% 1943/2561 [24:23<07:40,  1.34it/s]\u001b[A\n",
            "Iteration:  76% 1944/2561 [24:24<07:38,  1.35it/s]\u001b[A\n",
            "Iteration:  76% 1945/2561 [24:25<07:39,  1.34it/s]\u001b[A\n",
            "Iteration:  76% 1946/2561 [24:26<07:37,  1.34it/s]\u001b[A\n",
            "Iteration:  76% 1947/2561 [24:26<07:39,  1.34it/s]\u001b[A\n",
            "Iteration:  76% 1948/2561 [24:27<07:38,  1.34it/s]\u001b[A\n",
            "Iteration:  76% 1949/2561 [24:28<07:38,  1.33it/s]\u001b[A\n",
            "Iteration:  76% 1950/2561 [24:29<07:37,  1.33it/s]\u001b[A\n",
            "Iteration:  76% 1951/2561 [24:29<07:35,  1.34it/s]\u001b[A\n",
            "Iteration:  76% 1952/2561 [24:30<07:35,  1.34it/s]\u001b[A\n",
            "Iteration:  76% 1953/2561 [24:31<07:33,  1.34it/s]\u001b[A\n",
            "Iteration:  76% 1954/2561 [24:32<07:33,  1.34it/s]\u001b[A\n",
            "Iteration:  76% 1955/2561 [24:32<07:31,  1.34it/s]\u001b[A\n",
            "Iteration:  76% 1956/2561 [24:33<07:32,  1.34it/s]\u001b[A\n",
            "Iteration:  76% 1957/2561 [24:34<07:30,  1.34it/s]\u001b[A\n",
            "Iteration:  76% 1958/2561 [24:35<07:31,  1.33it/s]\u001b[A\n",
            "Iteration:  76% 1959/2561 [24:35<07:30,  1.34it/s]\u001b[A\n",
            "Iteration:  77% 1960/2561 [24:36<07:30,  1.33it/s]\u001b[A\n",
            "Iteration:  77% 1961/2561 [24:37<07:29,  1.33it/s]\u001b[A\n",
            "Iteration:  77% 1962/2561 [24:38<07:26,  1.34it/s]\u001b[A\n",
            "Iteration:  77% 1963/2561 [24:38<07:26,  1.34it/s]\u001b[A\n",
            "Iteration:  77% 1964/2561 [24:39<07:24,  1.34it/s]\u001b[A\n",
            "Iteration:  77% 1965/2561 [24:40<07:25,  1.34it/s]\u001b[A\n",
            "Iteration:  77% 1966/2561 [24:41<07:23,  1.34it/s]\u001b[A\n",
            "Iteration:  77% 1967/2561 [24:41<07:25,  1.33it/s]\u001b[A\n",
            "Iteration:  77% 1968/2561 [24:42<07:23,  1.34it/s]\u001b[A\n",
            "Iteration:  77% 1969/2561 [24:43<07:20,  1.34it/s]\u001b[A\n",
            "Iteration:  77% 1970/2561 [24:44<07:19,  1.34it/s]\u001b[A\n",
            "Iteration:  77% 1971/2561 [24:44<07:17,  1.35it/s]\u001b[A\n",
            "Iteration:  77% 1972/2561 [24:45<07:18,  1.34it/s]\u001b[A\n",
            "Iteration:  77% 1973/2561 [24:46<07:17,  1.34it/s]\u001b[A\n",
            "Iteration:  77% 1974/2561 [24:47<07:18,  1.34it/s]\u001b[A\n",
            "Iteration:  77% 1975/2561 [24:47<07:18,  1.34it/s]\u001b[A\n",
            "Iteration:  77% 1976/2561 [24:48<07:17,  1.34it/s]\u001b[A\n",
            "Iteration:  77% 1977/2561 [24:49<07:17,  1.34it/s]\u001b[A\n",
            "Iteration:  77% 1978/2561 [24:50<07:14,  1.34it/s]\u001b[A\n",
            "Iteration:  77% 1979/2561 [24:50<07:15,  1.34it/s]\u001b[A\n",
            "Iteration:  77% 1980/2561 [24:51<07:13,  1.34it/s]\u001b[A\n",
            "Iteration:  77% 1981/2561 [24:52<07:13,  1.34it/s]\u001b[A\n",
            "Iteration:  77% 1982/2561 [24:53<07:12,  1.34it/s]\u001b[A\n",
            "Iteration:  77% 1983/2561 [24:53<07:11,  1.34it/s]\u001b[A\n",
            "Iteration:  77% 1984/2561 [24:54<07:11,  1.34it/s]\u001b[A\n",
            "Iteration:  78% 1985/2561 [24:55<07:09,  1.34it/s]\u001b[A\n",
            "Iteration:  78% 1986/2561 [24:56<07:09,  1.34it/s]\u001b[A\n",
            "Iteration:  78% 1987/2561 [24:56<07:07,  1.34it/s]\u001b[A\n",
            "Iteration:  78% 1988/2561 [24:57<07:08,  1.34it/s]\u001b[A\n",
            "Iteration:  78% 1989/2561 [24:58<07:06,  1.34it/s]\u001b[A\n",
            "Iteration:  78% 1990/2561 [24:59<07:07,  1.34it/s]\u001b[A\n",
            "Iteration:  78% 1991/2561 [24:59<07:06,  1.34it/s]\u001b[A\n",
            "Iteration:  78% 1992/2561 [25:00<07:06,  1.33it/s]\u001b[A\n",
            "Iteration:  78% 1993/2561 [25:01<07:05,  1.33it/s]\u001b[A\n",
            "Iteration:  78% 1994/2561 [25:02<07:04,  1.34it/s]\u001b[A\n",
            "Iteration:  78% 1995/2561 [25:02<07:03,  1.34it/s]\u001b[A\n",
            "Iteration:  78% 1996/2561 [25:03<07:00,  1.34it/s]\u001b[A\n",
            "Iteration:  78% 1997/2561 [25:04<07:01,  1.34it/s]\u001b[A\n",
            "Iteration:  78% 1998/2561 [25:05<06:59,  1.34it/s]\u001b[A\n",
            "Iteration:  78% 1999/2561 [25:05<07:00,  1.34it/s]\u001b[A02/29/2020 07:53:15 - INFO - transformers.configuration_utils -   Configuration saved in output_roberta_GB/checkpoint-2000/config.json\n",
            "02/29/2020 07:53:16 - INFO - transformers.modeling_utils -   Model weights saved in output_roberta_GB/checkpoint-2000/pytorch_model.bin\n",
            "02/29/2020 07:53:16 - INFO - __main__ -   Saving model checkpoint to output_roberta_GB/checkpoint-2000\n",
            "02/29/2020 07:53:20 - INFO - __main__ -   Saving optimizer and scheduler states to output_roberta_GB/checkpoint-2000\n",
            "\n",
            "Iteration:  78% 2000/2561 [25:11<21:42,  2.32s/it]\u001b[A\n",
            "Iteration:  78% 2001/2561 [25:12<17:18,  1.85s/it]\u001b[A\n",
            "Iteration:  78% 2002/2561 [25:13<14:08,  1.52s/it]\u001b[A\n",
            "Iteration:  78% 2003/2561 [25:14<11:58,  1.29s/it]\u001b[A\n",
            "Iteration:  78% 2004/2561 [25:14<10:26,  1.12s/it]\u001b[A\n",
            "Iteration:  78% 2005/2561 [25:15<09:22,  1.01s/it]\u001b[A\n",
            "Iteration:  78% 2006/2561 [25:16<08:36,  1.07it/s]\u001b[A\n",
            "Iteration:  78% 2007/2561 [25:17<08:06,  1.14it/s]\u001b[A\n",
            "Iteration:  78% 2008/2561 [25:17<07:43,  1.19it/s]\u001b[A\n",
            "Iteration:  78% 2009/2561 [25:18<07:28,  1.23it/s]\u001b[A\n",
            "Iteration:  78% 2010/2561 [25:19<07:16,  1.26it/s]\u001b[A\n",
            "Iteration:  79% 2011/2561 [25:20<07:07,  1.29it/s]\u001b[A\n",
            "Iteration:  79% 2012/2561 [25:20<07:01,  1.30it/s]\u001b[A\n",
            "Iteration:  79% 2013/2561 [25:21<06:55,  1.32it/s]\u001b[A\n",
            "Iteration:  79% 2014/2561 [25:22<06:53,  1.32it/s]\u001b[A\n",
            "Iteration:  79% 2015/2561 [25:23<06:50,  1.33it/s]\u001b[A\n",
            "Iteration:  79% 2016/2561 [25:23<06:50,  1.33it/s]\u001b[A\n",
            "Iteration:  79% 2017/2561 [25:24<06:48,  1.33it/s]\u001b[A\n",
            "Iteration:  79% 2018/2561 [25:25<06:48,  1.33it/s]\u001b[A\n",
            "Iteration:  79% 2019/2561 [25:26<06:47,  1.33it/s]\u001b[A\n",
            "Iteration:  79% 2020/2561 [25:26<06:44,  1.34it/s]\u001b[A\n",
            "Iteration:  79% 2021/2561 [25:27<06:43,  1.34it/s]\u001b[A\n",
            "Iteration:  79% 2022/2561 [25:28<06:42,  1.34it/s]\u001b[A\n",
            "Iteration:  79% 2023/2561 [25:28<06:42,  1.34it/s]\u001b[A\n",
            "Iteration:  79% 2024/2561 [25:29<06:40,  1.34it/s]\u001b[A\n",
            "Iteration:  79% 2025/2561 [25:30<06:41,  1.33it/s]\u001b[A\n",
            "Iteration:  79% 2026/2561 [25:31<06:40,  1.33it/s]\u001b[A\n",
            "Iteration:  79% 2027/2561 [25:31<06:41,  1.33it/s]\u001b[A\n",
            "Iteration:  79% 2028/2561 [25:32<06:38,  1.34it/s]\u001b[A\n",
            "Iteration:  79% 2029/2561 [25:33<06:35,  1.34it/s]\u001b[A\n",
            "Iteration:  79% 2030/2561 [25:34<06:36,  1.34it/s]\u001b[A\n",
            "Iteration:  79% 2031/2561 [25:34<06:34,  1.34it/s]\u001b[A\n",
            "Iteration:  79% 2032/2561 [25:35<06:35,  1.34it/s]\u001b[A\n",
            "Iteration:  79% 2033/2561 [25:36<06:33,  1.34it/s]\u001b[A\n",
            "Iteration:  79% 2034/2561 [25:37<06:34,  1.33it/s]\u001b[A\n",
            "Iteration:  79% 2035/2561 [25:37<06:34,  1.33it/s]\u001b[A\n",
            "Iteration:  80% 2036/2561 [25:38<06:35,  1.33it/s]\u001b[A\n",
            "Iteration:  80% 2037/2561 [25:39<06:34,  1.33it/s]\u001b[A\n",
            "Iteration:  80% 2038/2561 [25:40<06:33,  1.33it/s]\u001b[A\n",
            "Iteration:  80% 2039/2561 [25:40<06:31,  1.33it/s]\u001b[A\n",
            "Iteration:  80% 2040/2561 [25:41<06:29,  1.34it/s]\u001b[A\n",
            "Iteration:  80% 2041/2561 [25:42<06:29,  1.33it/s]\u001b[A\n",
            "Iteration:  80% 2042/2561 [25:43<06:27,  1.34it/s]\u001b[A\n",
            "Iteration:  80% 2043/2561 [25:43<06:27,  1.34it/s]\u001b[A\n",
            "Iteration:  80% 2044/2561 [25:44<06:26,  1.34it/s]\u001b[A\n",
            "Iteration:  80% 2045/2561 [25:45<06:27,  1.33it/s]\u001b[A\n",
            "Iteration:  80% 2046/2561 [25:46<06:25,  1.33it/s]\u001b[A\n",
            "Iteration:  80% 2047/2561 [25:46<06:27,  1.33it/s]\u001b[A\n",
            "Iteration:  80% 2048/2561 [25:47<06:25,  1.33it/s]\u001b[A\n",
            "Iteration:  80% 2049/2561 [25:48<06:25,  1.33it/s]\u001b[A\n",
            "Iteration:  80% 2050/2561 [25:49<06:24,  1.33it/s]\u001b[A\n",
            "Iteration:  80% 2051/2561 [25:49<06:21,  1.34it/s]\u001b[A\n",
            "Iteration:  80% 2052/2561 [25:50<06:20,  1.34it/s]\u001b[A\n",
            "Iteration:  80% 2053/2561 [25:51<06:18,  1.34it/s]\u001b[A\n",
            "Iteration:  80% 2054/2561 [25:52<06:19,  1.34it/s]\u001b[A\n",
            "Iteration:  80% 2055/2561 [25:52<06:18,  1.34it/s]\u001b[A\n",
            "Iteration:  80% 2056/2561 [25:53<06:18,  1.33it/s]\u001b[A\n",
            "Iteration:  80% 2057/2561 [25:54<06:17,  1.33it/s]\u001b[A\n",
            "Iteration:  80% 2058/2561 [25:55<06:18,  1.33it/s]\u001b[A\n",
            "Iteration:  80% 2059/2561 [25:55<06:17,  1.33it/s]\u001b[A\n",
            "Iteration:  80% 2060/2561 [25:56<06:17,  1.33it/s]\u001b[A\n",
            "Iteration:  80% 2061/2561 [25:57<06:16,  1.33it/s]\u001b[A\n",
            "Iteration:  81% 2062/2561 [25:58<06:12,  1.34it/s]\u001b[A\n",
            "Iteration:  81% 2063/2561 [25:58<06:12,  1.34it/s]\u001b[A\n",
            "Iteration:  81% 2064/2561 [25:59<06:10,  1.34it/s]\u001b[A\n",
            "Iteration:  81% 2065/2561 [26:00<06:10,  1.34it/s]\u001b[A\n",
            "Iteration:  81% 2066/2561 [26:01<06:09,  1.34it/s]\u001b[A\n",
            "Iteration:  81% 2067/2561 [26:01<06:10,  1.33it/s]\u001b[A\n",
            "Iteration:  81% 2068/2561 [26:02<06:09,  1.33it/s]\u001b[A\n",
            "Iteration:  81% 2069/2561 [26:03<06:08,  1.34it/s]\u001b[A\n",
            "Iteration:  81% 2070/2561 [26:04<06:07,  1.34it/s]\u001b[A\n",
            "Iteration:  81% 2071/2561 [26:04<06:04,  1.34it/s]\u001b[A\n",
            "Iteration:  81% 2072/2561 [26:05<06:05,  1.34it/s]\u001b[A\n",
            "Iteration:  81% 2073/2561 [26:06<06:03,  1.34it/s]\u001b[A\n",
            "Iteration:  81% 2074/2561 [26:07<06:04,  1.34it/s]\u001b[A\n",
            "Iteration:  81% 2075/2561 [26:07<06:03,  1.34it/s]\u001b[A\n",
            "Iteration:  81% 2076/2561 [26:08<06:03,  1.33it/s]\u001b[A\n",
            "Iteration:  81% 2077/2561 [26:09<06:02,  1.34it/s]\u001b[A\n",
            "Iteration:  81% 2078/2561 [26:10<06:02,  1.33it/s]\u001b[A\n",
            "Iteration:  81% 2079/2561 [26:10<06:02,  1.33it/s]\u001b[A\n",
            "Iteration:  81% 2080/2561 [26:11<06:02,  1.33it/s]\u001b[A\n",
            "Iteration:  81% 2081/2561 [26:12<06:00,  1.33it/s]\u001b[A\n",
            "Iteration:  81% 2082/2561 [26:13<05:57,  1.34it/s]\u001b[A\n",
            "Iteration:  81% 2083/2561 [26:13<05:57,  1.34it/s]\u001b[A\n",
            "Iteration:  81% 2084/2561 [26:14<05:55,  1.34it/s]\u001b[A\n",
            "Iteration:  81% 2085/2561 [26:15<05:55,  1.34it/s]\u001b[A\n",
            "Iteration:  81% 2086/2561 [26:16<05:54,  1.34it/s]\u001b[A\n",
            "Iteration:  81% 2087/2561 [26:16<05:55,  1.33it/s]\u001b[A\n",
            "Iteration:  82% 2088/2561 [26:17<05:54,  1.34it/s]\u001b[A\n",
            "Iteration:  82% 2089/2561 [26:18<05:52,  1.34it/s]\u001b[A\n",
            "Iteration:  82% 2090/2561 [26:19<05:51,  1.34it/s]\u001b[A\n",
            "Iteration:  82% 2091/2561 [26:19<05:49,  1.34it/s]\u001b[A\n",
            "Iteration:  82% 2092/2561 [26:20<05:49,  1.34it/s]\u001b[A\n",
            "Iteration:  82% 2093/2561 [26:21<05:48,  1.34it/s]\u001b[A\n",
            "Iteration:  82% 2094/2561 [26:22<05:49,  1.34it/s]\u001b[A\n",
            "Iteration:  82% 2095/2561 [26:22<05:47,  1.34it/s]\u001b[A\n",
            "Iteration:  82% 2096/2561 [26:23<05:47,  1.34it/s]\u001b[A\n",
            "Iteration:  82% 2097/2561 [26:24<05:46,  1.34it/s]\u001b[A\n",
            "Iteration:  82% 2098/2561 [26:25<05:43,  1.35it/s]\u001b[A\n",
            "Iteration:  82% 2099/2561 [26:25<05:44,  1.34it/s]\u001b[A\n",
            "Iteration:  82% 2100/2561 [26:26<05:42,  1.35it/s]\u001b[A\n",
            "Iteration:  82% 2101/2561 [26:27<05:43,  1.34it/s]\u001b[A\n",
            "Iteration:  82% 2102/2561 [26:28<05:42,  1.34it/s]\u001b[A\n",
            "Iteration:  82% 2103/2561 [26:28<05:43,  1.33it/s]\u001b[A\n",
            "Iteration:  82% 2104/2561 [26:29<05:42,  1.33it/s]\u001b[A\n",
            "Iteration:  82% 2105/2561 [26:30<05:42,  1.33it/s]\u001b[A\n",
            "Iteration:  82% 2106/2561 [26:31<05:41,  1.33it/s]\u001b[A\n",
            "Iteration:  82% 2107/2561 [26:31<05:38,  1.34it/s]\u001b[A\n",
            "Iteration:  82% 2108/2561 [26:32<05:38,  1.34it/s]\u001b[A\n",
            "Iteration:  82% 2109/2561 [26:33<05:36,  1.34it/s]\u001b[A\n",
            "Iteration:  82% 2110/2561 [26:34<05:36,  1.34it/s]\u001b[A\n",
            "Iteration:  82% 2111/2561 [26:34<05:35,  1.34it/s]\u001b[A\n",
            "Iteration:  82% 2112/2561 [26:35<05:35,  1.34it/s]\u001b[A\n",
            "Iteration:  83% 2113/2561 [26:36<05:35,  1.34it/s]\u001b[A\n",
            "Iteration:  83% 2114/2561 [26:37<05:35,  1.33it/s]\u001b[A\n",
            "Iteration:  83% 2115/2561 [26:37<05:34,  1.33it/s]\u001b[A\n",
            "Iteration:  83% 2116/2561 [26:38<05:34,  1.33it/s]\u001b[A\n",
            "Iteration:  83% 2117/2561 [26:39<05:33,  1.33it/s]\u001b[A\n",
            "Iteration:  83% 2118/2561 [26:40<05:30,  1.34it/s]\u001b[A\n",
            "Iteration:  83% 2119/2561 [26:40<05:28,  1.34it/s]\u001b[A\n",
            "Iteration:  83% 2120/2561 [26:41<05:28,  1.34it/s]\u001b[A\n",
            "Iteration:  83% 2121/2561 [26:42<05:29,  1.34it/s]\u001b[A\n",
            "Iteration:  83% 2122/2561 [26:43<05:27,  1.34it/s]\u001b[A\n",
            "Iteration:  83% 2123/2561 [26:43<05:28,  1.33it/s]\u001b[A\n",
            "Iteration:  83% 2124/2561 [26:44<05:27,  1.34it/s]\u001b[A\n",
            "Iteration:  83% 2125/2561 [26:45<05:27,  1.33it/s]\u001b[A\n",
            "Iteration:  83% 2126/2561 [26:46<05:26,  1.33it/s]\u001b[A\n",
            "Iteration:  83% 2127/2561 [26:46<05:26,  1.33it/s]\u001b[A\n",
            "Iteration:  83% 2128/2561 [26:47<05:25,  1.33it/s]\u001b[A\n",
            "Iteration:  83% 2129/2561 [26:48<05:23,  1.34it/s]\u001b[A\n",
            "Iteration:  83% 2130/2561 [26:49<05:22,  1.34it/s]\u001b[A\n",
            "Iteration:  83% 2131/2561 [26:49<05:20,  1.34it/s]\u001b[A\n",
            "Iteration:  83% 2132/2561 [26:50<05:21,  1.33it/s]\u001b[A\n",
            "Iteration:  83% 2133/2561 [26:51<05:19,  1.34it/s]\u001b[A\n",
            "Iteration:  83% 2134/2561 [26:52<05:20,  1.33it/s]\u001b[A\n",
            "Iteration:  83% 2135/2561 [26:52<05:18,  1.34it/s]\u001b[A\n",
            "Iteration:  83% 2136/2561 [26:53<05:19,  1.33it/s]\u001b[A\n",
            "Iteration:  83% 2137/2561 [26:54<05:17,  1.33it/s]\u001b[A\n",
            "Iteration:  83% 2138/2561 [26:55<05:18,  1.33it/s]\u001b[A\n",
            "Iteration:  84% 2139/2561 [26:55<05:16,  1.33it/s]\u001b[A\n",
            "Iteration:  84% 2140/2561 [26:56<05:14,  1.34it/s]\u001b[A\n",
            "Iteration:  84% 2141/2561 [26:57<05:14,  1.33it/s]\u001b[A\n",
            "Iteration:  84% 2142/2561 [26:58<05:12,  1.34it/s]\u001b[A\n",
            "Iteration:  84% 2143/2561 [26:58<05:12,  1.34it/s]\u001b[A\n",
            "Iteration:  84% 2144/2561 [26:59<05:11,  1.34it/s]\u001b[A\n",
            "Iteration:  84% 2145/2561 [27:00<05:12,  1.33it/s]\u001b[A\n",
            "Iteration:  84% 2146/2561 [27:01<05:10,  1.34it/s]\u001b[A\n",
            "Iteration:  84% 2147/2561 [27:01<05:10,  1.33it/s]\u001b[A\n",
            "Iteration:  84% 2148/2561 [27:02<05:09,  1.33it/s]\u001b[A\n",
            "Iteration:  84% 2149/2561 [27:03<05:07,  1.34it/s]\u001b[A\n",
            "Iteration:  84% 2150/2561 [27:04<05:07,  1.34it/s]\u001b[A\n",
            "Iteration:  84% 2151/2561 [27:04<05:06,  1.34it/s]\u001b[A\n",
            "Iteration:  84% 2152/2561 [27:05<05:06,  1.33it/s]\u001b[A\n",
            "Iteration:  84% 2153/2561 [27:06<05:05,  1.34it/s]\u001b[A\n",
            "Iteration:  84% 2154/2561 [27:07<05:05,  1.33it/s]\u001b[A\n",
            "Iteration:  84% 2155/2561 [27:07<05:04,  1.33it/s]\u001b[A\n",
            "Iteration:  84% 2156/2561 [27:08<05:04,  1.33it/s]\u001b[A\n",
            "Iteration:  84% 2157/2561 [27:09<05:03,  1.33it/s]\u001b[A\n",
            "Iteration:  84% 2158/2561 [27:10<05:00,  1.34it/s]\u001b[A\n",
            "Iteration:  84% 2159/2561 [27:10<05:00,  1.34it/s]\u001b[A\n",
            "Iteration:  84% 2160/2561 [27:11<04:58,  1.34it/s]\u001b[A\n",
            "Iteration:  84% 2161/2561 [27:12<04:58,  1.34it/s]\u001b[A\n",
            "Iteration:  84% 2162/2561 [27:13<04:57,  1.34it/s]\u001b[A\n",
            "Iteration:  84% 2163/2561 [27:13<04:57,  1.34it/s]\u001b[A\n",
            "Iteration:  84% 2164/2561 [27:14<04:57,  1.34it/s]\u001b[A\n",
            "Iteration:  85% 2165/2561 [27:15<04:55,  1.34it/s]\u001b[A\n",
            "Iteration:  85% 2166/2561 [27:16<04:55,  1.34it/s]\u001b[A\n",
            "Iteration:  85% 2167/2561 [27:16<04:53,  1.34it/s]\u001b[A\n",
            "Iteration:  85% 2168/2561 [27:17<04:53,  1.34it/s]\u001b[A\n",
            "Iteration:  85% 2169/2561 [27:18<04:52,  1.34it/s]\u001b[A\n",
            "Iteration:  85% 2170/2561 [27:18<04:51,  1.34it/s]\u001b[A\n",
            "Iteration:  85% 2171/2561 [27:19<04:50,  1.34it/s]\u001b[A\n",
            "Iteration:  85% 2172/2561 [27:20<04:51,  1.34it/s]\u001b[A\n",
            "Iteration:  85% 2173/2561 [27:21<04:49,  1.34it/s]\u001b[A\n",
            "Iteration:  85% 2174/2561 [27:21<04:49,  1.34it/s]\u001b[A\n",
            "Iteration:  85% 2175/2561 [27:22<04:48,  1.34it/s]\u001b[A\n",
            "Iteration:  85% 2176/2561 [27:23<04:47,  1.34it/s]\u001b[A\n",
            "Iteration:  85% 2177/2561 [27:24<04:46,  1.34it/s]\u001b[A\n",
            "Iteration:  85% 2178/2561 [27:24<04:44,  1.35it/s]\u001b[A\n",
            "Iteration:  85% 2179/2561 [27:25<04:45,  1.34it/s]\u001b[A\n",
            "Iteration:  85% 2180/2561 [27:26<04:44,  1.34it/s]\u001b[A\n",
            "Iteration:  85% 2181/2561 [27:27<04:44,  1.34it/s]\u001b[A\n",
            "Iteration:  85% 2182/2561 [27:27<04:43,  1.34it/s]\u001b[A\n",
            "Iteration:  85% 2183/2561 [27:28<04:42,  1.34it/s]\u001b[A\n",
            "Iteration:  85% 2184/2561 [27:29<04:42,  1.34it/s]\u001b[A\n",
            "Iteration:  85% 2185/2561 [27:30<04:40,  1.34it/s]\u001b[A\n",
            "Iteration:  85% 2186/2561 [27:30<04:40,  1.34it/s]\u001b[A\n",
            "Iteration:  85% 2187/2561 [27:31<04:39,  1.34it/s]\u001b[A\n",
            "Iteration:  85% 2188/2561 [27:32<04:39,  1.33it/s]\u001b[A\n",
            "Iteration:  85% 2189/2561 [27:33<04:38,  1.34it/s]\u001b[A\n",
            "Iteration:  86% 2190/2561 [27:33<04:38,  1.33it/s]\u001b[A\n",
            "Iteration:  86% 2191/2561 [27:34<04:37,  1.33it/s]\u001b[A\n",
            "Iteration:  86% 2192/2561 [27:35<04:35,  1.34it/s]\u001b[A\n",
            "Iteration:  86% 2193/2561 [27:36<04:35,  1.34it/s]\u001b[A\n",
            "Iteration:  86% 2194/2561 [27:36<04:33,  1.34it/s]\u001b[A\n",
            "Iteration:  86% 2195/2561 [27:37<04:33,  1.34it/s]\u001b[A\n",
            "Iteration:  86% 2196/2561 [27:38<04:31,  1.34it/s]\u001b[A\n",
            "Iteration:  86% 2197/2561 [27:39<04:31,  1.34it/s]\u001b[A\n",
            "Iteration:  86% 2198/2561 [27:39<04:31,  1.34it/s]\u001b[A\n",
            "Iteration:  86% 2199/2561 [27:40<04:31,  1.33it/s]\u001b[A\n",
            "Iteration:  86% 2200/2561 [27:41<04:30,  1.33it/s]\u001b[A\n",
            "Iteration:  86% 2201/2561 [27:42<04:28,  1.34it/s]\u001b[A\n",
            "Iteration:  86% 2202/2561 [27:42<04:28,  1.34it/s]\u001b[A\n",
            "Iteration:  86% 2203/2561 [27:43<04:26,  1.34it/s]\u001b[A\n",
            "Iteration:  86% 2204/2561 [27:44<04:26,  1.34it/s]\u001b[A\n",
            "Iteration:  86% 2205/2561 [27:45<04:25,  1.34it/s]\u001b[A\n",
            "Iteration:  86% 2206/2561 [27:45<04:25,  1.34it/s]\u001b[A\n",
            "Iteration:  86% 2207/2561 [27:46<04:24,  1.34it/s]\u001b[A\n",
            "Iteration:  86% 2208/2561 [27:47<04:24,  1.33it/s]\u001b[A\n",
            "Iteration:  86% 2209/2561 [27:48<04:23,  1.33it/s]\u001b[A\n",
            "Iteration:  86% 2210/2561 [27:48<04:21,  1.34it/s]\u001b[A\n",
            "Iteration:  86% 2211/2561 [27:49<04:21,  1.34it/s]\u001b[A\n",
            "Iteration:  86% 2212/2561 [27:50<04:19,  1.34it/s]\u001b[A\n",
            "Iteration:  86% 2213/2561 [27:51<04:19,  1.34it/s]\u001b[A\n",
            "Iteration:  86% 2214/2561 [27:51<04:18,  1.34it/s]\u001b[A\n",
            "Iteration:  86% 2215/2561 [27:52<04:19,  1.34it/s]\u001b[A\n",
            "Iteration:  87% 2216/2561 [27:53<04:18,  1.34it/s]\u001b[A\n",
            "Iteration:  87% 2217/2561 [27:54<04:18,  1.33it/s]\u001b[A\n",
            "Iteration:  87% 2218/2561 [27:54<04:17,  1.33it/s]\u001b[A\n",
            "Iteration:  87% 2219/2561 [27:55<04:15,  1.34it/s]\u001b[A\n",
            "Iteration:  87% 2220/2561 [27:56<04:14,  1.34it/s]\u001b[A\n",
            "Iteration:  87% 2221/2561 [27:57<04:13,  1.34it/s]\u001b[A\n",
            "Iteration:  87% 2222/2561 [27:57<04:12,  1.34it/s]\u001b[A\n",
            "Iteration:  87% 2223/2561 [27:58<04:11,  1.34it/s]\u001b[A\n",
            "Iteration:  87% 2224/2561 [27:59<04:11,  1.34it/s]\u001b[A\n",
            "Iteration:  87% 2225/2561 [28:00<04:11,  1.34it/s]\u001b[A\n",
            "Iteration:  87% 2226/2561 [28:00<04:11,  1.33it/s]\u001b[A\n",
            "Iteration:  87% 2227/2561 [28:01<04:10,  1.33it/s]\u001b[A\n",
            "Iteration:  87% 2228/2561 [28:02<04:08,  1.34it/s]\u001b[A\n",
            "Iteration:  87% 2229/2561 [28:03<04:08,  1.34it/s]\u001b[A\n",
            "Iteration:  87% 2230/2561 [28:03<04:06,  1.34it/s]\u001b[A\n",
            "Iteration:  87% 2231/2561 [28:04<04:06,  1.34it/s]\u001b[A\n",
            "Iteration:  87% 2232/2561 [28:05<04:05,  1.34it/s]\u001b[A\n",
            "Iteration:  87% 2233/2561 [28:06<04:06,  1.33it/s]\u001b[A\n",
            "Iteration:  87% 2234/2561 [28:06<04:05,  1.33it/s]\u001b[A\n",
            "Iteration:  87% 2235/2561 [28:07<04:03,  1.34it/s]\u001b[A\n",
            "Iteration:  87% 2236/2561 [28:08<04:03,  1.34it/s]\u001b[A\n",
            "Iteration:  87% 2237/2561 [28:09<04:01,  1.34it/s]\u001b[A\n",
            "Iteration:  87% 2238/2561 [28:09<04:01,  1.34it/s]\u001b[A\n",
            "Iteration:  87% 2239/2561 [28:10<03:59,  1.34it/s]\u001b[A\n",
            "Iteration:  87% 2240/2561 [28:11<03:59,  1.34it/s]\u001b[A\n",
            "Iteration:  88% 2241/2561 [28:12<03:59,  1.34it/s]\u001b[A\n",
            "Iteration:  88% 2242/2561 [28:12<03:59,  1.33it/s]\u001b[A\n",
            "Iteration:  88% 2243/2561 [28:13<03:58,  1.33it/s]\u001b[A\n",
            "Iteration:  88% 2244/2561 [28:14<03:56,  1.34it/s]\u001b[A\n",
            "Iteration:  88% 2245/2561 [28:15<03:56,  1.34it/s]\u001b[A\n",
            "Iteration:  88% 2246/2561 [28:15<03:54,  1.34it/s]\u001b[A\n",
            "Iteration:  88% 2247/2561 [28:16<03:55,  1.34it/s]\u001b[A\n",
            "Iteration:  88% 2248/2561 [28:17<03:53,  1.34it/s]\u001b[A\n",
            "Iteration:  88% 2249/2561 [28:18<03:54,  1.33it/s]\u001b[A\n",
            "Iteration:  88% 2250/2561 [28:18<03:53,  1.33it/s]\u001b[A\n",
            "Iteration:  88% 2251/2561 [28:19<03:53,  1.33it/s]\u001b[A\n",
            "Iteration:  88% 2252/2561 [28:20<03:51,  1.33it/s]\u001b[A\n",
            "Iteration:  88% 2253/2561 [28:21<03:51,  1.33it/s]\u001b[A\n",
            "Iteration:  88% 2254/2561 [28:21<03:50,  1.33it/s]\u001b[A\n",
            "Iteration:  88% 2255/2561 [28:22<03:48,  1.34it/s]\u001b[A\n",
            "Iteration:  88% 2256/2561 [28:23<03:48,  1.33it/s]\u001b[A\n",
            "Iteration:  88% 2257/2561 [28:24<03:47,  1.34it/s]\u001b[A\n",
            "Iteration:  88% 2258/2561 [28:24<03:47,  1.33it/s]\u001b[A\n",
            "Iteration:  88% 2259/2561 [28:25<03:45,  1.34it/s]\u001b[A\n",
            "Iteration:  88% 2260/2561 [28:26<03:45,  1.33it/s]\u001b[A\n",
            "Iteration:  88% 2261/2561 [28:27<03:44,  1.34it/s]\u001b[A\n",
            "Iteration:  88% 2262/2561 [28:27<03:44,  1.33it/s]\u001b[A\n",
            "Iteration:  88% 2263/2561 [28:28<03:43,  1.34it/s]\u001b[A\n",
            "Iteration:  88% 2264/2561 [28:29<03:43,  1.33it/s]\u001b[A\n",
            "Iteration:  88% 2265/2561 [28:30<03:42,  1.33it/s]\u001b[A\n",
            "Iteration:  88% 2266/2561 [28:30<03:42,  1.33it/s]\u001b[A\n",
            "Iteration:  89% 2267/2561 [28:31<03:41,  1.33it/s]\u001b[A\n",
            "Iteration:  89% 2268/2561 [28:32<03:39,  1.34it/s]\u001b[A\n",
            "Iteration:  89% 2269/2561 [28:33<03:38,  1.34it/s]\u001b[A\n",
            "Iteration:  89% 2270/2561 [28:33<03:37,  1.34it/s]\u001b[A\n",
            "Iteration:  89% 2271/2561 [28:34<03:37,  1.34it/s]\u001b[A\n",
            "Iteration:  89% 2272/2561 [28:35<03:36,  1.34it/s]\u001b[A\n",
            "Iteration:  89% 2273/2561 [28:36<03:36,  1.33it/s]\u001b[A\n",
            "Iteration:  89% 2274/2561 [28:36<03:34,  1.34it/s]\u001b[A\n",
            "Iteration:  89% 2275/2561 [28:37<03:34,  1.33it/s]\u001b[A\n",
            "Iteration:  89% 2276/2561 [28:38<03:33,  1.33it/s]\u001b[A\n",
            "Iteration:  89% 2277/2561 [28:39<03:34,  1.33it/s]\u001b[A\n",
            "Iteration:  89% 2278/2561 [28:39<03:33,  1.33it/s]\u001b[A\n",
            "Iteration:  89% 2279/2561 [28:40<03:31,  1.34it/s]\u001b[A\n",
            "Iteration:  89% 2280/2561 [28:41<03:30,  1.33it/s]\u001b[A\n",
            "Iteration:  89% 2281/2561 [28:42<03:28,  1.34it/s]\u001b[A\n",
            "Iteration:  89% 2282/2561 [28:42<03:28,  1.34it/s]\u001b[A\n",
            "Iteration:  89% 2283/2561 [28:43<03:27,  1.34it/s]\u001b[A\n",
            "Iteration:  89% 2284/2561 [28:44<03:27,  1.34it/s]\u001b[A\n",
            "Iteration:  89% 2285/2561 [28:45<03:26,  1.34it/s]\u001b[A\n",
            "Iteration:  89% 2286/2561 [28:45<03:26,  1.33it/s]\u001b[A\n",
            "Iteration:  89% 2287/2561 [28:46<03:25,  1.33it/s]\u001b[A\n",
            "Iteration:  89% 2288/2561 [28:47<03:25,  1.33it/s]\u001b[A\n",
            "Iteration:  89% 2289/2561 [28:48<03:24,  1.33it/s]\u001b[A\n",
            "Iteration:  89% 2290/2561 [28:48<03:22,  1.34it/s]\u001b[A\n",
            "Iteration:  89% 2291/2561 [28:49<03:21,  1.34it/s]\u001b[A\n",
            "Iteration:  89% 2292/2561 [28:50<03:20,  1.34it/s]\u001b[A\n",
            "Iteration:  90% 2293/2561 [28:51<03:20,  1.34it/s]\u001b[A\n",
            "Iteration:  90% 2294/2561 [28:51<03:19,  1.34it/s]\u001b[A\n",
            "Iteration:  90% 2295/2561 [28:52<03:19,  1.33it/s]\u001b[A\n",
            "Iteration:  90% 2296/2561 [28:53<03:18,  1.34it/s]\u001b[A\n",
            "Iteration:  90% 2297/2561 [28:54<03:18,  1.33it/s]\u001b[A\n",
            "Iteration:  90% 2298/2561 [28:54<03:17,  1.33it/s]\u001b[A\n",
            "Iteration:  90% 2299/2561 [28:55<03:15,  1.34it/s]\u001b[A\n",
            "Iteration:  90% 2300/2561 [28:56<03:15,  1.34it/s]\u001b[A\n",
            "Iteration:  90% 2301/2561 [28:56<03:13,  1.34it/s]\u001b[A\n",
            "Iteration:  90% 2302/2561 [28:57<03:13,  1.34it/s]\u001b[A\n",
            "Iteration:  90% 2303/2561 [28:58<03:12,  1.34it/s]\u001b[A\n",
            "Iteration:  90% 2304/2561 [28:59<03:12,  1.33it/s]\u001b[A\n",
            "Iteration:  90% 2305/2561 [28:59<03:11,  1.33it/s]\u001b[A\n",
            "Iteration:  90% 2306/2561 [29:00<03:11,  1.33it/s]\u001b[A\n",
            "Iteration:  90% 2307/2561 [29:01<03:10,  1.33it/s]\u001b[A\n",
            "Iteration:  90% 2308/2561 [29:02<03:10,  1.33it/s]\u001b[A\n",
            "Iteration:  90% 2309/2561 [29:03<03:09,  1.33it/s]\u001b[A\n",
            "Iteration:  90% 2310/2561 [29:03<03:09,  1.33it/s]\u001b[A\n",
            "Iteration:  90% 2311/2561 [29:04<03:08,  1.33it/s]\u001b[A\n",
            "Iteration:  90% 2312/2561 [29:05<03:06,  1.34it/s]\u001b[A\n",
            "Iteration:  90% 2313/2561 [29:06<03:05,  1.34it/s]\u001b[A\n",
            "Iteration:  90% 2314/2561 [29:06<03:04,  1.34it/s]\u001b[A\n",
            "Iteration:  90% 2315/2561 [29:07<03:04,  1.34it/s]\u001b[A\n",
            "Iteration:  90% 2316/2561 [29:08<03:03,  1.34it/s]\u001b[A\n",
            "Iteration:  90% 2317/2561 [29:08<03:03,  1.33it/s]\u001b[A\n",
            "Iteration:  91% 2318/2561 [29:09<03:01,  1.34it/s]\u001b[A\n",
            "Iteration:  91% 2319/2561 [29:10<03:01,  1.33it/s]\u001b[A\n",
            "Iteration:  91% 2320/2561 [29:11<03:00,  1.33it/s]\u001b[A\n",
            "Iteration:  91% 2321/2561 [29:12<03:00,  1.33it/s]\u001b[A\n",
            "Iteration:  91% 2322/2561 [29:12<02:59,  1.33it/s]\u001b[A\n",
            "Iteration:  91% 2323/2561 [29:13<02:57,  1.34it/s]\u001b[A\n",
            "Iteration:  91% 2324/2561 [29:14<02:56,  1.34it/s]\u001b[A\n",
            "Iteration:  91% 2325/2561 [29:14<02:56,  1.34it/s]\u001b[A\n",
            "Iteration:  91% 2326/2561 [29:15<02:56,  1.33it/s]\u001b[A\n",
            "Iteration:  91% 2327/2561 [29:16<02:54,  1.34it/s]\u001b[A\n",
            "Iteration:  91% 2328/2561 [29:17<02:54,  1.33it/s]\u001b[A\n",
            "Iteration:  91% 2329/2561 [29:17<02:53,  1.34it/s]\u001b[A\n",
            "Iteration:  91% 2330/2561 [29:18<02:53,  1.33it/s]\u001b[A\n",
            "Iteration:  91% 2331/2561 [29:19<02:52,  1.33it/s]\u001b[A\n",
            "Iteration:  91% 2332/2561 [29:20<02:52,  1.33it/s]\u001b[A\n",
            "Iteration:  91% 2333/2561 [29:20<02:51,  1.33it/s]\u001b[A\n",
            "Iteration:  91% 2334/2561 [29:21<02:49,  1.34it/s]\u001b[A\n",
            "Iteration:  91% 2335/2561 [29:22<02:49,  1.34it/s]\u001b[A\n",
            "Iteration:  91% 2336/2561 [29:23<02:48,  1.34it/s]\u001b[A\n",
            "Iteration:  91% 2337/2561 [29:23<02:48,  1.33it/s]\u001b[A\n",
            "Iteration:  91% 2338/2561 [29:24<02:46,  1.34it/s]\u001b[A\n",
            "Iteration:  91% 2339/2561 [29:25<02:46,  1.33it/s]\u001b[A\n",
            "Iteration:  91% 2340/2561 [29:26<02:45,  1.33it/s]\u001b[A\n",
            "Iteration:  91% 2341/2561 [29:26<02:45,  1.33it/s]\u001b[A\n",
            "Iteration:  91% 2342/2561 [29:27<02:44,  1.33it/s]\u001b[A\n",
            "Iteration:  91% 2343/2561 [29:28<02:44,  1.33it/s]\u001b[A\n",
            "Iteration:  92% 2344/2561 [29:29<02:43,  1.33it/s]\u001b[A\n",
            "Iteration:  92% 2345/2561 [29:30<02:43,  1.32it/s]\u001b[A\n",
            "Iteration:  92% 2346/2561 [29:30<02:41,  1.33it/s]\u001b[A\n",
            "Iteration:  92% 2347/2561 [29:31<02:40,  1.34it/s]\u001b[A\n",
            "Iteration:  92% 2348/2561 [29:32<02:39,  1.33it/s]\u001b[A\n",
            "Iteration:  92% 2349/2561 [29:32<02:38,  1.34it/s]\u001b[A\n",
            "Iteration:  92% 2350/2561 [29:33<02:38,  1.33it/s]\u001b[A\n",
            "Iteration:  92% 2351/2561 [29:34<02:37,  1.34it/s]\u001b[A\n",
            "Iteration:  92% 2352/2561 [29:35<02:36,  1.33it/s]\u001b[A\n",
            "Iteration:  92% 2353/2561 [29:35<02:35,  1.33it/s]\u001b[A\n",
            "Iteration:  92% 2354/2561 [29:36<02:35,  1.33it/s]\u001b[A\n",
            "Iteration:  92% 2355/2561 [29:37<02:34,  1.33it/s]\u001b[A\n",
            "Iteration:  92% 2356/2561 [29:38<02:34,  1.33it/s]\u001b[A\n",
            "Iteration:  92% 2357/2561 [29:38<02:33,  1.33it/s]\u001b[A\n",
            "Iteration:  92% 2358/2561 [29:39<02:32,  1.33it/s]\u001b[A\n",
            "Iteration:  92% 2359/2561 [29:40<02:31,  1.33it/s]\u001b[A\n",
            "Iteration:  92% 2360/2561 [29:41<02:30,  1.34it/s]\u001b[A\n",
            "Iteration:  92% 2361/2561 [29:41<02:29,  1.34it/s]\u001b[A\n",
            "Iteration:  92% 2362/2561 [29:42<02:28,  1.34it/s]\u001b[A\n",
            "Iteration:  92% 2363/2561 [29:43<02:28,  1.34it/s]\u001b[A\n",
            "Iteration:  92% 2364/2561 [29:44<02:27,  1.34it/s]\u001b[A\n",
            "Iteration:  92% 2365/2561 [29:44<02:27,  1.33it/s]\u001b[A\n",
            "Iteration:  92% 2366/2561 [29:45<02:26,  1.33it/s]\u001b[A\n",
            "Iteration:  92% 2367/2561 [29:46<02:25,  1.33it/s]\u001b[A\n",
            "Iteration:  92% 2368/2561 [29:47<02:24,  1.33it/s]\u001b[A\n",
            "Iteration:  93% 2369/2561 [29:47<02:24,  1.33it/s]\u001b[A\n",
            "Iteration:  93% 2370/2561 [29:48<02:23,  1.33it/s]\u001b[A\n",
            "Iteration:  93% 2371/2561 [29:49<02:21,  1.34it/s]\u001b[A\n",
            "Iteration:  93% 2372/2561 [29:50<02:21,  1.34it/s]\u001b[A\n",
            "Iteration:  93% 2373/2561 [29:50<02:20,  1.34it/s]\u001b[A\n",
            "Iteration:  93% 2374/2561 [29:51<02:19,  1.34it/s]\u001b[A\n",
            "Iteration:  93% 2375/2561 [29:52<02:19,  1.34it/s]\u001b[A\n",
            "Iteration:  93% 2376/2561 [29:53<02:18,  1.33it/s]\u001b[A\n",
            "Iteration:  93% 2377/2561 [29:53<02:17,  1.33it/s]\u001b[A\n",
            "Iteration:  93% 2378/2561 [29:54<02:17,  1.33it/s]\u001b[A\n",
            "Iteration:  93% 2379/2561 [29:55<02:16,  1.33it/s]\u001b[A\n",
            "Iteration:  93% 2380/2561 [29:56<02:16,  1.33it/s]\u001b[A\n",
            "Iteration:  93% 2381/2561 [29:56<02:15,  1.33it/s]\u001b[A\n",
            "Iteration:  93% 2382/2561 [29:57<02:13,  1.34it/s]\u001b[A\n",
            "Iteration:  93% 2383/2561 [29:58<02:13,  1.34it/s]\u001b[A\n",
            "Iteration:  93% 2384/2561 [29:59<02:12,  1.34it/s]\u001b[A\n",
            "Iteration:  93% 2385/2561 [29:59<02:11,  1.34it/s]\u001b[A\n",
            "Iteration:  93% 2386/2561 [30:00<02:10,  1.34it/s]\u001b[A\n",
            "Iteration:  93% 2387/2561 [30:01<02:10,  1.33it/s]\u001b[A\n",
            "Iteration:  93% 2388/2561 [30:02<02:09,  1.34it/s]\u001b[A\n",
            "Iteration:  93% 2389/2561 [30:02<02:09,  1.33it/s]\u001b[A\n",
            "Iteration:  93% 2390/2561 [30:03<02:08,  1.33it/s]\u001b[A\n",
            "Iteration:  93% 2391/2561 [30:04<02:07,  1.33it/s]\u001b[A\n",
            "Iteration:  93% 2392/2561 [30:05<02:07,  1.33it/s]\u001b[A\n",
            "Iteration:  93% 2393/2561 [30:05<02:06,  1.33it/s]\u001b[A\n",
            "Iteration:  93% 2394/2561 [30:06<02:05,  1.33it/s]\u001b[A\n",
            "Iteration:  94% 2395/2561 [30:07<02:04,  1.34it/s]\u001b[A\n",
            "Iteration:  94% 2396/2561 [30:08<02:03,  1.33it/s]\u001b[A\n",
            "Iteration:  94% 2397/2561 [30:08<02:02,  1.34it/s]\u001b[A\n",
            "Iteration:  94% 2398/2561 [30:09<02:02,  1.33it/s]\u001b[A\n",
            "Iteration:  94% 2399/2561 [30:10<02:01,  1.34it/s]\u001b[A\n",
            "Iteration:  94% 2400/2561 [30:11<02:00,  1.33it/s]\u001b[A\n",
            "Iteration:  94% 2401/2561 [30:11<01:59,  1.34it/s]\u001b[A\n",
            "Iteration:  94% 2402/2561 [30:12<01:59,  1.33it/s]\u001b[A\n",
            "Iteration:  94% 2403/2561 [30:13<01:58,  1.34it/s]\u001b[A\n",
            "Iteration:  94% 2404/2561 [30:14<01:58,  1.33it/s]\u001b[A\n",
            "Iteration:  94% 2405/2561 [30:14<01:57,  1.33it/s]\u001b[A\n",
            "Iteration:  94% 2406/2561 [30:15<01:55,  1.34it/s]\u001b[A\n",
            "Iteration:  94% 2407/2561 [30:16<01:55,  1.34it/s]\u001b[A\n",
            "Iteration:  94% 2408/2561 [30:17<01:54,  1.34it/s]\u001b[A\n",
            "Iteration:  94% 2409/2561 [30:17<01:53,  1.34it/s]\u001b[A\n",
            "Iteration:  94% 2410/2561 [30:18<01:52,  1.34it/s]\u001b[A\n",
            "Iteration:  94% 2411/2561 [30:19<01:52,  1.33it/s]\u001b[A\n",
            "Iteration:  94% 2412/2561 [30:20<01:51,  1.33it/s]\u001b[A\n",
            "Iteration:  94% 2413/2561 [30:20<01:51,  1.33it/s]\u001b[A\n",
            "Iteration:  94% 2414/2561 [30:21<01:50,  1.33it/s]\u001b[A\n",
            "Iteration:  94% 2415/2561 [30:22<01:49,  1.33it/s]\u001b[A\n",
            "Iteration:  94% 2416/2561 [30:23<01:49,  1.33it/s]\u001b[A\n",
            "Iteration:  94% 2417/2561 [30:23<01:47,  1.34it/s]\u001b[A\n",
            "Iteration:  94% 2418/2561 [30:24<01:47,  1.33it/s]\u001b[A\n",
            "Iteration:  94% 2419/2561 [30:25<01:46,  1.34it/s]\u001b[A\n",
            "Iteration:  94% 2420/2561 [30:26<01:45,  1.33it/s]\u001b[A\n",
            "Iteration:  95% 2421/2561 [30:26<01:44,  1.34it/s]\u001b[A\n",
            "Iteration:  95% 2422/2561 [30:27<01:44,  1.33it/s]\u001b[A\n",
            "Iteration:  95% 2423/2561 [30:28<01:43,  1.33it/s]\u001b[A\n",
            "Iteration:  95% 2424/2561 [30:29<01:43,  1.33it/s]\u001b[A\n",
            "Iteration:  95% 2425/2561 [30:29<01:42,  1.33it/s]\u001b[A\n",
            "Iteration:  95% 2426/2561 [30:30<01:41,  1.33it/s]\u001b[A\n",
            "Iteration:  95% 2427/2561 [30:31<01:40,  1.33it/s]\u001b[A\n",
            "Iteration:  95% 2428/2561 [30:32<01:39,  1.34it/s]\u001b[A\n",
            "Iteration:  95% 2429/2561 [30:32<01:38,  1.33it/s]\u001b[A\n",
            "Iteration:  95% 2430/2561 [30:33<01:37,  1.34it/s]\u001b[A\n",
            "Iteration:  95% 2431/2561 [30:34<01:37,  1.33it/s]\u001b[A\n",
            "Iteration:  95% 2432/2561 [30:35<01:36,  1.34it/s]\u001b[A\n",
            "Iteration:  95% 2433/2561 [30:35<01:36,  1.33it/s]\u001b[A\n",
            "Iteration:  95% 2434/2561 [30:36<01:35,  1.34it/s]\u001b[A\n",
            "Iteration:  95% 2435/2561 [30:37<01:34,  1.33it/s]\u001b[A\n",
            "Iteration:  95% 2436/2561 [30:38<01:33,  1.33it/s]\u001b[A\n",
            "Iteration:  95% 2437/2561 [30:38<01:32,  1.34it/s]\u001b[A\n",
            "Iteration:  95% 2438/2561 [30:39<01:32,  1.34it/s]\u001b[A\n",
            "Iteration:  95% 2439/2561 [30:40<01:30,  1.34it/s]\u001b[A\n",
            "Iteration:  95% 2440/2561 [30:41<01:30,  1.34it/s]\u001b[A\n",
            "Iteration:  95% 2441/2561 [30:41<01:29,  1.34it/s]\u001b[A\n",
            "Iteration:  95% 2442/2561 [30:42<01:28,  1.34it/s]\u001b[A\n",
            "Iteration:  95% 2443/2561 [30:43<01:27,  1.34it/s]\u001b[A\n",
            "Iteration:  95% 2444/2561 [30:44<01:27,  1.33it/s]\u001b[A\n",
            "Iteration:  95% 2445/2561 [30:44<01:26,  1.33it/s]\u001b[A\n",
            "Iteration:  96% 2446/2561 [30:45<01:26,  1.33it/s]\u001b[A\n",
            "Iteration:  96% 2447/2561 [30:46<01:25,  1.33it/s]\u001b[A\n",
            "Iteration:  96% 2448/2561 [30:47<01:24,  1.34it/s]\u001b[A\n",
            "Iteration:  96% 2449/2561 [30:47<01:23,  1.34it/s]\u001b[A\n",
            "Iteration:  96% 2450/2561 [30:48<01:22,  1.34it/s]\u001b[A\n",
            "Iteration:  96% 2451/2561 [30:49<01:22,  1.34it/s]\u001b[A\n",
            "Iteration:  96% 2452/2561 [30:50<01:21,  1.34it/s]\u001b[A\n",
            "Iteration:  96% 2453/2561 [30:50<01:21,  1.33it/s]\u001b[A\n",
            "Iteration:  96% 2454/2561 [30:51<01:20,  1.33it/s]\u001b[A\n",
            "Iteration:  96% 2455/2561 [30:52<01:19,  1.33it/s]\u001b[A\n",
            "Iteration:  96% 2456/2561 [30:53<01:18,  1.34it/s]\u001b[A\n",
            "Iteration:  96% 2457/2561 [30:53<01:18,  1.33it/s]\u001b[A\n",
            "Iteration:  96% 2458/2561 [30:54<01:17,  1.33it/s]\u001b[A\n",
            "Iteration:  96% 2459/2561 [30:55<01:16,  1.34it/s]\u001b[A\n",
            "Iteration:  96% 2460/2561 [30:56<01:15,  1.34it/s]\u001b[A\n",
            "Iteration:  96% 2461/2561 [30:56<01:14,  1.34it/s]\u001b[A\n",
            "Iteration:  96% 2462/2561 [30:57<01:14,  1.33it/s]\u001b[A\n",
            "Iteration:  96% 2463/2561 [30:58<01:13,  1.34it/s]\u001b[A\n",
            "Iteration:  96% 2464/2561 [30:59<01:12,  1.33it/s]\u001b[A\n",
            "Iteration:  96% 2465/2561 [30:59<01:11,  1.34it/s]\u001b[A\n",
            "Iteration:  96% 2466/2561 [31:00<01:11,  1.33it/s]\u001b[A\n",
            "Iteration:  96% 2467/2561 [31:01<01:10,  1.33it/s]\u001b[A\n",
            "Iteration:  96% 2468/2561 [31:02<01:10,  1.33it/s]\u001b[A\n",
            "Iteration:  96% 2469/2561 [31:02<01:09,  1.33it/s]\u001b[A\n",
            "Iteration:  96% 2470/2561 [31:03<01:08,  1.33it/s]\u001b[A\n",
            "Iteration:  96% 2471/2561 [31:04<01:07,  1.33it/s]\u001b[A\n",
            "Iteration:  97% 2472/2561 [31:05<01:07,  1.33it/s]\u001b[A\n",
            "Iteration:  97% 2473/2561 [31:05<01:06,  1.33it/s]\u001b[A\n",
            "Iteration:  97% 2474/2561 [31:06<01:05,  1.32it/s]\u001b[A\n",
            "Iteration:  97% 2475/2561 [31:07<01:04,  1.33it/s]\u001b[A\n",
            "Iteration:  97% 2476/2561 [31:08<01:03,  1.33it/s]\u001b[A\n",
            "Iteration:  97% 2477/2561 [31:08<01:03,  1.33it/s]\u001b[A\n",
            "Iteration:  97% 2478/2561 [31:09<01:02,  1.34it/s]\u001b[A\n",
            "Iteration:  97% 2479/2561 [31:10<01:01,  1.33it/s]\u001b[A\n",
            "Iteration:  97% 2480/2561 [31:11<01:00,  1.34it/s]\u001b[A\n",
            "Iteration:  97% 2481/2561 [31:11<00:59,  1.33it/s]\u001b[A\n",
            "Iteration:  97% 2482/2561 [31:12<00:59,  1.34it/s]\u001b[A\n",
            "Iteration:  97% 2483/2561 [31:13<00:58,  1.33it/s]\u001b[A\n",
            "Iteration:  97% 2484/2561 [31:14<00:57,  1.34it/s]\u001b[A\n",
            "Iteration:  97% 2485/2561 [31:14<00:57,  1.33it/s]\u001b[A\n",
            "Iteration:  97% 2486/2561 [31:15<00:56,  1.33it/s]\u001b[A\n",
            "Iteration:  97% 2487/2561 [31:16<00:55,  1.34it/s]\u001b[A\n",
            "Iteration:  97% 2488/2561 [31:17<00:54,  1.34it/s]\u001b[A\n",
            "Iteration:  97% 2489/2561 [31:17<00:53,  1.34it/s]\u001b[A\n",
            "Iteration:  97% 2490/2561 [31:18<00:53,  1.34it/s]\u001b[A\n",
            "Iteration:  97% 2491/2561 [31:19<00:52,  1.34it/s]\u001b[A\n",
            "Iteration:  97% 2492/2561 [31:20<00:51,  1.34it/s]\u001b[A\n",
            "Iteration:  97% 2493/2561 [31:20<00:50,  1.34it/s]\u001b[A\n",
            "Iteration:  97% 2494/2561 [31:21<00:50,  1.33it/s]\u001b[A\n",
            "Iteration:  97% 2495/2561 [31:22<00:49,  1.33it/s]\u001b[A\n",
            "Iteration:  97% 2496/2561 [31:23<00:48,  1.34it/s]\u001b[A\n",
            "Iteration:  98% 2497/2561 [31:23<00:47,  1.34it/s]\u001b[A\n",
            "Iteration:  98% 2498/2561 [31:24<00:46,  1.34it/s]\u001b[A\n",
            "Iteration:  98% 2499/2561 [31:25<00:46,  1.34it/s]\u001b[A02/29/2020 07:59:34 - INFO - transformers.configuration_utils -   Configuration saved in output_roberta_GB/checkpoint-2500/config.json\n",
            "02/29/2020 07:59:36 - INFO - transformers.modeling_utils -   Model weights saved in output_roberta_GB/checkpoint-2500/pytorch_model.bin\n",
            "02/29/2020 07:59:36 - INFO - __main__ -   Saving model checkpoint to output_roberta_GB/checkpoint-2500\n",
            "02/29/2020 07:59:40 - INFO - __main__ -   Saving optimizer and scheduler states to output_roberta_GB/checkpoint-2500\n",
            "\n",
            "Iteration:  98% 2500/2561 [31:31<02:20,  2.31s/it]\u001b[A\n",
            "Iteration:  98% 2501/2561 [31:32<01:50,  1.85s/it]\u001b[A\n",
            "Iteration:  98% 2502/2561 [31:32<01:29,  1.52s/it]\u001b[A\n",
            "Iteration:  98% 2503/2561 [31:33<01:14,  1.29s/it]\u001b[A\n",
            "Iteration:  98% 2504/2561 [31:34<01:04,  1.12s/it]\u001b[A\n",
            "Iteration:  98% 2505/2561 [31:35<00:56,  1.02s/it]\u001b[A\n",
            "Iteration:  98% 2506/2561 [31:35<00:51,  1.07it/s]\u001b[A\n",
            "Iteration:  98% 2507/2561 [31:36<00:47,  1.14it/s]\u001b[A\n",
            "Iteration:  98% 2508/2561 [31:37<00:44,  1.19it/s]\u001b[A\n",
            "Iteration:  98% 2509/2561 [31:38<00:42,  1.23it/s]\u001b[A\n",
            "Iteration:  98% 2510/2561 [31:38<00:40,  1.26it/s]\u001b[A\n",
            "Iteration:  98% 2511/2561 [31:39<00:39,  1.27it/s]\u001b[A\n",
            "Iteration:  98% 2512/2561 [31:40<00:37,  1.29it/s]\u001b[A\n",
            "Iteration:  98% 2513/2561 [31:41<00:36,  1.30it/s]\u001b[A\n",
            "Iteration:  98% 2514/2561 [31:41<00:35,  1.31it/s]\u001b[A\n",
            "Iteration:  98% 2515/2561 [31:42<00:34,  1.32it/s]\u001b[A\n",
            "Iteration:  98% 2516/2561 [31:43<00:33,  1.33it/s]\u001b[A\n",
            "Iteration:  98% 2517/2561 [31:44<00:33,  1.33it/s]\u001b[A\n",
            "Iteration:  98% 2518/2561 [31:44<00:32,  1.33it/s]\u001b[A\n",
            "Iteration:  98% 2519/2561 [31:45<00:31,  1.34it/s]\u001b[A\n",
            "Iteration:  98% 2520/2561 [31:46<00:30,  1.33it/s]\u001b[A\n",
            "Iteration:  98% 2521/2561 [31:47<00:30,  1.33it/s]\u001b[A\n",
            "Iteration:  98% 2522/2561 [31:47<00:29,  1.33it/s]\u001b[A\n",
            "Iteration:  99% 2523/2561 [31:48<00:28,  1.33it/s]\u001b[A\n",
            "Iteration:  99% 2524/2561 [31:49<00:27,  1.34it/s]\u001b[A\n",
            "Iteration:  99% 2525/2561 [31:50<00:26,  1.33it/s]\u001b[A\n",
            "Iteration:  99% 2526/2561 [31:50<00:26,  1.34it/s]\u001b[A\n",
            "Iteration:  99% 2527/2561 [31:51<00:25,  1.34it/s]\u001b[A\n",
            "Iteration:  99% 2528/2561 [31:52<00:24,  1.34it/s]\u001b[A\n",
            "Iteration:  99% 2529/2561 [31:53<00:23,  1.33it/s]\u001b[A\n",
            "Iteration:  99% 2530/2561 [31:53<00:23,  1.34it/s]\u001b[A\n",
            "Iteration:  99% 2531/2561 [31:54<00:22,  1.33it/s]\u001b[A\n",
            "Iteration:  99% 2532/2561 [31:55<00:21,  1.33it/s]\u001b[A\n",
            "Iteration:  99% 2533/2561 [31:56<00:21,  1.33it/s]\u001b[A\n",
            "Iteration:  99% 2534/2561 [31:56<00:20,  1.33it/s]\u001b[A\n",
            "Iteration:  99% 2535/2561 [31:57<00:19,  1.33it/s]\u001b[A\n",
            "Iteration:  99% 2536/2561 [31:58<00:18,  1.33it/s]\u001b[A\n",
            "Iteration:  99% 2537/2561 [31:59<00:17,  1.33it/s]\u001b[A\n",
            "Iteration:  99% 2538/2561 [31:59<00:17,  1.33it/s]\u001b[A\n",
            "Iteration:  99% 2539/2561 [32:00<00:16,  1.34it/s]\u001b[A\n",
            "Iteration:  99% 2540/2561 [32:01<00:15,  1.33it/s]\u001b[A\n",
            "Iteration:  99% 2541/2561 [32:02<00:14,  1.34it/s]\u001b[A\n",
            "Iteration:  99% 2542/2561 [32:02<00:14,  1.33it/s]\u001b[A\n",
            "Iteration:  99% 2543/2561 [32:03<00:13,  1.33it/s]\u001b[A\n",
            "Iteration:  99% 2544/2561 [32:04<00:12,  1.33it/s]\u001b[A\n",
            "Iteration:  99% 2545/2561 [32:05<00:12,  1.33it/s]\u001b[A\n",
            "Iteration:  99% 2546/2561 [32:05<00:11,  1.33it/s]\u001b[A\n",
            "Iteration:  99% 2547/2561 [32:06<00:10,  1.33it/s]\u001b[A\n",
            "Iteration:  99% 2548/2561 [32:07<00:09,  1.33it/s]\u001b[A\n",
            "Iteration: 100% 2549/2561 [32:08<00:09,  1.33it/s]\u001b[A\n",
            "Iteration: 100% 2550/2561 [32:08<00:08,  1.33it/s]\u001b[A\n",
            "Iteration: 100% 2551/2561 [32:09<00:07,  1.33it/s]\u001b[A\n",
            "Iteration: 100% 2552/2561 [32:10<00:06,  1.34it/s]\u001b[A\n",
            "Iteration: 100% 2553/2561 [32:11<00:05,  1.33it/s]\u001b[A\n",
            "Iteration: 100% 2554/2561 [32:11<00:05,  1.34it/s]\u001b[A\n",
            "Iteration: 100% 2555/2561 [32:12<00:04,  1.33it/s]\u001b[A\n",
            "Iteration: 100% 2556/2561 [32:13<00:03,  1.34it/s]\u001b[A\n",
            "Iteration: 100% 2557/2561 [32:14<00:03,  1.33it/s]\u001b[A\n",
            "Iteration: 100% 2558/2561 [32:14<00:02,  1.33it/s]\u001b[A\n",
            "Iteration: 100% 2559/2561 [32:15<00:01,  1.33it/s]\u001b[A\n",
            "Iteration: 100% 2560/2561 [32:16<00:00,  1.33it/s]\u001b[A\n",
            "Iteration: 100% 2561/2561 [32:16<00:00,  1.65it/s]\u001b[A\n",
            "Epoch: 100% 1/1 [32:16<00:00, 1936.69s/it]\n",
            "02/29/2020 08:00:25 - INFO - __main__ -    global_step = 2561, average loss = 2.1631468487921657\n",
            "02/29/2020 08:00:25 - INFO - __main__ -   Saving model checkpoint to output_roberta_GB\n",
            "02/29/2020 08:00:25 - INFO - transformers.configuration_utils -   Configuration saved in output_roberta_GB/config.json\n",
            "02/29/2020 08:00:26 - INFO - transformers.modeling_utils -   Model weights saved in output_roberta_GB/pytorch_model.bin\n",
            "02/29/2020 08:00:26 - INFO - transformers.configuration_utils -   loading configuration file output_roberta_GB/config.json\n",
            "02/29/2020 08:00:26 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "02/29/2020 08:00:26 - INFO - transformers.modeling_utils -   loading weights file output_roberta_GB/pytorch_model.bin\n",
            "02/29/2020 08:00:31 - INFO - transformers.tokenization_utils -   Model name 'output_roberta_GB' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'output_roberta_GB' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "02/29/2020 08:00:31 - INFO - transformers.tokenization_utils -   Didn't find file output_roberta_GB/added_tokens.json. We won't load it.\n",
            "02/29/2020 08:00:31 - INFO - transformers.tokenization_utils -   loading file output_roberta_GB/vocab.json\n",
            "02/29/2020 08:00:31 - INFO - transformers.tokenization_utils -   loading file output_roberta_GB/merges.txt\n",
            "02/29/2020 08:00:31 - INFO - transformers.tokenization_utils -   loading file None\n",
            "02/29/2020 08:00:31 - INFO - transformers.tokenization_utils -   loading file output_roberta_GB/special_tokens_map.json\n",
            "02/29/2020 08:00:31 - INFO - transformers.tokenization_utils -   loading file output_roberta_GB/tokenizer_config.json\n",
            "02/29/2020 08:00:31 - INFO - __main__ -   Evaluate the following checkpoints: ['output_roberta_GB']\n",
            "02/29/2020 08:00:31 - INFO - transformers.configuration_utils -   loading configuration file output_roberta_GB/config.json\n",
            "02/29/2020 08:00:31 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "02/29/2020 08:00:31 - INFO - transformers.modeling_utils -   loading weights file output_roberta_GB/pytorch_model.bin\n",
            "02/29/2020 08:00:36 - INFO - __main__ -   Creating features from dataset file at \n",
            "02/29/2020 08:00:42 - INFO - __main__ -   Saving features into cached file roberta_cached_lm_510_gb_blog_test\n",
            "02/29/2020 08:00:42 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "02/29/2020 08:00:42 - INFO - __main__ -     Num examples = 2571\n",
            "02/29/2020 08:00:42 - INFO - __main__ -     Batch size = 4\n",
            "Evaluating: 100% 643/643 [02:34<00:00,  4.49it/s]\n",
            "02/29/2020 08:03:17 - INFO - __main__ -   ***** Eval results  *****\n",
            "02/29/2020 08:03:17 - INFO - __main__ -     perplexity = tensor(7.1367)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nx_2Xi7sp7-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46JPmKEzBcJ1",
        "colab_type": "text"
      },
      "source": [
        "### Post-training\n",
        "\n",
        "After you finish training your models, you can download them by right clicking on each of the 8 files inside the name of your output folder. The largest file, pytorch_model.bin, is the file which does most of the heavy lifting but you need the other files to be able to use it later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtcKAEMJD3sX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}